{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction to ANN with Keras**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Birds inspired us to fly, burdock plants inspired Velcro, and nature has\n",
    " inspired countless more inventions. It seems only logical, then, to look at\n",
    " the brain’s architecture for inspiration on how to build an intelligent\n",
    " machine. This is the logic that sparked artificial neural networks (ANNs),\n",
    " machine learning models inspired by the networks of biological neurons\n",
    " found in our brains. However, although planes were inspired by birds, they\n",
    " don’t have to flap their wings to fly. Similarly, ANNs have gradually\n",
    " become quite different from their biological cousins. Some researchers even\n",
    " argue that we should drop the biological analogy altogether (e.g., by saying\n",
    " “units” rather than “neurons”), lest we restrict our creativity to biologically\n",
    " plausible systems. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ANNs are at the very core of deep learning. They are versatile, powerful,\n",
    " and scalable, making them ideal to tackle large and highly complex\n",
    " machine learning tasks such as classifying billions of images (e.g., Google\n",
    " Images), powering speech recognition services (e.g., Apple’s Siri),\n",
    " recommending the best videos to watch to hundreds of millions of users\n",
    " every day (e.g., YouTube), or learning to beat the world champion at the\n",
    " game of Go (DeepMind’s AlphaGo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The first part of this chapter introduces artificial neural networks, starting\n",
    " with a quick tour of the very first ANN architectures and leading up to\n",
    " multilayer perceptrons, which are heavily used today (other architectures\n",
    " will be explored in the next chapters). In the second part, we will look at\n",
    " how to implement neural networks using TensorFlow’s Keras API. This is a\n",
    " beautifully designed and simple high-level API for building, training,\n",
    " evaluating, and running neural networks. But don’t be fooled by itssimplicity: it is expressive and flexible enough to let you build a wide\n",
    " variety of neural network architectures. In fact, it will probably be sufficient\n",
    " for most of your use cases. And should you ever need extra flexibility, you\n",
    " can always write custom Keras components using its lower-level API, or\n",
    " even use TensorFlow directly, as you will see in Chapter 12.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **From Biological to Artificial Neurons**\n",
    "Surprisingly, ANNs have been around for quite a while: they were first\n",
    " introduced back in 1943 by the neurophysiologist Warren McCulloch and\n",
    " the mathematician Walter Pitts. In their landmark paper  “A Logical\n",
    " Calculus of Ideas Immanent in Nervous Activity”, McCulloch and Pitts\n",
    " presented a simplified computational model of how biological neurons\n",
    " might work together in animal brains to perform complex computations\n",
    " using ***propositional*** logic. This was the first artificial neural network\n",
    " architecture. Since then many other architectures have been invented, as\n",
    " you will see.\n",
    "\n",
    "The early successes of ANNs led to the widespread belief that we would\n",
    " soon be conversing with truly intelligent machines. When it became clear in\n",
    " the 1960s that this promise would go unfulfilled (at least for quite a while),\n",
    " funding flew elsewhere, and ANNs entered a long winter. In the early\n",
    " 1980s, new architectures were invented and better training techniques were\n",
    " developed, sparking a revival of interest in ***connectionism***, the study of\n",
    " neural networks. But progress was slow, and by the 1990s other powerful\n",
    " machine learning techniques had been invented, such as support vector\n",
    " machines (see Chapter 5). These techniques seemed to offer better results\n",
    " and stronger theoretical foundations than ANNs, so once again the study of\n",
    " neural networks was put on hold.\n",
    "\n",
    "We are now witnessing yet another wave of interest in ANNs. Will this\n",
    " wave die out like the previous ones did? Well, here are a few good reasons\n",
    "to believe that this time is different and that the renewed interest in ANNs\n",
    " will have a much more profound impact on our lives:\n",
    "-  There is now a huge quantity of data available to train neural networks,\n",
    " and ANNs frequently outperform other ML techniques on very large\n",
    " and complex problems.\n",
    "\n",
    "-  The tremendous increase in computing power since the 1990s now\n",
    " makes it possible to train large neural networks in a reasonable amount\n",
    " of time. This is in part due to Moore’s law (the number of components\n",
    " in integrated circuits has doubled about every 2 years over the last 50\n",
    " years), but also thanks to the gaming industry, which has stimulated\n",
    " the production of powerful GPU cards by the millions. Moreover,\n",
    " cloud platforms have made this power accessible to everyone.\n",
    "\n",
    "-  The training algorithms have been improved. To be fair they are only\n",
    " slightly different from the ones used in the 1990s, but these relatively\n",
    " small tweaks have had a huge positive impact.\n",
    "\n",
    "-  Some theoretical limitations of ANNs have turned out to be benign in\n",
    " practice. For example, many people thought that ANN training\n",
    " algorithms were doomed because they were likely to get stuck in local\n",
    " optima, but it turns out that this is not a big problem in practice,\n",
    " especially for larger neural networks: the local optima often perform\n",
    " almost as well as the global optimum.\n",
    "\n",
    "-  ANNs seem to have entered a virtuous circle of funding and progress.\n",
    " Amazing products based on ANNs regularly make the headline news,\n",
    " which pulls more and more attention and funding toward them,\n",
    " resulting in more and more progress and even more amazing products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Biological Neurons**\n",
    " Before we discuss artificial neurons, let’s take a quick look at a biological\n",
    " neuron. It is an unusual-looking cell mostly\n",
    " found in animal brains. It’s composed of a cell body containing the nucleus\n",
    " and most of the cell’s complex components, many branching extensionscalled dendrites, plus one very long extension called the axon. The axon’s\n",
    " length may be just a few times longer than the cell body, or up to tens of\n",
    " thousands of times longer. Near its extremity the axon splits off into many\n",
    " branches called telodendria, and at the tip of these branches are minuscule\n",
    " structures called synaptic terminals (or simply synapses), which are\n",
    " connected to the dendrites or cell bodies of other neurons.  Biological\n",
    " neurons produce short electrical impulses called action potentials (APs, or\n",
    " just signals), which travel along the axons and make the synapses release\n",
    " chemical signals called neurotransmitters. When a neuron receives a\n",
    " sufficient amount of these neurotransmitters within a few milliseconds, it\n",
    " fires its own electrical impulses (actually, it depends on the\n",
    " neurotransmitters, as some of them inhibit the neuron from firing).\n",
    "> ***Try finding the Biological neuron diagram for more demonstrations***.....\n",
    "\n",
    "Thus, individual biological neurons seem to behave in a simple way, but\n",
    " they’re organized in a vast network of billions, with each neuron typically\n",
    " connected to thousands of other neurons. Highly complex computations can\n",
    " be performed by a network of fairly simple neurons, much like a complex  anthill can emerge from the combined efforts of simple ants. The\n",
    " architecture of biological neural networks (BNNs)  is the subject of active\n",
    " research, but some parts of the brain have been mapped. These efforts show\n",
    " that neurons are often organized in consecutive layers, especially in the\n",
    " cerebral cortex (the outer layer of the brain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Logical Computations with Neurons**\n",
    " McCulloch and Pitts proposed a very simple model of the biological\n",
    " neuron, which later became known as an ***artificial neuron***: it has one or\n",
    " more binary (on/off) inputs and one binary output. The artificial neuron\n",
    " activates its output when more than a certain number of its inputs are active.\n",
    " In their paper, McCulloch and Pitts showed that even with such a simplified\n",
    " model it is possible to build a network of artificial neurons that can compute\n",
    " any logical proposition you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The Perceptron**\n",
    " The ***perceptron*** is one of the simplest ANN architectures, invented in 1957 by Frank Rosenblatt. It is based on a slightly different artificial neuron called a ***threshold logic unit*** **(TLU)**, or sometimes a ***linear\n",
    " threshold unit*** **(LTU)**. The inputs and output are numbers (instead of binary on/off values), and each input connection is associated with a weight. The TLU first computes a linear function of its inputs: *z = w<sub>1</sub>x<sub>1</sub> + w<sub>2</sub>x<sub>2</sub> + ⋯ + w<sub>n</sub>x<sub>n</sub> + b = **w**<sup>T</sup> **x** + b*. Then it applies a step function to the result: h<sub>w</sub>(x) = step(z). So it’s almost like logistic regression, except it uses a step function instead of the logistic function (Chapter 4). Just like in logistic regression, the model parameters are the input weights **w** and the bias term b.\n",
    "\n",
    "The most common step function used in perceptrons is the ***Heaviside step***\n",
    " function (see Equation below). Sometimes the sign function is used instead.\n",
    "\n",
    "> **Equation:** ***Common step functions used in perceptrons (assuming threshold = 0)***\n",
    "> \n",
    "> ***heaviside (z) = {(0 if z < 0), (1 if z >= 1)***\n",
    "> \n",
    "> ***sgn (z) = {(-1 if z < 0), (0 if z = 0), (+1 if z > 0)***\n",
    "\n",
    " A single TLU can be used for simple linear binary classification. It\n",
    " computes a linear function of its inputs, and if the result exceeds a\n",
    " threshold, it outputs the positive class. Otherwise, it outputs the negative\n",
    " class. This may remind you of logistic regression (Chapter 4) or linear SVM\n",
    " classification (Chapter 5). You could, for example, use a single TLU to\n",
    " classify iris flowers based on petal length and width. Training such a TLU\n",
    " would require finding the right values for w<sub>1</sub>, w<sub>2</sub>, and b (the training algorithm is discussed shortly).\n",
    "\n",
    " A perceptron is composed of one or more TLUs organized in a single layer,\n",
    " where every TLU is connected to every input. Such a layer is called a ***fully\n",
    " connected layer***, or a dense layer. The inputs constitute the input layer. And since the layer of TLUs produces the final outputs, it is called the output layer.\n",
    "\n",
    "Thanks to the magic of linear algebra, Equation below can be used to\n",
    " efficiently compute the outputs of a layer of artificial neurons for several\n",
    " instances at once.\n",
    "\n",
    "> **Equation:** ***Computing the outputs o a fully connected layer***\n",
    ">\n",
    "> ***hW,b***(**X**) = ***ϕ***(**XW**+**b**)\n",
    ">\n",
    "> In this equation:\n",
    ">- As always, **X** represents the matrix of input features. It has one row per\n",
    " instance and one column per feature.\n",
    ">- The weight matrix **W** contains all the connection weights. It has one\n",
    " row per input and one column per neuron.\n",
    ">- The bias vector **b** contains all the bias terms: one per neuron.\n",
    ">- The function **ϕ** is called the ***activation function***: when the artificial neurons are TLUs, it is a step function.\n",
    "\n",
    "> #### **NOTE**\n",
    "> In mathematics, the sum of a matrix and a vector is undefined. However, in data science, we allow “broadcasting”: adding a vector to a matrix means adding it to every row in the matrix. So, **XW + b** first multiplies **X** by **W**—which results in a matrix with one row per instance and one column per output—then adds the vector **b88 to every row of that matrix, which adds each bias term to the corresponding output, for every instance. Moreover, ϕ is then applied itemwise to each item in the resulting matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how is a perceptron trained? The perceptron training algorithm\n",
    " proposed by Rosenblatt was largely inspired by Hebb’s rule. In his 1949\n",
    " book The ***Organization of Behavior*** (Wiley), Donald Hebb suggested that\n",
    " when a biological neuron triggers another neuron often, the connection\n",
    " between these two neurons grows stronger. Siegrid Löwel later summarized\n",
    " Hebb’s idea in the catchy phrase, “Cells that fire together, wire together”;\n",
    " that is, the connection weight between two neurons tends to increase when\n",
    " they fire simultaneously. This rule later became known as Hebb’s rule (or\n",
    " ***Hebbian learning***). Perceptrons are trained using a variant of this rule that\n",
    " takes into account the error made by the network when it makes a\n",
    " prediction; the perceptron learning rule reinforces connections that help\n",
    " reduce the error. More specifically, the perceptron is fed one training\n",
    " instance at a time, and for each instance it makes its predictions. For every\n",
    " output neuron that produced a wrong prediction, it reinforces the connection\n",
    "weights from the inputs that would have contributed to the correct\n",
    " prediction. The rule is shown in Equation below.\n",
    "\n",
    "> **Equation:** ***Perceptron learning rule(weight update)***\n",
    "> \n",
    "> ***w<sub>i,j</sub><sup>(next step)</sup> = w<sub>i,j</sub> + η(y<sub>j</sub> − <sup>ˆ</sup>y<sub>j</sub>)x<sub>i</sub>***\n",
    ">\n",
    "> In this equation:\n",
    "> - ***w<sub>i,j</sub>*** is the connection weight between the ***i<sup>th</sup>*** input and the ***j<sup>th</sup>*** neuron.\n",
    "> - ***x<sub>i</sub>*** is the i<sup>th</sup> input value of the current training instance.\n",
    "> - ***<sup>^</sup>y<sub>j</sub>*** is the output of the j output neuron for the current training instance.\n",
    "> -  ***y<sub>j</sub>*** is the target output of the ***j<sup>th</sup>*** output neuron for the current training instance.\n",
    "> - ***η*** is the learning rate\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The decision boundary of each output neuron is linear, so perceptrons are\n",
    " incapable of learning complex patterns (just like logistic regression\n",
    " classifiers). However, if the training instances are linearly separable,\n",
    " Rosenblatt demonstrated that this algorithm would converge to a solution. \n",
    "This is called the ***perceptron convergence theorem***.\n",
    "\n",
    "Scikit-Learn provides a Perceptron class that can be used pretty much\n",
    " as you would expect—for example, on the iris dataset (introduced in\n",
    " Chapter 4):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as numpy\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris(as_frame=True)\n",
    "x = iris.data[['petal length (cm)', 'petal width (cm)']].values\n",
    "y = (iris.target == 0) # Iris setosa\n",
    "\n",
    "per_clf = Perceptron(random_state=50)\n",
    "per_clf.fit(x, y)\n",
    "x_new = [[2, 0.5], [3, 1]]\n",
    "y_pred = per_clf.predict(x_new) # Predicts True and False for these 2 flowers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that the perceptron learning algorithm strongly resembles stochastic gradient descent (introduced in Chapter 4). In fact Scikit-Learn’s ***Perceptron*** class is equivalent to using an ***SGDClassifier*** with the following hyperparameters: ***loss=\"perceptron\"***, ***learning_rate=\"constant\"***, ***eta0=1*** (the learning rate), and ***penalty=None*** (no regularization).\n",
    "\n",
    "In their 1969 monograph Perceptrons, Marvin Minsky and Seymour Papert\n",
    " highlighted a number of serious weaknesses of perceptrons—in particular,\n",
    " the fact that they are incapable of solving some trivial problems (e.g., the\n",
    " exclusive OR (XOR) classification problem). This is true of any other linear classification model (such as logistic regression classifiers), but researchers had expected much more from perceptrons, and some were so disappointed that they dropped neural networks altogether in favor of higher-level problems such as logic, problem solving, and search. The lack of practical applications also didn’t help.\n",
    "\n",
    " It turns out that some of the limitations of perceptrons can be eliminated by\n",
    " stacking multiple perceptrons. The resulting ANN is called a multilayer\n",
    " ***perceptron*** **(MLP)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### **NOTE**\n",
    ">Contrary to logistic regression classifiers, perceptrons do not output a class probability.\n",
    " This is one reason to prefer logistic regression over perceptrons. Moreover perceptrons are no more prediction errors on the training set, so the mode typically does not generalize as well as logistic regression or a linear SVM classifier. However, perceptrons may train a bit faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The Multilevel Perceptron and Backpropagation**\n",
    " An MLP is composed of one input layer, one or more layers of TLUs called\n",
    " ***hidden layers***, and one final layer of TLUs called the ***output layer***. The layers close to the input layer are usually called the ***lower\n",
    " layers***, and the ones close to the outputs are usually called the ***upper layers***.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### **NOTE**\n",
    "> The signal flows only in one direction (from the inputs to the outputs), so this architecture is an example of a ***feedforward neural network*** **(FNN)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " When an ANN contains a deep stack of hidden layers,  it is called a ***deep neural network*** **(DNN)**. The field of deep learning studies DNNs, and more generally it is interested in models containing deep stacks of computations Even so, many people talk about deep learning whenever neural networks are involved (even shallow ones).\n",
    "\n",
    " For many years researchers struggled to find a way to train MLPs, without\n",
    " success. In the early 1960s several researchers discussed the possibility of\n",
    " using gradient descent to train neural networks, but as we saw in Chapter 4,\n",
    " this requires computing the gradients of the model’s error with regard to the\n",
    " model parameters; it wasn’t clear at the time how to do this efficiently with\n",
    " such a complex model containing so many parameters, especially with the\n",
    " computers they had back then.\n",
    "\n",
    "Then, in 1970, a researcher named Seppo Linnainmaa introduced in his\n",
    " master’s thesis a technique to compute all the gradients automatically and\n",
    " efficiently. This algorithm is now called ***reverse-mode automatic differentiation*** (or ***reverse-mode autodiff*** for short). In just two passes through the network (one forward, one backward), it is able to compute the gradients of the neural network’s error with regard to every single model\n",
    " parameter. In other words, it can find out how each connection weight and\n",
    " each bias should be tweaked in order to reduce the neural network’s error.\n",
    " These gradients can then be used to perform a gradient descent step. If you\n",
    " repeat this process of computing the gradients automatically and taking a\n",
    " gradient descent step, the neural network’s error will gradually drop until it\n",
    " eventually reaches a minimum. This combination of reverse-mode autodiff\n",
    " and gradient descent is now called ***backpropagation*** (or ***backprop*** for short)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### NOTE\n",
    ">  There are various autodiff techniques, with different pros and cons. ***Reverse-mode autodiff*** is well suited when the function to differentiate has many variables (e.g., connection weights and biases) and few outputs (e.g., one loss)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation can actually be applied to all sorts of computational graphs, not just neural networks: indeed, Linnainmaa’s master’s thesis was not about neural nets, it was more general. It was several more years before backprop started to be used to train neural networks, but it still wasn’t mainstream Then, in 1985, David Rumelhart, Geoffrey Hinton, and Ronald Williams published a ***groundbreaking paper(https://homl.info/44)***  analyzing how backpropagation allowed neural networks to learn useful internal representations. Their results were so impressive that backpropagation was quickly popularized in the field. Today, it is by far the most popular training technique for neural networks.\n",
    "\n",
    "Let's run through how backpropagation works again in a bit more detail:\n",
    "- It handles one mini-batch at a time (for example, containing 32 instances each), and it goes through the full training set multiple times. Each pass is called an ***epoch***.\n",
    "\n",
    "- Each mini-batch enters the network through the input layer. The algorithm then computes the output of all the neurons in the first hidden layer, for every instance in the mini-batch. The result is passed on to the next layer, its output is computed and passed to the next layer, and so on until we get the output of the last layer, the output layer. This is the ***forward pass***: it is exactly like making predictions, except all intermediate results are preserved since they are needed for the backward pass.\n",
    "\n",
    "- Next, the algorithm measures the network’s output error (i.e., it uses a loss function that compares the desired output and the actual output of the network, and returns some measure of the error).\n",
    "\n",
    "- Then it computes how much each output bias and each connection to the output layer contributed to the error. This is done analytically by applying the ***chain rule*** (perhaps the most fundamental rule in calculus), which makes this step fast and precise.\n",
    "\n",
    "- The algorithm then measures how much of these error contributions came from each connection in the layer below, again using the chain rule, working backward until it reaches the input layer. As explained earlier, this reverse pass efficiently measures the error gradient across all the connection weights and biases in the network by propagating the error gradient backward through the network (hence the name of the algorithm).\n",
    "\n",
    "-  Finally, the algorithm performs a ***gradient descent*** step to tweak all the connection weights in the network, using the error gradients it just computed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### **WARNING**\n",
    ">  It is important to initialize all the hidden layers’ connection weights randomly, or else training will fail. For example, if you initialize all weights and biases to zero, then all neurons in a given layer will be perfectly identical, and thus backpropagation will affect them in exactly the same way, so they will remain identical. In other words, despite having hundreds of neurons per layer, your model will act as if it had only one neuron per layer: it won’t be too smart. If instead you randomly initialize the weights, you ***break the symmetry*** and allow backpropagation to train a diverse team of neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In short, backpropagation makes predictions for a mini-batch (forward\n",
    " pass), measures the error, then goes through each layer in reverse to\n",
    " measure the error contribution from each parameter (reverse pass), and\n",
    " finally tweaks the connection weights and biases to reduce the error\n",
    " (gradient descent step).\n",
    "\n",
    " In order for backprop to work properly, Rumelhart and his colleagues made\n",
    " a key change to the MLP’s architecture: they replaced the step function with\n",
    " the logistic function, ***σ(z) = 1 / (1 + exp(–z))***, also called the sigmoid\n",
    " function. This was essential because the step function contains only flat\n",
    " segments, so there is no gradient to work with (gradient descent cannot\n",
    " move on a flat surface), while the sigmoid function has a well-defined\n",
    " nonzero derivative everywhere, allowing gradient descent to make some\n",
    " progress at every step. In fact, the backpropagation algorithm works well\n",
    " with many other activation functions, not just the sigmoid function. Here\n",
    " are two other popular choices:\n",
    "- ***The hyperbolic tangent function: tanh(z) = 2σ(2z) – 1***\n",
    "\n",
    "  Just like the sigmoid function, this activation function is S-shaped,\n",
    " continuous, and differentiable, but its output value ranges from –1 to 1\n",
    " (instead of 0 to 1 in the case of the sigmoid function). That range tends\n",
    " to make each layer’s output more or less centered around 0 at the\n",
    " beginning of training, which often helps speed up convergence.\n",
    "\n",
    "- ***The rectified linear unit function: ReLU(z) = max(0, z)***\n",
    "  \n",
    "   The ReLU function is continuous but unfortunately not differentiable at\n",
    " z = 0 (the slope changes abruptly, which can make gradient descent\n",
    " bounce around), and its derivative is 0 for z < 0. In practice, however, it\n",
    " works very well and has the advantage of being fast to compute, so it\n",
    " has become the default.  Importantly, the fact that it does not have a\n",
    " maximum output value helps reduce some issues during gradient\n",
    " descent.\n",
    "\n",
    "OK! You know where neural nets came from, what their architecture is, and\n",
    " how to compute their outputs. You’ve also learned about the\n",
    " backpropagation algorithm. But what exactly can you do with neural nets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Regression MLPs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " First, MLPs can be used for regression tasks. If you want to predict a single value (e.g., the price of a house, given many of its features), then you just need a single output neuron: its output is the predicted value. For multivariate regression (i.e., to predict multiple values at once), you need one output neuron per output dimension. For example, to locate the center of an object in an image, you need to predict 2D coordinates, so you need two output neurons. If you also want to place a bounding box around the object, then you need two more numbers: the width and the height of the object. So, you end up with four output neurons.\n",
    "\n",
    "Scikit-Learn includes an ***MLPRegressor*** class, so let’s use it to build an\n",
    " MLP with three hidden layers composed of 50 neurons each, and train it on\n",
    " the California housing dataset. For simplicity, we will use Scikit-Learn’s\n",
    " ***fetch_california_housing()*** function to load the data. This\n",
    " dataset is simpler than the one we used in Chapter 2, since it contains only\n",
    " numerical features (there is no ***ocean_proximity*** feature), and there are\n",
    " no missing values.\n",
    "\n",
    "The following code starts by fetching and splitting the dataset, then it creates a pipeline to standardize the input features before sending them to the ***MLPRegressor***. This is very important for neural networks because they are trained using gradient descent, and as we saw in Chapter 4, gradient descent does not converge very well when the features have very different scales. Finally, the code trains the model and evaluates its validation error. The model uses the ReLU activation function in the hidden layers, and it uses a variant of gradient descent called ***Adam*** (see Chapter 11) to minimize the mean squared error, with a little bit of ℓ regularization (which you can control via the ***alpha*** hyperparameter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\C'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\C'\n",
      "C:\\Users\\jacks\\AppData\\Local\\Temp\\ipykernel_1216\\3484483884.py:1: SyntaxWarning: invalid escape sequence '\\C'\n",
      "  housing = pd.read_csv(\"D:\\CONTENTS\\AI\\CODE\\The Fundamentals of Machine Learning\\datasets\\housing\\housing.csv\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "housing = pd.read_csv(\"D:\\CONTENTS\\AI\\CODE\\The Fundamentals of Machine Learning\\datasets\\housing\\housing.csv\")\n",
    "housing = housing.drop('ocean_proximity', axis=1)\n",
    "housing_data = housing.drop('median_house_value', axis=1)\n",
    "housing_labels = housing['median_house_value'].copy()\n",
    "x_train_full, x_test, y_train_full, y_test = train_test_split(housing_data, housing_labels, test_size=0.2, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='median')\n",
    "imputer.fit(x_train_full)\n",
    "x_train_full = imputer.transform(x_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train_full, y_train_full, random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mlp_reg = MLPRegressor(hidden_layer_sizes=[50, 50, 50], random_state=50)\n",
    "pipeline = make_pipeline(StandardScaler(), mlp_reg)\n",
    "pipeline.fit(x_train, y_train)\n",
    "y_pred = pipeline.predict(x_valid)\n",
    "rmse = mean_squared_error(y_valid, y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63917.312916266324"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a validation RMSE of about 0.505, which is comparable to what you would get with a random forest classifier. Not too bad for a first try! Note that this MLP does not use any activation function for the output layer, so it’s free to output any value it wants. This is generally fine, but if you want to guarantee that the output will always be positive, then you should use the ReLU activation function in the output layer, or the softplus activation function, which is a smooth variant of ReLU: softplus(z) = log(1 + exp(z)). Softplus is close to 0 when z is negative, and close to z when z is positive. Finally, if you want to guarantee that the predictions will always fall within a given range of values, then you should use the sigmoid function or the hyperbolic tangent, and scale the targets to the appropriate range: 0 to 1 for sigmoid and –1 to 1 for tanh. Sadly, the MLPRegressor class does not support activation functions in the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### **WARNING**\n",
    ">  Building and training a standard MLP with Scikit-Learn in just a few lines of code is very convenient, but the neural net features are limited. This is why we will switch to Keras in the second part of this chapter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ***MLPRegressor*** class uses the mean squared error, which is usually what you want for regression, but if you have a lot of outliers in the training set, you may prefer to use the mean absolute error instead. Alternatively, you may want to use the Huber loss, which is a combination of both. It is quadratic when the error is smaller than a threshold δ (typically 1) but linear when the error is larger than δ. The linear part makes it less sensitive to outliers than the mean squared error, and the quadratic part allows it to converge faster and be more precise than the mean absolute error. However, MLPRegressor only supports the MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Classification MLPs**\n",
    " MLPs can also be used for classification tasks. For a binary classification\n",
    " problem, you just need a single output neuron using the sigmoid activation\n",
    " function: the output will be a number between 0 and 1, which you can\n",
    " interpret as the estimated probability of the positive class. The estimated\n",
    " probability of the negative class is equal to one minus that number.\n",
    " MLPs can also easily handle multilabel binary classification tasks (see\n",
    " Chapter 3). For example, you could have an email classification system that\n",
    " predicts whether each incoming email is ham or spam, and simultaneously\n",
    " predicts whether it is an urgent or nonurgent email. In this case, you would\n",
    " need two output neurons, both using the sigmoid activation function: the\n",
    " first would output the probability that the email is spam, and the second\n",
    " would output the probability that it is urgent. More generally, you would\n",
    " dedicate one output neuron for each positive class. Note that the output\n",
    " probabilities do not necessarily add up to 1. This lets the model output any\n",
    " combination of labels: you can have nonurgent ham, urgent ham, nonurgent\n",
    " spam, and perhaps even urgent spam (although that would probably be an\n",
    " error).\n",
    "\n",
    " If each instance can belong only to a single class, out of three or more\n",
    " possible classes (e.g., classes 0 through 9 for digit image classification),\n",
    " then you need to have one output neuron per class, and you should use the\n",
    " softmax activation function for the whole output layer. The softmax function (introduced in Chapter 4) will ensure that all the estimated probabilities are between 0 and 1 and that they add up to 1, since the classes are exclusive. As you saw in Chapter 3, this is called multiclass classification.\n",
    "\n",
    " Regarding the loss function, since we are predicting probability distributions, the cross-entropy loss (or ***x-entropy*** or log loss for short, see Chapter 4) is generally a good choice.\n",
    "\n",
    " Scikit-Learn has an ***MLPClassifier*** class in the ***sklearn.neural_network*** package. It is almost identical to the ***MLPRegressor*** class, except that it minimizes the cross entropy rather than the MSE. Give it a try now, for example on the iris dataset. It’s almost a linear task, so a single layer with 5 to 10 neurons should suffice (make sure to scale the features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris(as_frame=True)\n",
    "x, y = iris.data, iris.target\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, \n",
    "                                                    random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mlp_clf = MLPClassifier(hidden_layer_sizes=[50, 50, 50], random_state=50)\n",
    "pipeline = make_pipeline(StandardScaler(), mlp_clf)\n",
    "pipeline.fit(x_train, y_train)\n",
    "y_prediction = pipeline.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['versicolor'], dtype='<U10')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance = [[5.6, 3.0, 4.1, 1.3]]\n",
    "\n",
    "def prediction(Instance):\n",
    "    \"\"\"Predicting the new instances.\"\"\"\n",
    "    names = iris.target_names\n",
    "    return names[pipeline.predict(instance)]\n",
    "    \n",
    "prediction(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.   , 0.975, 0.95 ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(mlp_clf, x_train, y_train, cv=3, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! The accuracy is Quite amaizing man as you can see Its almost correto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(estimator=MLPClassifier(hidden_layer_sizes=[50, 50, 50],\n",
       "                                           random_state=50),\n",
       "                   n_iter=100,\n",
       "                   param_distributions={&#x27;hidden_layer_sizes&#x27;: &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001D3E29538F0&gt;},\n",
       "                   random_state=50, scoring=&#x27;neg_root_mean_squared_error&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;RandomizedSearchCV<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\">?<span>Documentation for RandomizedSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>RandomizedSearchCV(estimator=MLPClassifier(hidden_layer_sizes=[50, 50, 50],\n",
       "                                           random_state=50),\n",
       "                   n_iter=100,\n",
       "                   param_distributions={&#x27;hidden_layer_sizes&#x27;: &lt;scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001D3E29538F0&gt;},\n",
       "                   random_state=50, scoring=&#x27;neg_root_mean_squared_error&#x27;)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">estimator: MLPClassifier</label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(hidden_layer_sizes=[50, 50, 50], random_state=50)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;MLPClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(hidden_layer_sizes=[50, 50, 50], random_state=50)</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(estimator=MLPClassifier(hidden_layer_sizes=[50, 50, 50],\n",
       "                                           random_state=50),\n",
       "                   n_iter=100,\n",
       "                   param_distributions={'hidden_layer_sizes': <scipy.stats._distn_infrastructure.rv_discrete_frozen object at 0x000001D3E29538F0>},\n",
       "                   random_state=50, scoring='neg_root_mean_squared_error')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_distribs = {'hidden_layer_sizes': randint(low=4, high=100)}\n",
    "rnd_search = RandomizedSearchCV(mlp_clf, param_distributions=param_distribs, n_iter=100, \n",
    "                                scoring='neg_root_mean_squared_error', random_state=50)\n",
    "rnd_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidden_layer_sizes': 35}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = rnd_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### **TIP**\n",
    ">Before we go on, I recommend you go through exercise 1 at the end of this chapter. Youwill play with various neural network architectures and visualize their outputs using the ***TensorFlow playground***. This will be very useful to better understand MLPs, including the effects of all the hyperparameters (number of layers and neurons, activation functions, and more). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Implementing MLPs with Keras**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Keras is TensorFlow’s high-level deep learning API: it allows you to build,\n",
    " train, evaluate, and execute all sorts of neural networks. The original Keras\n",
    " library was developed by François Chollet as part of a research project \n",
    "and was released as a standalone open source project in March 2015. It\n",
    " quickly gained popularity, owing to its ease of use, flexibility, and beautiful\n",
    " design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### **NOTE**\n",
    "> Keras used to support multiple backends, including TensorFlow, PlaidML, Theano, and Microsoft Cognitive Toolkit (CNTK) (the last two are sadly deprecated), but since version 2.4, Keras is TensorFlow-only. Similarly, TensorFlow used to include multiple high-level APIs, but Keras was officially chosen as its preferred high-level API when TensorFlow 2 came out. Installing TensorFlow will automatically install Keras as well, and Keras will not work without TensorFlow installed. In short, Keras and TensorFlow fell in love and got married. Other popular deep learning libraries include ***PyTorch by Facebook*** *(https://pytorch.org)* and ***JAX by Google*** *(https://github.com/google/jax)*.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now let’s use Keras! We will start by building an MLP for image\n",
    " classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### **NOTE**\n",
    ">  Colab runtimes come with recent versions of TensorFlow and Keras preinstalled. However, if you want to install them on your own machine, please see the installation instructions at *https://homl.info/install*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Building an Image Classifier Using the Sequential API**\n",
    "First, we need to load a dataset. We will use Fashion MNIST, which is a\n",
    " drop-in replacement of MNIST (introduced in Chapter 3). It has the exact\n",
    " same format as MNIST (70,000 grayscale images of 28 × 28 pixels each,\n",
    " with 10 classes), but the images represent fashion items rather than\n",
    " handwritten digits, so each class is more diverse, and the problem turns out\n",
    " to be significantly more challenging than MNIST. For example, a simple\n",
    " linear model reaches about 92% accuracy on MNIST, but only about 83%\n",
    " on Fashion MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Using Keras to load the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras provides some utility functions to fetch and load common datasets,\n",
    " including MNIST, Fashion MNIST, and a few more. Let’s load Fashion\n",
    " MNIST. It’s already shuffled and split into a training set (60,000 images)\n",
    "and a test set (10,000 images), but we’ll hold out the last 5,000 images from\n",
    " the training set for validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
    "(x_train_full, y_train_full), (x_test, y_test) = fashion_mnist\n",
    "x_train, y_train = x_train_full[:-5000], y_train_full[:-5000]\n",
    "x_valid, y_valid = x_train_full[-5000:], y_train_full[-5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### **TIP**\n",
    ">  TensorFlow is usually imported as ***tf***, and the Keras API is available via ***tf.keras.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " When loading MNIST or Fashion MNIST using Keras rather than Scikit\n",
    "Learn, one important difference is that every image is represented as a 28 ×\n",
    " 28 array rather than a 1D array of size 784. Moreover, the pixel intensities\n",
    " are represented as integers (from 0 to 255) rather than floats (from 0.0 to\n",
    " 255.0). Let’s take a look at the shape and data type of the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 28, 28)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For simplicity, we’ll scale the pixel intensities down to the 0–1 range by\n",
    " dividing them by 255.0 (this also converts them to floats):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, x_test = x_train / 255., x_valid / 255., x_test / 255. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\n",
    "    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For example, the first image in the training set represents an ankle boot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ankle boot'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Creating the model using the sequential API**\n",
    "Now let’s build the neural network! Here is a classification MLP with two hidden layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(50) # Setting tensorflow's random seed to make sure it's reproducible.\n",
    "model = tf.keras.Sequential() # Creates sequential model\n",
    "model.add(tf.keras.layers.Input(shape=[28, 28])) # Building of the first layer(an Input layer) and adding it to the model.\n",
    "model.add(tf.keras.layers.Flatten()) # Adding Flatten layer for converting each input image into 1D array.\n",
    "model.add(tf.keras.layers.Dense(300, activation='relu')) # Adding dense hidden layer with 300 neurons, using ReLU activation function.\n",
    "model.add(tf.keras.layers.Dense(100, activation='relu')) # Adding second dense hidden laye with 100 neurons, also using ReLU activation function.\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax')) # Finall, adding a dense output layer with 10 neurons (one per class), Using the softmax activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s go through this code line by line:\n",
    "- First, set TensorFlow’s random seed to make the results reproducible: the random weights of the hidden layers and the output layer will be the same every time you run the notebook. You could also choose to use the ***tf.keras.utils.set_random_seed()*** function,which conveniently sets the random seeds f or TensorFlow, Python (***random.seed()***), and NumPy (***np.random.seed()***).\n",
    "  \n",
    "- The next line creates a ***Sequential*** model. This is the simplest kind of Keras model for neural networks that are just composed of a single stack of layers connected sequentially. This is called the sequential API.\n",
    "\n",
    "- Next, we build the first layer (an ***Input*** layer) and add it to the\n",
    " model. We specify the input shape, which doesn’t include the batch\n",
    " size, only the shape of the instances. Keras needs to know the shape of\n",
    " the inputs so it can determine the shape of the connection weight\n",
    " matrix of the first hidden layer.\n",
    "\n",
    "-  Then we add a ***Flatten layer***. Its role is to convert each input image\n",
    " into a 1D array: for example, if it receives a batch of shape [32, 28,\n",
    " 28], it will reshape it to [32, 784]. In other words, if it receives input\n",
    " data X, it computes ***X.reshape(-1, 784)***. This layer doesn’t have\n",
    " any parameters; it’s just there to do some simple preprocessing.\n",
    "\n",
    "- Next we add a ***Dense*** hidden layer with 300 neurons. It will use the ReLU activation function. Each ***Dense*** layer manages its own weight matrix, containing all the connection weights between the neurons and their inputs. It also manages a vector of bias terms (one per neuron). When it receives some input data, it computes\n",
    "\n",
    "- Then we add a second ***Dense*** hidden layer with 100 neurons, also using the ReLU activation function.\n",
    "\n",
    "- Finally, we add a ***Dense*** output layer with 10 neurons (one per class), using the softmax activation function because the classes are exclusive.\n",
    "  \n",
    "> #### **TIP**\n",
    "> Specifying ***activation=\"relu\"*** is equivalent to specifying ***activation=tf.keras.activations.relu***. Other activation functions are available in the ***tf.keras.activations*** package. We will use many of them in this book; see *https://keras.io/api/layers/activations* for the full list. We will also define our own custom activation functions in Chapter 12.\n",
    "\n",
    "Instead of adding the layers one by one as we just did, it’s often more convenient to pass a list of layers when creating the ***Sequential*** model. You can also drop the ***Input*** layer and instead specify the ***input_shape*** in the first layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[28, 28]), \n",
    "    tf.keras.layers.Dense(300, activation='relu'), \n",
    "    tf.keras.layers.Dense(100, activation='relu'), \n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The model’s ***summary()*** method displays all the model’s layers, \n",
    "including each layer’s name (which is automatically generated unless you\n",
    " set it when creating the layer), its output shape (None means the batch size\n",
    " can be anything), and its number of parameters. The summary ends with the\n",
    " total number of parameters, including trainable and non-trainable\n",
    " parameters. Here we only have trainable parameters (you will see some\n",
    " non-trainable parameters later in this chapter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">784</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">235,500</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">30,100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,010</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m784\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │       \u001b[38;5;34m235,500\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m30,100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,010\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">266,610</span> (1.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m266,610\u001b[0m (1.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">266,610</span> (1.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m266,610\u001b[0m (1.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that ***Dense*** layers often have a lot of parameters. For example, the first hidden layer has 784 × 300 connection weights, plus 300 bias terms,\n",
    "which adds up to 235,500 parameters! This gives the model quite a lot of\n",
    "flexibility to fit the training data, but it also means that the model runs the risk of overfitting, especially when you do not have a lot of training data. We will come back to this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Each layer in a model must have a unique name (e.g., \"***dense_2***\"). You\n",
    " can set the layer names explicitly using the constructor’s ***name*** argument,\n",
    " but generally it’s simpler to let Keras name the layers automatically, as we\n",
    " just did. Keras takes the layer’s class name and converts it to snake case\n",
    " (e.g., a layer from the ***MyCoolLayer*** class is named\n",
    " \"***my_cool_layer***\" by default). Keras also ensures that the name is\n",
    " globally unique, even across models, by appending an index if needed, as in\n",
    " \"***dense_2***\". But why does it bother making the names unique across\n",
    " models? Well, this makes it possible to merge models easily without getting\n",
    " name conflicts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### **TIP**\n",
    ">  All global state managed by Keras is stored in a Keras session, which you can clear using ***tf.keras.backend.clear_session()***. In particular, this resets the name counters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " You can easily get a model’s list of layers using the ***layers*** attribute, or use the ***get_layer()*** method to access a layer by name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Flatten name=flatten_1, built=True>,\n",
       " <Dense name=dense_3, built=True>,\n",
       " <Dense name=dense_4, built=True>,\n",
       " <Dense name=dense_5, built=True>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense_3'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1 = model.layers[1]\n",
    "hidden1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_layer('dense_4') is hidden1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " All the parameters of a layer can be accessed using its ***get_weights()*** and ***set_weights()*** methods. For a ***Dense*** layer, this includes both the connection weights and the bias terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04061993,  0.05263878,  0.01976267, ..., -0.01709368,\n",
       "         0.04550751, -0.00755292],\n",
       "       [-0.06104468, -0.04251107, -0.06040183, ..., -0.0728574 ,\n",
       "        -0.06021762, -0.03848756],\n",
       "       [-0.02107225, -0.0481306 ,  0.05012794, ...,  0.0427199 ,\n",
       "        -0.05466549,  0.0346661 ],\n",
       "       ...,\n",
       "       [ 0.00462282, -0.07082872, -0.0374566 , ...,  0.01830951,\n",
       "         0.05111666,  0.03550609],\n",
       "       [ 0.01519047, -0.02951877,  0.02454186, ..., -0.06491356,\n",
       "        -0.04198295, -0.04676589],\n",
       "       [-0.05108195,  0.0675797 , -0.05103516, ...,  0.01267258,\n",
       "        -0.05729102,  0.05912109]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, biases = hidden1.get_weights()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 300)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the ***Dense*** layer initialized the connection weights randomly (which is needed to break symmetry, as discussed earlier), and the biases were initialized to zeros, which is fine. If you want to use a different initialization method, you can set ***kernel_initializer*** (kernel is another name for the matrix of connection weights) or ***bias_initializer*** when creating the layer. We’ll discuss initializers further in Chapter 11, and the full list is at *https://keras.io/api/layers/initializers*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### **NOTE**\n",
    "> The shape of the weight matrix depends on the number of inputs, which is why we specified the ***input_shape*** when creating the model. If you do not specify the input shape, it’s OK: Keras will simply wait until it knows the input shape before it actually builds the model parameters. This will happen either when you feed it some data (e.g., during training), or when you call its ***build()*** method. Until the model parameters are built, you will not be able to do certain things, such as display the model summary or save the model. So, if you know the input shape when creating the model, it is best to specify it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Compiling the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a model is created, you must call its ***compile()*** method to specify\n",
    " the loss function and the optimizer to use. Optionally, you can specify a list\n",
    " of extra metrics to compute during training and evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='sgd', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### **NOTE**\n",
    "> Using ***loss***=\"***sparse_categorical_crossentropy***\" is the equivalent of using ***loss***=***tf.keras.losses.sparse_categorical_ cross entropy.*** Similarly, using ***optimizer***=\"***sgd***\" is the equivalent of using ***optimizer***=***tf.keras.optimizers.SGD()***, and using metrics= [\"***accuracy***\"] is the equivalent of using ***metrics***=[***tf.keras.metrics.sparse_categorical_accuracy***] (when using this loss). We will use many other losses, optimizers, and metrics in this book; for the full lists, see *https://keras.io/api/losses*, *https://keras.io/api/optimizers*, and  *https://keras.io/api/metrics*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code requires explanation. We use the \"***sparse_categorical_crossentropy***\" loss because we have sparse labels (i.e., for each instance, there is just a target class index, from 0 to 9 in this case), and the classes are exclusive. If instead we had one target probability per class for each instance (such as one-hot vectors, e.g., [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.] to represent class 3), then we would need to use the \"***categorical_crossentropy***\" loss instead. If we were doing binary classification or multilabel binary classification, then we would use the \"***sigmoid***\" activation function in the output layer instead of the \"***softmax***\" activation function, and we would use the \"***binary_crossentropy***\" loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### **TIP**\n",
    "> If you want to convert sparse labels (i.e., class indices) to one-hot vector labels, use the ***tf.keras.utils.to_categorical()*** function. To go the other way round, use the ***np.argmax()*** function with ***axis=1***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the optimizer, \"***sgd***\" means that we will train the model using\n",
    " stochastic gradient descent. In other words, Keras will perform the\n",
    " backpropagation algorithm described earlier (i.e., reverse-mode autodiff\n",
    " plus gradient descent). We will discuss more efficient optimizers in\n",
    " Chapter 11. They improve gradient descent, not autodiff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### **NOTE**\n",
    "> When using the **SGD** optimizer, it is important to tune the learning rate. So, you will generally want to use ***optimizer=tf.keras.optimizers.SGD(learning_rate=__???__)*** to set the learning rate, rather than ***optimizer***=\"***sgd***\", which defaults to a learning rate of 0.01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Finally, since this is a classifier, it’s useful to measure its accuracy during training and evaluation, which is why we set ***metrics***=[\"***accuracy***\"]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Training and evaluating the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now the model is ready to be trained. For this we simply need to call its\n",
    " ***fit()*** method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9564 - loss: 0.1311 - val_accuracy: 0.8834 - val_loss: 0.3711\n",
      "Epoch 2/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9566 - loss: 0.1290 - val_accuracy: 0.8816 - val_loss: 0.3792\n",
      "Epoch 3/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9578 - loss: 0.1265 - val_accuracy: 0.8818 - val_loss: 0.3801\n",
      "Epoch 4/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9590 - loss: 0.1240 - val_accuracy: 0.8828 - val_loss: 0.3813\n",
      "Epoch 5/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9600 - loss: 0.1220 - val_accuracy: 0.8810 - val_loss: 0.3860\n",
      "Epoch 6/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9608 - loss: 0.1195 - val_accuracy: 0.8806 - val_loss: 0.3903\n",
      "Epoch 7/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9620 - loss: 0.1175 - val_accuracy: 0.8802 - val_loss: 0.3935\n",
      "Epoch 8/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9625 - loss: 0.1150 - val_accuracy: 0.8792 - val_loss: 0.3984\n",
      "Epoch 9/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9638 - loss: 0.1130 - val_accuracy: 0.8826 - val_loss: 0.3961\n",
      "Epoch 10/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9641 - loss: 0.1107 - val_accuracy: 0.8808 - val_loss: 0.4022\n",
      "Epoch 11/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9651 - loss: 0.1086 - val_accuracy: 0.8808 - val_loss: 0.4077\n",
      "Epoch 12/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9665 - loss: 0.1064 - val_accuracy: 0.8802 - val_loss: 0.4105\n",
      "Epoch 13/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9668 - loss: 0.1043 - val_accuracy: 0.8820 - val_loss: 0.4094\n",
      "Epoch 14/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9675 - loss: 0.1023 - val_accuracy: 0.8800 - val_loss: 0.4171\n",
      "Epoch 15/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9679 - loss: 0.0999 - val_accuracy: 0.8798 - val_loss: 0.4191\n",
      "Epoch 16/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9689 - loss: 0.0976 - val_accuracy: 0.8792 - val_loss: 0.4283\n",
      "Epoch 17/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9699 - loss: 0.0958 - val_accuracy: 0.8786 - val_loss: 0.4324\n",
      "Epoch 18/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9695 - loss: 0.0943 - val_accuracy: 0.8804 - val_loss: 0.4332\n",
      "Epoch 19/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9716 - loss: 0.0920 - val_accuracy: 0.8796 - val_loss: 0.4369\n",
      "Epoch 20/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9717 - loss: 0.0900 - val_accuracy: 0.8800 - val_loss: 0.4354\n",
      "Epoch 21/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9723 - loss: 0.0877 - val_accuracy: 0.8794 - val_loss: 0.4413\n",
      "Epoch 22/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9727 - loss: 0.0861 - val_accuracy: 0.8798 - val_loss: 0.4431\n",
      "Epoch 23/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9744 - loss: 0.0841 - val_accuracy: 0.8772 - val_loss: 0.4517\n",
      "Epoch 24/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9742 - loss: 0.0824 - val_accuracy: 0.8782 - val_loss: 0.4524\n",
      "Epoch 25/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9746 - loss: 0.0805 - val_accuracy: 0.8804 - val_loss: 0.4501\n",
      "Epoch 26/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9765 - loss: 0.0786 - val_accuracy: 0.8760 - val_loss: 0.4681\n",
      "Epoch 27/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9763 - loss: 0.0773 - val_accuracy: 0.8766 - val_loss: 0.4722\n",
      "Epoch 28/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9773 - loss: 0.0757 - val_accuracy: 0.8772 - val_loss: 0.4695\n",
      "Epoch 29/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9780 - loss: 0.0733 - val_accuracy: 0.8790 - val_loss: 0.4655\n",
      "Epoch 30/30\n",
      "\u001b[1m1719/1719\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9782 - loss: 0.0723 - val_accuracy: 0.8780 - val_loss: 0.4753\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=30, validation_data=(x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass it the input features (***X_train***) and the target classes (***y_train***), as well as the number of epochs to train (or else it would default to just 1, which would definitely not be enough to converge to a good solution). We also pass a validation set (this is optional). Keras will measure the loss and the extra metrics on this set at the end of each epoch, which is very useful to see how well the model really performs. If the performance on the training set is much better than on the validation set, your model is probably overfitting the training set, or there is a bug, such as a data mismatch between the training set and the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### **TIP**\n",
    "> Shape errors are quite common, especially when getting started, so you should familiarize yourself with the error messages: try fitting a model with inputs and/or labels of the wrong shape, and see the errors you get. Similarly, try compiling the model with ***loss***=\"***categorical_crossentropy***\" instead of ***loss***=\"***sparse_cat egorical_crossentropy***\". Or you can remove the ***Flatten*** layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that’s it! The neural network is trained. At each epoch during training, Keras displays the number of mini-batches processed so far on the left side of the progress bar. The batch size is 32 by default, and since the training set has 55,000 images, the model goes through 1,719 batches per epoch: 1,718 of size 32, and 1 of size 24. After the progress bar, you can see the mean training time per sample, and the loss and accuracy (or any other extra metrics you asked for) on both the training set and the validation set. Notice that the training loss went down, which is a good sign, and the validation accuracy reached 88.24% after 30 epochs. That’s slightly below the training accuracy, so there is a little bit of overfitting going on, but not a huge amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### **TIP**\n",
    ">  Instead of passing a validation set using the ***validation_data*** argument, you could set ***validation_split*** to the ratio of the training set that you want Keras to use for validation. For example, ***validation_split=0.1*** tells Keras to use the last 10% of the data (before shuffling) for validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If the training set was very skewed, with some classes being\n",
    " overrepresented and others underrepresented, it would be useful to set the\n",
    " ***class_weight*** argument when calling the ***fit()*** method, to give a\n",
    " larger weight to underrepresented classes and a lower weight to\n",
    " overrepresented classes. These weights would be used by Keras when\n",
    " computing the loss. If you need per-instance weights, set the\n",
    " ***sample_weight*** argument. If both ***class_weight*** and\n",
    " ***sample_weight*** are provided, then Keras multiplies them. Per-instance\n",
    " weights could be useful, for example, if some instances were labeled by\n",
    " experts while others were labeled using a crowdsourcing platform: you\n",
    " might want to give more weight to the former. You can also provide sample\n",
    " weights (but not class weights) for the validation set by adding them as a\n",
    " third item in the ***validation_data*** tuple.\n",
    "\n",
    "\n",
    "The ***fit()*** method returns a ***History*** object containing the training parameters (***history.params***), the list of epochs it went through (***history.epoch***), and most importantly a dictionary (***history.history***) containing the loss and extra metrics it measured at the end of each epoch on the training set and on the validation set (if any). If you use this dictionary to create a Pandas DataFrame and call its ***plot()*** method, you get the learning curves shown in Figure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(history.history).plot(\n",
    "    figsize=(8, 5), xlim=[0, 29], ylim=[0, 1], grid=True, \n",
    "    xlabel='Epoch', style=['r--', 'r--.', 'b-', 'b-*']\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that both the training accuracy and the validation accuracy steadily increase during training, while the training loss and the validation loss decrease. This is good. The validation curves are relatively close to each other at first, but they get further apart over time, which shows that there’s a little bit of overfitting. In this particular case, the model looks like it performed better on the validation set than on the training set at the beginning of training, but that’s not actually the case. The validation error is computed at the end of each epoch, while the training error is computed using a running mean during each epoch, so the training curve should be shifted by half an epoch to the left. If you do that, you will see that the training and validation curves overlap almost perfectly at the beginning of training.\n",
    "\n",
    "The training set performance ends up beating the validation performance, as is generally the case when you train for long enough. You can tell that the model has not quite converged yet, as the validation loss is still going down, so you should probably continue training. This is as simple as calling the fit() method again, since Keras just continues training where it left off: you should be able to reach about 89.8% validation accuracy, while the training accuracy will continue to rise up to 100% (this is not always the case).\n",
    "\n",
    "If you are not satisfied with the performance of your model, you should go back and tune the hyperparameters. The first one to check is the learning rate. If that doesn’t help, try another optimizer (and always retune the learning rate after changing any hyperparameter). If the performance is still not great, then try tuning model hyperparameters such as the number of layers, the number of neurons per layer, and the types of activation functions to use for each hidden layer. You can also try tuning other hyperparameters, such as the batch size (it can be set in the ***fit()*** method using the ***batch_size*** argument, which defaults to 32). We will get back to hyperparameter tuning at the end of this chapter. Once you are satisfied with your model’s validation accuracy, you should evaluate it on the test set to estimate the generalization error before you deploy the model to production. You can easily do this using the ***evaluate()*** method (it also supports several other arguments, such as ***batch_size*** and ***sample_weight***; please check the documentation for more details):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8744 - loss: 0.5145\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.49801069498062134, 0.8743000030517578]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As you saw in Chapter 2, it is common to get slightly lower performance on\n",
    " the test set than on the validation set, because the hyperparameters are tuned\n",
    " on the validation set, not the test set (however, in this example, we did not\n",
    " do any hyperparameter tuning, so the lower accuracy is just bad luck).\n",
    " Remember to resist the temptation to tweak the hyperparameters on the test\n",
    " set, or else your estimate of the generalization error will be too optimistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Using the model to make predictions**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
