{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Natural Language Processing with RNNs and Attention**\n",
    "When Alan Turing imagined his famous [Turing test](https://homl.info/turingtest)  in 1950, he proposed a way to evaluate a machine’s ability to match human intelligence. He could have tested for many things, such as the ability to recognize cats in pictures, play chess, compose music, or escape a maze, but, interestingly, he chose a linguistic task. More specifically, he devised a chatbot capable of fooling its interlocutor into thinking it was human.  This test does have its weaknesses: a set of hardcoded rules can fool unsuspecting or naive humans (e.g., the machine could give vague predefined answers in response to some keywords, it could pretend that it is joking or drunk to get a pass on its weirdest answers, or it could escape difficult questions by answering them with its own questions), and many aspects of human intelligence are utterly ignored (e.g., the ability to interpret nonverbal communication such as facial expressions, or to learn a manual task). But the test does highlight the fact that mastering language is arguably Homo sapiens’s greatest cognitive ability.\n",
    "\n",
    "Can we build a machine that can master written and spoken language? This is the ultimate goal of NLP research, but it’s a bit too broad, so in practice researchers focus on more specific tasks, such as text classification, translation, summarization, question answering, and many more.\n",
    "\n",
    "A common approach for natural language tasks is to use recurrent neural networks. We will therefore continue to explore RNNs (introduced in Chapter 15), starting with a character RNN, or char-RNN, trained to predict the next character in a sentence. This will allow us to generate some original text. We will first use a stateless RNN (which learns on random portions of text at each iteration, without any information on the rest of the text), then we will build a stateful RNN (which preserves the hidden state between training iterations and continues reading where it left off, allowing it to learn longer patterns). Next, we will build an RNN to perform sentiment analysis (e.g., reading movie reviews and extracting the rater’s feeling about the movie), this time treating sentences as sequences of words, rather than characters. Then we will show how RNNs can be used to build an encoder–decoder architecture capable of performing neural machine translation (NMT), translating English to Spanish.\n",
    "\n",
    "In the second part of this chapter, we will explore attention mechanisms. As their name suggests, these are neural network components that learn to select the part of the inputs that the rest of the model should focus on at each time step. First, we will boost the performance of an RNN-based encoder–decoder architecture using attention. Next, we will drop RNNs altogether and use a very successful attention-only architecture, called the transformer, to build a translation model. We will then discuss some of the most important advances in NLP in the last few years, including incredibly powerful language models such as GPT and BERT, both based on transformers. Lastly, I will show you how to get started with the excellent Transformers library by Hugging Face.\n",
    "\n",
    "Let's start with simple and fun model that can write like Shakespeare (sort of)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Generating Shakespearean Text Using a Character RNN**\n",
    "In a famous 2015 blog post titled “The Unreasonable Effectiveness of Recurrent Neural Networks”, Andrej Karpathy showed how to train an RNN to predict the next character in a sentence. This char-RNN can then be used to generate novel text, one character at a time. Here is a small sample of the text generated by a char-RNN model after it was trained on all of Shakespeare’s works:\n",
    "- **PANDARUS*:\n",
    "- *Alas, I think he shall be come approached and the day*\n",
    "- *When little srain would be attain’d into being never fed*,\n",
    "- *And who is but a chain and subjects of his death*,\n",
    "- *I should not sleep.*\n",
    "  \n",
    "Not exactly a masterpiece, but it is still impressive that the model was able to learn words, grammar, proper punctuation, and more, just by learning to predict the next character in a sentence. This is our first example of a language model; similar (but much more powerful) language models, discussed later in this chapter, are at the core of modern NLP. In the remainder of this section we’ll build a char-RNN step by step, starting with the creation of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating the Dataset**\n",
    "First, using Keras’s handy tf.keras.utils.get_file() function, let’s download all of Shakespeare’s works. The data is loaded from Andrej Karpathy’s [char-rnn project](https://github.com/karpathy/char-rnn):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 07:33:07.275963: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753245187.379020   10215 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753245187.407294   10215 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1753245187.486993   10215 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753245187.487082   10215 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753245187.487098   10215 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753245187.487108   10215 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-23 07:33:07.514130: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "shakespeare_url = \"https://homl.info/shakespeare\" # shortcut URL\n",
    "filepath = tf.keras.utils.get_file(\"shakespear.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the first few lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(shakespeare_text[:149])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look's like shakespeare all right!\n",
    "\n",
    "Next, we’ll use a ***tf.keras.layers.TextVectorization*** layer (introduced in Chapter 13) to encode this text. We set ***split=\"character\"*** to get character-level encoding rather than the default word-level encoding, and we use ***standardize=\"lower\"*** to convert the text to lowercase (which will simplify the task):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1753221097.507433    7933 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1698 MB memory:  -> device: 0, name: NVIDIA GeForce MX150, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "2025-07-23 00:51:41.367295: W external/local_xla/xla/service/gpu/llvm_gpu_backend/default/nvptx_libdevice_path.cc:40] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.\n",
      "Searched for CUDA in the following directories:\n",
      "  ./cuda_sdk_lib\n",
      "  ipykernel_launcher.runfiles/cuda_nvcc\n",
      "  ipykern/cuda_nvcc\n",
      "  \n",
      "  /usr/local/cuda\n",
      "  /opt/cuda\n",
      "  /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/tensorflow/python/platform/../../../nvidia/cuda_nvcc\n",
      "  /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/tensorflow/python/platform/../../../../nvidia/cuda_nvcc\n",
      "  /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/tensorflow/python/platform/../../cuda\n",
      "  /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/tensorflow/python/platform/../../../../../..\n",
      "  /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/tensorflow/python/platform/../../../../../../..\n",
      "  .\n",
      "You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.\n"
     ]
    }
   ],
   "source": [
    "text_vec_layer = tf.keras.layers.TextVectorization(split='character', \n",
    "                                                   standardize='lower')\n",
    "text_vec_layer.adapt([shakespeare_text])\n",
    "encoded = text_vec_layer([shakespeare_text]) [0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each character is now mapped to an integer, starting at 2. The ***TextVectorization*** layer reserved the value 0 for padding tokens, and it reserved 1 for unknown characters. We won’t need either of these tokens for now, so let’s subtract 2 from the character IDs and compute the number of distinct characters and the total number of characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded -= 2 # drop tokens 0 (pad) and 1 (unknown), which will not use\n",
    "n_tokens = text_vec_layer.vocabulary_size() - 2 # number of distinct chars = 39\n",
    "dataset_size = len(encoded) # total number of chars = 1,115,394\n",
    "dataset_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, just like we did in Chapter 15, we can turn this very long sequence into a dataset of windows that we can then use to train a sequence-to-sequence RNN. The targets will be similar to the inputs, but shifted by one time step into the “future”. For example, one sample in the dataset may be a sequence of character IDs representing the text “to be or not to b” (without the final “e”), and the corresponding target—a sequence of character IDs representing the text “o be or not to be” (with the final “e”, but without the leading “t”). Let’s write a small utility function to convert a long sequence of character IDs into a dataset of input/target window pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\n",
    "    sequence = tf.where(sequence < 0, 0, sequence)\n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "    ds = ds.window(length + 1, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda window_ds: window_ds.batch(length + 1))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=100_000, seed=seed)\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function starts much like the ***to_windows()*** custom utility function we created in Chapter 15:\n",
    "- It takes a sequence as input (i.e., the encoded text), and creates a dataset containing all the windows of the desired length.\n",
    "- It increases the length by one, since we need the next character for the target.\n",
    "- Then, it shuffles the windows (optionally), batches them, splits them into input/output pairs, and activates prefetching.\n",
    "\n",
    "[Figure 16-1](#fig161) summarizes the dataset preparation steps: it shows windows of length 11, and a batch size of 3. The start index of each window is indicated next to it.\n",
    "\n",
    "<span id=fig161>\n",
    "![Preparing a dataset of shuffled windows](f161.png)\n",
    "</span>\n",
    "\n",
    "Now we’re ready to create the training set, the validation set, and the test set. We will use roughly 90% of the text for training, 5% for validation, and 5% for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 100\n",
    "train = int((dataset_size*0.9)//1)\n",
    "remain = (dataset_size - train) // 2\n",
    "tf.random.set_seed(50)\n",
    "train_set = to_dataset(encoded[:train], length=length, shuffle=True, seed=50)\n",
    "valid_set = to_dataset(encoded[train:(train+remain)], length=length)\n",
    "test_set = to_dataset(encoded[-remain:], length=length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### **TIP**\n",
    "> We set the window length to 100, but you can try tuning it: it’s easier and faster to train RNNs on shorter input sequences, but the RNN will not be able to learn any pattern longer than length, so don’t make it too small.\n",
    "\n",
    "That's it! Preparing the dataset was the hardest part. Now let's create the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Building and Training the Char-RNN Model**\n",
    "Since our dataset is reasonably large, and modeling language is quite a difficult task, we need more than a simple RNN with a few recurrent neurons. Let’s build and train a model with one GRU layer composed of 128 units (you can try tweaking the number of layers and units later, if needed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 01:09:30.305188: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Graph execution error:\n\nDetected at node sequential_6/gru_8/gru_cell/recurrent_kernel/Initializer/Sign defined at (most recent call last):\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n\n  File \"/tmp/ipykernel_7933/1277159966.py\", line 42, in <module>\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 377, in fit\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 220, in function\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 133, in multi_step_on_iterator\n\nJIT compilation failed.\n\t [[{{node sequential_6/gru_8/gru_cell/recurrent_kernel/Initializer/Sign}}]] [Op:__inference_initialize_variables_8394]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 42\u001b[0m\n\u001b[1;32m     38\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnadam\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     39\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     40\u001b[0m model_ckpt \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_shakespear_model.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 42\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmodel_ckpt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:133\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.multi_step_on_iterator\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;129m@tf\u001b[39m\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mdo_not_convert\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmulti_step_on_iterator\u001b[39m(iterator):\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_execution \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mOptional\u001b[38;5;241m.\u001b[39mfrom_value(\n\u001b[0;32m--> 133\u001b[0m             \u001b[43mone_step_on_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m         )\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# the spec is set lazily during the tracing of `tf.while_loop`\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     empty_outputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mOptional\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mUnknownError\u001b[0m: Graph execution error:\n\nDetected at node sequential_6/gru_8/gru_cell/recurrent_kernel/Initializer/Sign defined at (most recent call last):\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n\n  File \"/tmp/ipykernel_7933/1277159966.py\", line 42, in <module>\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 377, in fit\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 220, in function\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 133, in multi_step_on_iterator\n\nJIT compilation failed.\n\t [[{{node sequential_6/gru_8/gru_cell/recurrent_kernel/Initializer/Sign}}]] [Op:__inference_initialize_variables_8394]"
     ]
    }
   ],
   "source": [
    "tf.config.optimizer.set_jit(False)\n",
    "\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16), \n",
    "    tf.keras.layers.GRU(128, return_sequences=True), \n",
    "    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='nadam', \n",
    "              metrics=['accuracy'])\n",
    "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"my_shakespear_model.keras\", monitor=\"val_accuracy\", save_best_only=True)\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=10, callbacks=[model_ckpt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=\"nadam\",\n",
    "    metrics=[\"accuracy\"],\n",
    "    run_eagerly=True   # <— this forces eager execution, no JIT at all\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 01:16:38.051352: W external/local_xla/xla/service/gpu/llvm_gpu_backend/nvptx_backend.cc:110] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "2025-07-23 01:16:38.060594: W external/local_xla/xla/service/gpu/llvm_gpu_backend/nvptx_backend.cc:110] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "2025-07-23 01:16:38.070643: W external/local_xla/xla/service/gpu/llvm_gpu_backend/nvptx_backend.cc:110] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "2025-07-23 01:16:38.080369: W external/local_xla/xla/service/gpu/llvm_gpu_backend/nvptx_backend.cc:110] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "2025-07-23 01:16:38.089755: W external/local_xla/xla/service/gpu/llvm_gpu_backend/nvptx_backend.cc:110] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "2025-07-23 01:16:38.098293: W external/local_xla/xla/service/gpu/llvm_gpu_backend/nvptx_backend.cc:110] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "2025-07-23 01:16:38.107044: W external/local_xla/xla/service/gpu/llvm_gpu_backend/nvptx_backend.cc:110] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "error: libdevice not found at ./libdevice.10.bc\n",
      "2025-07-23 01:16:38.118297: W external/local_xla/xla/service/gpu/llvm_gpu_backend/nvptx_backend.cc:110] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "error: libdevice not found at ./libdevice.10.bc\n",
      "error: libdevice not found at ./libdevice.10.bc\n",
      "error: libdevice not found at ./libdevice.10.bc\n",
      "error: libdevice not found at ./libdevice.10.bc\n",
      "error: libdevice not found at ./libdevice.10.bc\n",
      "error: libdevice not found at ./libdevice.10.bc\n",
      "error: libdevice not found at ./libdevice.10.bc\n",
      "2025-07-23 01:16:38.120826: E tensorflow/compiler/mlir/tools/kernel_gen/tf_framework_c_interface.cc:227] INTERNAL: Generating device code failed.\n",
      "2025-07-23 01:16:38.125902: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-23 01:16:38.126032: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 2312000165700662266\n",
      "2025-07-23 01:16:38.126125: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 3299376801925039924\n",
      "2025-07-23 01:16:38.126168: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 4745518012032808365\n",
      "2025-07-23 01:16:38.126299: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-23 01:16:38.126378: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-23 01:16:38.126434: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-23 01:16:38.126486: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-23 01:16:38.126537: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-23 01:16:38.126591: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-23 01:16:38.126679: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-23 01:16:38.126764: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-23 01:16:38.126841: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-23 01:16:38.126940: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-23 01:16:38.127018: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-23 01:16:38.127086: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-23 01:16:38.127138: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-23 01:16:38.127191: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-23 01:16:38.127242: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-23 01:16:38.127300: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-23 01:16:38.127363: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-23 01:16:38.127416: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Graph execution error:\n\nDetected at node nadam/Pow_4 defined at (most recent call last):\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n\n  File \"/tmp/ipykernel_7933/2763532858.py\", line 21, in <module>\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 377, in fit\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 220, in function\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 133, in multi_step_on_iterator\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 114, in one_step_on_data\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 81, in train_step\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/optimizers/base_optimizer.py\", line 463, in apply_gradients\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/optimizers/base_optimizer.py\", line 527, in apply\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/optimizers/base_optimizer.py\", line 593, in _backend_apply_gradients\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/optimizers/nadam.py\", line 106, in _backend_update_step\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/optimizer.py\", line 120, in _backend_update_step\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/optimizer.py\", line 134, in _distributed_tf_update_step\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/optimizer.py\", line 131, in apply_grad_to_update_var\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/optimizers/nadam.py\", line 119, in update_step\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/numpy.py\", line 6391, in power\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/numpy.py\", line 2653, in power\n\nJIT compilation failed.\n\t [[{{node nadam/Pow_4}}]] [Op:__inference_multi_step_on_iterator_12190]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 21\u001b[0m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential([\n\u001b[1;32m      6\u001b[0m     Embedding(input_dim\u001b[38;5;241m=\u001b[39mn_tokens, output_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m),\n\u001b[1;32m      7\u001b[0m     GRU(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     Dense(n_tokens, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m ])\n\u001b[1;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m     17\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnadam\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     19\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     20\u001b[0m )\n\u001b[0;32m---> 21\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnknownError\u001b[0m: Graph execution error:\n\nDetected at node nadam/Pow_4 defined at (most recent call last):\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n\n  File \"/tmp/ipykernel_7933/2763532858.py\", line 21, in <module>\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 377, in fit\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 220, in function\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 133, in multi_step_on_iterator\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 114, in one_step_on_data\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 81, in train_step\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/optimizers/base_optimizer.py\", line 463, in apply_gradients\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/optimizers/base_optimizer.py\", line 527, in apply\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/optimizers/base_optimizer.py\", line 593, in _backend_apply_gradients\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/optimizers/nadam.py\", line 106, in _backend_update_step\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/optimizer.py\", line 120, in _backend_update_step\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/optimizer.py\", line 134, in _distributed_tf_update_step\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/optimizer.py\", line 131, in apply_grad_to_update_var\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/optimizers/nadam.py\", line 119, in update_step\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/numpy.py\", line 6391, in power\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/numpy.py\", line 2653, in power\n\nJIT compilation failed.\n\t [[{{node nadam/Pow_4}}]] [Op:__inference_multi_step_on_iterator_12190]"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=n_tokens, output_dim=16),\n",
    "    GRU(\n",
    "      128,\n",
    "      return_sequences=True,\n",
    "      # Use a uniform initializer which doesn’t call tf.sign under the hood\n",
    "      recurrent_initializer=GlorotUniform()\n",
    "    ),\n",
    "    Dense(n_tokens, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=\"nadam\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go over this code:\n",
    "- We use an ***Embedding*** layer as the first layer, to encode the character IDs (embeddings were introduced in Chapter 13). The ***Embedding*** layer’s number of input dimensions is the number of distinct character IDs, and the number of output dimensions is a hyperparameter you can tune—we’ll set it to 16 for now. Whereas the inputs of the ***Embedding*** layer will be 2D tensors of shape [*batch size, window length*], the output of the ***Embedding*** layer will be a 3D tensor of shape [*batch size, window length, embedding size*].\n",
    "\n",
    "- We use a ***Dense*** layer for the output layer: it must have 39 units (***n_tokens***) because there are 39 distinct characters in the text, and we want to output a probability for each possible character (at each time step). The 39 output probabilities should sum up to 1 at each time step, so we apply the softmax activation function to the outputs of the ***Dense*** layer.\n",
    "\n",
    "- Lastly, we compile this model, using the \"***sparse_categorical_crossentropy***\" loss and a Nadam optimizer, and we train the model for several epochs, using a ***ModelCheckpoint*** callback to save the best model (in terms of validation accuracy) as training progresses.\n",
    "\n",
    "> #### **TIP**\n",
    ">  If you are running this code on Colab with a GPU activated, then training should take roughly one to two hours. You can reduce the number of epochs if you don’t want to wait that long, but of course the model’s accuracy will probably be lower. If the Colab session times out, make sure to reconnect quickly, or else the Colab runtime will be destroyed.\n",
    "\n",
    "This model does not handle text preprocessing, so let’s wrap it in a final model containing the ***tf.keras.layers.TextVectorization*** layer as the first layer, plus a ***tf.keras.layers.Lambda*** layer to subtract 2 from the character IDs since we’re not using the padding and unknown tokens for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 01:17:20.007086: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "{{function_node __wrapped__Sign_device_/job:localhost/replica:0/task:0/device:GPU:0}} JIT compilation failed. [Op:Sign] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmy_shakespeare_model.keras\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/saving/saving_api.py:189\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    186\u001b[0m         is_keras_zip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_keras_zip \u001b[38;5;129;01mor\u001b[39;00m is_keras_dir \u001b[38;5;129;01mor\u001b[39;00m is_hf:\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msaving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format\u001b[38;5;241m.\u001b[39mload_model_from_hdf5(\n\u001b[1;32m    197\u001b[0m         filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m\n\u001b[1;32m    198\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:370\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    366\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid filename: expected a `.keras` extension. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    367\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    368\u001b[0m     )\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filepath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_model_from_fileobj\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:447\u001b[0m, in \u001b[0;36m_load_model_from_fileobj\u001b[0;34m(fileobj, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m zf\u001b[38;5;241m.\u001b[39mopen(_CONFIG_FILENAME, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    445\u001b[0m     config_json \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m--> 447\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43m_model_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m all_filenames \u001b[38;5;241m=\u001b[39m zf\u001b[38;5;241m.\u001b[39mnamelist()\n\u001b[1;32m    452\u001b[0m extract_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:436\u001b[0m, in \u001b[0;36m_model_from_config\u001b[0;34m(config_json, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# Construct the model from the configuration file in the archive.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ObjectSharingScope():\n\u001b[0;32m--> 436\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/saving/serialization_lib.py:718\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m custom_obj_scope, safe_mode_scope:\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 718\u001b[0m         instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    720\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    721\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m could not be deserialized properly. Please\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    722\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ensure that components that are Python object\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    726\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mconfig=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mException encountered: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    727\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/sequential.py:371\u001b[0m, in \u001b[0;36mSequential.from_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    366\u001b[0m         layer \u001b[38;5;241m=\u001b[39m saving_utils\u001b[38;5;241m.\u001b[39mmodel_from_config(\n\u001b[1;32m    367\u001b[0m             layer_config,\n\u001b[1;32m    368\u001b[0m             custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[1;32m    369\u001b[0m         )\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 371\u001b[0m         layer \u001b[38;5;241m=\u001b[39m \u001b[43mserialization_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlayer_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m     model\u001b[38;5;241m.\u001b[39madd(layer)\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m model\u001b[38;5;241m.\u001b[39m_functional\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuild_input_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m()\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m build_input_shape\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(build_input_shape, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m))\n\u001b[1;32m    381\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/saving/serialization_lib.py:730\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m build_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuild_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m build_config \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m instance\u001b[38;5;241m.\u001b[39mbuilt:\n\u001b[0;32m--> 730\u001b[0m     \u001b[43minstance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuild_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    731\u001b[0m     instance\u001b[38;5;241m.\u001b[39mbuilt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    732\u001b[0m compile_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompile_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/layer.py:481\u001b[0m, in \u001b[0;36mLayer.build_from_config\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config:\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config:\n\u001b[0;32m--> 481\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_shape\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config:\n\u001b[1;32m    483\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/layer.py:232\u001b[0m, in \u001b[0;36mLayer.__new__.<locals>.build_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_open_name_scope():\n\u001b[1;32m    231\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_path \u001b[38;5;241m=\u001b[39m current_path()\n\u001b[0;32m--> 232\u001b[0m     \u001b[43moriginal_build_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# Record build config.\u001b[39;00m\n\u001b[1;32m    234\u001b[0m signature \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(original_build_method)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:273\u001b[0m, in \u001b[0;36mRNN.build\u001b[0;34m(self, sequences_shape, initial_state_shape)\u001b[0m\n\u001b[1;32m    271\u001b[0m step_input_shape \u001b[38;5;241m=\u001b[39m (sequences_shape[\u001b[38;5;241m0\u001b[39m],) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mtuple\u001b[39m(sequences_shape[\u001b[38;5;241m2\u001b[39m:])\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell, Layer) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell\u001b[38;5;241m.\u001b[39mbuilt:\n\u001b[0;32m--> 273\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_input_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell\u001b[38;5;241m.\u001b[39mbuilt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstateful:\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/layer.py:232\u001b[0m, in \u001b[0;36mLayer.__new__.<locals>.build_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_open_name_scope():\n\u001b[1;32m    231\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_path \u001b[38;5;241m=\u001b[39m current_path()\n\u001b[0;32m--> 232\u001b[0m     \u001b[43moriginal_build_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# Record build config.\u001b[39;00m\n\u001b[1;32m    234\u001b[0m signature \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(original_build_method)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/rnn/gru.py:155\u001b[0m, in \u001b[0;36mGRUCell.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    147\u001b[0m input_dim \u001b[38;5;241m=\u001b[39m input_shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_weight(\n\u001b[1;32m    149\u001b[0m     shape\u001b[38;5;241m=\u001b[39m(input_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munits \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m),\n\u001b[1;32m    150\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkernel\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m     constraint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_constraint,\n\u001b[1;32m    154\u001b[0m )\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurrent_kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_weight\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munits\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrecurrent_kernel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecurrent_initializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregularizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecurrent_regularizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconstraint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecurrent_constraint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_bias:\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_after:\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/layer.py:575\u001b[0m, in \u001b[0;36mLayer.add_weight\u001b[0;34m(self, shape, initializer, dtype, trainable, autocast, regularizer, constraint, aggregation, overwrite_with_gradient, name)\u001b[0m\n\u001b[1;32m    573\u001b[0m initializer \u001b[38;5;241m=\u001b[39m initializers\u001b[38;5;241m.\u001b[39mget(initializer)\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, caller\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 575\u001b[0m     variable \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVariable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m        \u001b[49m\u001b[43minitializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautocast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautocast\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43maggregation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggregation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;66;03m# Will be added to layer.losses\u001b[39;00m\n\u001b[1;32m    585\u001b[0m variable\u001b[38;5;241m.\u001b[39mregularizer \u001b[38;5;241m=\u001b[39m regularizers\u001b[38;5;241m.\u001b[39mget(regularizer)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/common/variables.py:206\u001b[0m, in \u001b[0;36mVariable.__init__\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(initializer):\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_shape(shape)\n\u001b[0;32m--> 206\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_with_initializer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitializer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize(initializer)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/core.py:52\u001b[0m, in \u001b[0;36mVariable._initialize_with_initializer\u001b[0;34m(self, initializer)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_initialize_with_initializer\u001b[39m(\u001b[38;5;28mself\u001b[39m, initializer):\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minitializer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/core.py:42\u001b[0m, in \u001b[0;36mVariable._initialize\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVariable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43maggregation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_aggregation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggregation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynchronization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_synchronization\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynchronization\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/core.py:52\u001b[0m, in \u001b[0;36mVariable._initialize_with_initializer.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_initialize_with_initializer\u001b[39m(\u001b[38;5;28mself\u001b[39m, initializer):\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43minitializer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/initializers/random_initializers.py:707\u001b[0m, in \u001b[0;36mOrthogonal.__call__\u001b[0;34m(self, shape, dtype)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Make Q uniform\u001b[39;00m\n\u001b[1;32m    706\u001b[0m d \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mdiag(r)\n\u001b[0;32m--> 707\u001b[0m q \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msign\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_rows \u001b[38;5;241m<\u001b[39m num_cols:\n\u001b[1;32m    709\u001b[0m     q \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mtranspose(q)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/numpy.py:5204\u001b[0m, in \u001b[0;36msign\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   5202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors((x,)):\n\u001b[1;32m   5203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Sign()\u001b[38;5;241m.\u001b[39msymbolic_call(x)\n\u001b[0;32m-> 5204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msign\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/sparse.py:387\u001b[0m, in \u001b[0;36melementwise_unary.<locals>.sparse_wrapper\u001b[0;34m(x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mIndexedSlices(\n\u001b[1;32m    384\u001b[0m         func(x\u001b[38;5;241m.\u001b[39mvalues, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), x\u001b[38;5;241m.\u001b[39mindices, x\u001b[38;5;241m.\u001b[39mdense_shape\n\u001b[1;32m    385\u001b[0m     )\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/numpy.py:2106\u001b[0m, in \u001b[0;36msign\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   2104\u001b[0m     x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcast(tf\u001b[38;5;241m.\u001b[39msign(x), ori_dtype)\n\u001b[0;32m-> 2106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msign\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 10\u001b[0m, in \u001b[0;36m_sign_patch\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mis_integer:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcast(tf\u001b[38;5;241m.\u001b[39msign(tf\u001b[38;5;241m.\u001b[39mcast(x, tf\u001b[38;5;241m.\u001b[39mfloat32)), x\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_orig_tf_sign\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mUnknownError\u001b[0m: {{function_node __wrapped__Sign_device_/job:localhost/replica:0/task:0/device:GPU:0}} JIT compilation failed. [Op:Sign] name: "
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('my_shakespeare_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 01:17:44.919439: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "{{function_node __wrapped__Sign_device_/job:localhost/replica:0/task:0/device:GPU:0}} JIT compilation failed. [Op:Sign] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmy_shakespear_model.keras\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m shakespeare_model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[1;32m      3\u001b[0m     text_vec_layer, \n\u001b[1;32m      4\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mLambda(\u001b[38;5;28;01mlambda\u001b[39;00m x: x \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m), \u001b[38;5;66;03m# no <PAD> or <UNK> tokens\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     model\n\u001b[1;32m      6\u001b[0m ])\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/saving/saving_api.py:189\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    186\u001b[0m         is_keras_zip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_keras_zip \u001b[38;5;129;01mor\u001b[39;00m is_keras_dir \u001b[38;5;129;01mor\u001b[39;00m is_hf:\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msaving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format\u001b[38;5;241m.\u001b[39mload_model_from_hdf5(\n\u001b[1;32m    197\u001b[0m         filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m\n\u001b[1;32m    198\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:370\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    366\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid filename: expected a `.keras` extension. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    367\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    368\u001b[0m     )\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filepath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_model_from_fileobj\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:447\u001b[0m, in \u001b[0;36m_load_model_from_fileobj\u001b[0;34m(fileobj, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m zf\u001b[38;5;241m.\u001b[39mopen(_CONFIG_FILENAME, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    445\u001b[0m     config_json \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m--> 447\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43m_model_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m all_filenames \u001b[38;5;241m=\u001b[39m zf\u001b[38;5;241m.\u001b[39mnamelist()\n\u001b[1;32m    452\u001b[0m extract_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:436\u001b[0m, in \u001b[0;36m_model_from_config\u001b[0;34m(config_json, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# Construct the model from the configuration file in the archive.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ObjectSharingScope():\n\u001b[0;32m--> 436\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/saving/serialization_lib.py:718\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m custom_obj_scope, safe_mode_scope:\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 718\u001b[0m         instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    720\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    721\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m could not be deserialized properly. Please\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    722\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ensure that components that are Python object\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    726\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mconfig=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mException encountered: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    727\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/model.py:651\u001b[0m, in \u001b[0;36mModel.from_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_functional_config \u001b[38;5;129;01mand\u001b[39;00m revivable_as_functional:\n\u001b[1;32m    647\u001b[0m     \u001b[38;5;66;03m# Revive Functional model\u001b[39;00m\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;66;03m# (but not Functional subclasses with a custom __init__)\u001b[39;00m\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m functional_from_config\n\u001b[0;32m--> 651\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunctional_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;66;03m# Either the model has a custom __init__, or the config\u001b[39;00m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;66;03m# does not contain all the information necessary to\u001b[39;00m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;66;03m# revive a Functional model. This happens when the user creates\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;66;03m# In this case, we fall back to provide all config into the\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;66;03m# constructor of the class.\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py:560\u001b[0m, in \u001b[0;36mfunctional_from_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;66;03m# First, we create all layers and enqueue nodes to be processed\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_data \u001b[38;5;129;01min\u001b[39;00m functional_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayers\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 560\u001b[0m     \u001b[43mprocess_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# Then we process nodes in order of layer depth.\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m# Nodes that cannot yet be processed (if the inbound node\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# does not yet exist) are re-enqueued, and the process\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;66;03m# is repeated until all nodes are processed.\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m unprocessed_nodes:\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py:527\u001b[0m, in \u001b[0;36mfunctional_from_config.<locals>.process_layer\u001b[0;34m(layer_data)\u001b[0m\n\u001b[1;32m    523\u001b[0m     layer \u001b[38;5;241m=\u001b[39m saving_utils\u001b[38;5;241m.\u001b[39mmodel_from_config(\n\u001b[1;32m    524\u001b[0m         layer_data, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects\n\u001b[1;32m    525\u001b[0m     )\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 527\u001b[0m     layer \u001b[38;5;241m=\u001b[39m \u001b[43mserialization_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, Operation):\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected object from deserialization, expected a layer or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moperation, got a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(layer)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    534\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/saving/serialization_lib.py:730\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m build_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuild_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m build_config \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m instance\u001b[38;5;241m.\u001b[39mbuilt:\n\u001b[0;32m--> 730\u001b[0m     \u001b[43minstance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuild_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    731\u001b[0m     instance\u001b[38;5;241m.\u001b[39mbuilt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    732\u001b[0m compile_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompile_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/layer.py:481\u001b[0m, in \u001b[0;36mLayer.build_from_config\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config:\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config:\n\u001b[0;32m--> 481\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_shape\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config:\n\u001b[1;32m    483\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/layer.py:232\u001b[0m, in \u001b[0;36mLayer.__new__.<locals>.build_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_open_name_scope():\n\u001b[1;32m    231\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_path \u001b[38;5;241m=\u001b[39m current_path()\n\u001b[0;32m--> 232\u001b[0m     \u001b[43moriginal_build_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# Record build config.\u001b[39;00m\n\u001b[1;32m    234\u001b[0m signature \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(original_build_method)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:273\u001b[0m, in \u001b[0;36mRNN.build\u001b[0;34m(self, sequences_shape, initial_state_shape)\u001b[0m\n\u001b[1;32m    271\u001b[0m step_input_shape \u001b[38;5;241m=\u001b[39m (sequences_shape[\u001b[38;5;241m0\u001b[39m],) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mtuple\u001b[39m(sequences_shape[\u001b[38;5;241m2\u001b[39m:])\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell, Layer) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell\u001b[38;5;241m.\u001b[39mbuilt:\n\u001b[0;32m--> 273\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_input_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell\u001b[38;5;241m.\u001b[39mbuilt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstateful:\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/layer.py:232\u001b[0m, in \u001b[0;36mLayer.__new__.<locals>.build_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_open_name_scope():\n\u001b[1;32m    231\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_path \u001b[38;5;241m=\u001b[39m current_path()\n\u001b[0;32m--> 232\u001b[0m     \u001b[43moriginal_build_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# Record build config.\u001b[39;00m\n\u001b[1;32m    234\u001b[0m signature \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(original_build_method)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/rnn/gru.py:155\u001b[0m, in \u001b[0;36mGRUCell.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    147\u001b[0m input_dim \u001b[38;5;241m=\u001b[39m input_shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_weight(\n\u001b[1;32m    149\u001b[0m     shape\u001b[38;5;241m=\u001b[39m(input_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munits \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m),\n\u001b[1;32m    150\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkernel\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m     constraint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_constraint,\n\u001b[1;32m    154\u001b[0m )\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurrent_kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_weight\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munits\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrecurrent_kernel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecurrent_initializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregularizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecurrent_regularizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconstraint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecurrent_constraint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_bias:\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_after:\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/layer.py:575\u001b[0m, in \u001b[0;36mLayer.add_weight\u001b[0;34m(self, shape, initializer, dtype, trainable, autocast, regularizer, constraint, aggregation, overwrite_with_gradient, name)\u001b[0m\n\u001b[1;32m    573\u001b[0m initializer \u001b[38;5;241m=\u001b[39m initializers\u001b[38;5;241m.\u001b[39mget(initializer)\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, caller\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 575\u001b[0m     variable \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVariable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m        \u001b[49m\u001b[43minitializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautocast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautocast\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43maggregation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggregation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;66;03m# Will be added to layer.losses\u001b[39;00m\n\u001b[1;32m    585\u001b[0m variable\u001b[38;5;241m.\u001b[39mregularizer \u001b[38;5;241m=\u001b[39m regularizers\u001b[38;5;241m.\u001b[39mget(regularizer)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/common/variables.py:206\u001b[0m, in \u001b[0;36mVariable.__init__\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(initializer):\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_shape(shape)\n\u001b[0;32m--> 206\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_with_initializer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitializer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize(initializer)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/core.py:52\u001b[0m, in \u001b[0;36mVariable._initialize_with_initializer\u001b[0;34m(self, initializer)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_initialize_with_initializer\u001b[39m(\u001b[38;5;28mself\u001b[39m, initializer):\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minitializer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/core.py:42\u001b[0m, in \u001b[0;36mVariable._initialize\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVariable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43maggregation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_aggregation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggregation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynchronization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_synchronization\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynchronization\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/core.py:52\u001b[0m, in \u001b[0;36mVariable._initialize_with_initializer.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_initialize_with_initializer\u001b[39m(\u001b[38;5;28mself\u001b[39m, initializer):\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43minitializer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/initializers/random_initializers.py:707\u001b[0m, in \u001b[0;36mOrthogonal.__call__\u001b[0;34m(self, shape, dtype)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Make Q uniform\u001b[39;00m\n\u001b[1;32m    706\u001b[0m d \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mdiag(r)\n\u001b[0;32m--> 707\u001b[0m q \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msign\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_rows \u001b[38;5;241m<\u001b[39m num_cols:\n\u001b[1;32m    709\u001b[0m     q \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mtranspose(q)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/numpy.py:5204\u001b[0m, in \u001b[0;36msign\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   5202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors((x,)):\n\u001b[1;32m   5203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Sign()\u001b[38;5;241m.\u001b[39msymbolic_call(x)\n\u001b[0;32m-> 5204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msign\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/sparse.py:387\u001b[0m, in \u001b[0;36melementwise_unary.<locals>.sparse_wrapper\u001b[0;34m(x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mIndexedSlices(\n\u001b[1;32m    384\u001b[0m         func(x\u001b[38;5;241m.\u001b[39mvalues, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), x\u001b[38;5;241m.\u001b[39mindices, x\u001b[38;5;241m.\u001b[39mdense_shape\n\u001b[1;32m    385\u001b[0m     )\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/numpy.py:2106\u001b[0m, in \u001b[0;36msign\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   2104\u001b[0m     x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcast(tf\u001b[38;5;241m.\u001b[39msign(x), ori_dtype)\n\u001b[0;32m-> 2106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msign\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 10\u001b[0m, in \u001b[0;36m_sign_patch\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mis_integer:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcast(tf\u001b[38;5;241m.\u001b[39msign(tf\u001b[38;5;241m.\u001b[39mcast(x, tf\u001b[38;5;241m.\u001b[39mfloat32)), x\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_orig_tf_sign\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mUnknownError\u001b[0m: {{function_node __wrapped__Sign_device_/job:localhost/replica:0/task:0/device:GPU:0}} JIT compilation failed. [Op:Sign] name: "
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('my_shakespear_model.keras')\n",
    "shakespeare_model = tf.keras.Sequential([\n",
    "    text_vec_layer, \n",
    "    tf.keras.layers.Lambda(lambda x: x - 2), # no <PAD> or <UNK> tokens\n",
    "    model\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction failed: Graph execution error:\n",
      "\n",
      "Detected at node sequential_2_1/sequential_1_1/gru_1_1/split_1 defined at (most recent call last):\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "\n",
      "  File \"/tmp/ipykernel_7933/85086209.py\", line 11, in <module>\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 566, in predict\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 260, in one_step_on_data_distributed\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 250, in one_step_on_data\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 105, in predict_step\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/layer.py\", line 936, in __call__\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/operation.py\", line 58, in __call__\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/sequential.py\", line 220, in call\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py\", line 183, in call\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/function.py\", line 177, in _run_through_graph\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py\", line 648, in call\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/layer.py\", line 936, in __call__\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/operation.py\", line 58, in __call__\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/sequential.py\", line 220, in call\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py\", line 183, in call\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/function.py\", line 177, in _run_through_graph\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py\", line 648, in call\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/layer.py\", line 936, in __call__\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/operation.py\", line 58, in __call__\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/rnn/gru.py\", line 601, in call\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py\", line 406, in call\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/rnn/gru.py\", line 568, in inner_loop\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/rnn.py\", line 484, in gru\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/rnn.py\", line 704, in _cudnn_gru\n",
      "\n",
      "Detected at node sequential_2_1/sequential_1_1/gru_1_1/split_1 defined at (most recent call last):\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "\n",
      "  File \"/tmp/ipykernel_7933/85086209.py\", line 11, in <module>\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 566, in predict\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 260, in one_step_on_data_distributed\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 250, in one_step_on_data\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 105, in predict_step\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/layer.py\", line 936, in __call__\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/operation.py\", line 58, in __call__\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/sequential.py\", line 220, in call\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py\", line 183, in call\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/function.py\", line 177, in _run_through_graph\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py\", line 648, in call\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/layer.py\", line 936, in __call__\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/operation.py\", line 58, in __call__\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/sequential.py\", line 220, in call\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py\", line 183, in call\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/function.py\", line 177, in _run_through_graph\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py\", line 648, in call\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/layer.py\", line 936, in __call__\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/operation.py\", line 58, in __call__\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/rnn/gru.py\", line 601, in call\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py\", line 406, in call\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/rnn/gru.py\", line 568, in inner_loop\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/rnn.py\", line 484, in gru\n",
      "\n",
      "  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/rnn.py\", line 704, in _cudnn_gru\n",
      "\n",
      "2 root error(s) found.\n",
      "  (0) INVALID_ARGUMENT:  -input rank(-1) <= split_dim < input rank (1), but got 1\n",
      "\t [[{{node sequential_2_1/sequential_1_1/gru_1_1/split_1}}]]\n",
      "\t [[sequential_2_1/text_vectorization_1/UnicodeSplit/UnicodeEncode/UnicodeEncode/RaggedFromTensor/Reshape/_14]]\n",
      "  (1) INVALID_ARGUMENT:  -input rank(-1) <= split_dim < input rank (1), but got 1\n",
      "\t [[{{node sequential_2_1/sequential_1_1/gru_1_1/split_1}}]]\n",
      "0 successful operations.\n",
      "0 derived errors ignored. [Op:__inference_one_step_on_data_distributed_3421]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 00:53:51.268071: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 7858462850924658090\n",
      "2025-07-23 00:53:51.268140: I tensorflow/core/framework/local_rendezvous.cc:430] Local rendezvous send item cancelled. Key hash: 14693624709018958126\n",
      "2025-07-23 00:53:51.268162: I tensorflow/core/framework/local_rendezvous.cc:430] Local rendezvous send item cancelled. Key hash: 7994855678121314662\n",
      "2025-07-23 00:53:51.268194: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: -input rank(-1) <= split_dim < input rank (1), but got 1\n",
      "\t [[{{node sequential_2_1/sequential_1_1/gru_1_1/split_1}}]]\n",
      "\t [[sequential_2_1/text_vectorization_1/UnicodeSplit/UnicodeEncode/UnicodeEncode/RaggedFromTensor/Reshape/_14]]\n",
      "2025-07-23 00:53:51.268233: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 13125160337373383403\n",
      "2025-07-23 00:53:51.268263: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17520575464057185985\n",
      "2025-07-23 00:53:51.268282: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 1100417879316850649\n",
      "2025-07-23 00:53:51.268341: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 83755775956897098\n",
      "2025-07-23 00:53:51.268377: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 12983520393139297892\n"
     ]
    }
   ],
   "source": [
    "shakespeare_model = tf.keras.Sequential([\n",
    "    text_vec_layer, \n",
    "    tf.keras.layers.Lambda(lambda x: x - 2), # no <PAD> or <UNK> tokens\n",
    "    model\n",
    "])\n",
    "\n",
    "text = \"To be or not to b\"\n",
    "input_text = tf.constant([text])\n",
    "\n",
    "try:\n",
    "    y_proba = shakespeare_model.predict(input_text)[0, -1]\n",
    "    y_pred = tf.argmax(y_proba) # Choose the most probable character ID\n",
    "    predicted_char = text_vec_layer.get_vocabulary()[y_pred + 2]\n",
    "    print(text + predicted_char)\n",
    "except Exception as e:\n",
    "    print(\"Prediction failed:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's use it to predict the next character in a sentence:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performance seem to not performing well. Now let's use this model to pretend we're shakespeare!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Generating Fake Shakespearean Text**\n",
    "To generate new text using the char-RNN model, we could feed it some text, make the model predict the most likely next letter, add it to the end of the text, then give the extended text to the model to guess the next letter, and so on. This is called *greedy decoding*. But in practice this often leads to the same words being repeated over and over again. Instead, we can sample the next character randomly, with a probability equal to the estimated probability, using TensorFlow’s ***tf.random.categorical()*** function. This will generate more diverse and interesting text. The **categorical()** function samples random class indices, given the class log probabilities (logits). For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 00:54:21.555409: W external/local_xla/xla/service/gpu/llvm_gpu_backend/nvptx_backend.cc:110] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "error: libdevice not found at ./libdevice.10.bc\n",
      "2025-07-23 00:54:21.556778: E tensorflow/compiler/mlir/tools/kernel_gen/tf_framework_c_interface.cc:227] INTERNAL: Generating device code failed.\n",
      "2025-07-23 00:54:21.559431: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "{{function_node __wrapped__Log_device_/job:localhost/replica:0/task:0/device:GPU:0}} JIT compilation failed. [Op:Log]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m log_probas \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# probas = 50%, 40%, and 10%\u001b[39;00m\n\u001b[1;32m      2\u001b[0m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mset_seed(\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m      3\u001b[0m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mcategorical(log_probas, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:88\u001b[0m, in \u001b[0;36mweak_tensor_unary_op_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     87\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     90\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/tensorflow/python/ops/gen_math_ops.py:5689\u001b[0m, in \u001b[0;36mlog\u001b[0;34m(x, name)\u001b[0m\n\u001b[1;32m   5687\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m _result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   5688\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m-> 5689\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlog_eager_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5690\u001b[0m \u001b[43m      \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5691\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_SymbolicException:\n\u001b[1;32m   5692\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/tensorflow/python/ops/gen_math_ops.py:5733\u001b[0m, in \u001b[0;36mlog_eager_fallback\u001b[0;34m(x, name, ctx)\u001b[0m\n\u001b[1;32m   5731\u001b[0m _inputs_flat \u001b[38;5;241m=\u001b[39m [x]\n\u001b[1;32m   5732\u001b[0m _attrs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m, _attr_T)\n\u001b[0;32m-> 5733\u001b[0m _result \u001b[38;5;241m=\u001b[39m \u001b[43m_execute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_inputs_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5734\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _execute\u001b[38;5;241m.\u001b[39mmust_record_gradient():\n\u001b[1;32m   5736\u001b[0m   _execute\u001b[38;5;241m.\u001b[39mrecord_gradient(\n\u001b[1;32m   5737\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLog\u001b[39m\u001b[38;5;124m\"\u001b[39m, _inputs_flat, _attrs, _result)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnknownError\u001b[0m: {{function_node __wrapped__Log_device_/job:localhost/replica:0/task:0/device:GPU:0}} JIT compilation failed. [Op:Log]"
     ]
    }
   ],
   "source": [
    "log_probas = tf.math.log([[0.5, 0.4, 0.1]]) # probas = 50%, 40%, and 10%\n",
    "tf.random.set_seed(50)\n",
    "tf.random.categorical(log_probas, num_samples=8) # draw 8 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have more control over the diversity of the generated text, we can divide the logits by a number called the temperature, which we can tweak as we wish. A temperature close to zero favors high probability characters, while a high temperature gives all characters an equal probability. Lower temperatures are typically preferred when generating fairly rigid and precise text, such as mathematical equations, while higher temperatures are preferred when generating more diverse and creative text. The following ***next_char()*** custom helper function uses this approach to pick the next character to add to the input text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    y_proba = shakespeare_model.predict([text])[0, -1:]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0, 0]\n",
    "    return text_vec_layer.get_vocabulary()[char_id + 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can write another small helper function that will repeatedly call ***next_char()*** to get the next character and append it to the given text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wre now ready to generate some text! Let's try with different temperature values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py:241: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: keras_tensor_8\n",
      "Received: inputs=('Tensor(shape=(1,))',)\n",
      "  warnings.warn(msg)\n",
      "2025-07-23 00:55:47.735680: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 7858462850924658090\n",
      "2025-07-23 00:55:47.735795: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 13125160337373383403\n",
      "2025-07-23 00:55:47.735827: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17520575464057185985\n",
      "2025-07-23 00:55:47.735854: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 1100417879316850649\n",
      "2025-07-23 00:55:47.735935: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 83755775956897098\n",
      "2025-07-23 00:55:47.735975: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 12983520393139297892\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node sequential_2_1/sequential_1_1/gru_1_1/split_1 defined at (most recent call last):\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n\n  File \"/tmp/ipykernel_7933/3505144283.py\", line 3, in <module>\n\n  File \"/tmp/ipykernel_7933/3562896888.py\", line 3, in extend_text\n\n  File \"/tmp/ipykernel_7933/4243554360.py\", line 2, in next_char\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 566, in predict\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 260, in one_step_on_data_distributed\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 250, in one_step_on_data\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 105, in predict_step\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/layer.py\", line 936, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/operation.py\", line 58, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/sequential.py\", line 220, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py\", line 183, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/function.py\", line 177, in _run_through_graph\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py\", line 648, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/layer.py\", line 936, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/operation.py\", line 58, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/sequential.py\", line 220, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py\", line 183, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/function.py\", line 177, in _run_through_graph\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py\", line 648, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/layer.py\", line 936, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/operation.py\", line 58, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/rnn/gru.py\", line 601, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py\", line 406, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/rnn/gru.py\", line 568, in inner_loop\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/rnn.py\", line 484, in gru\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/rnn.py\", line 704, in _cudnn_gru\n\nDetected at node sequential_2_1/sequential_1_1/gru_1_1/split_1 defined at (most recent call last):\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n\n  File \"/tmp/ipykernel_7933/3505144283.py\", line 3, in <module>\n\n  File \"/tmp/ipykernel_7933/3562896888.py\", line 3, in extend_text\n\n  File \"/tmp/ipykernel_7933/4243554360.py\", line 2, in next_char\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 566, in predict\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 260, in one_step_on_data_distributed\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 250, in one_step_on_data\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 105, in predict_step\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/layer.py\", line 936, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/operation.py\", line 58, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/sequential.py\", line 220, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py\", line 183, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/function.py\", line 177, in _run_through_graph\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py\", line 648, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/layer.py\", line 936, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/operation.py\", line 58, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/sequential.py\", line 220, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py\", line 183, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/function.py\", line 177, in _run_through_graph\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py\", line 648, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/layer.py\", line 936, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/operation.py\", line 58, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/rnn/gru.py\", line 601, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py\", line 406, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/rnn/gru.py\", line 568, in inner_loop\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/rnn.py\", line 484, in gru\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/rnn.py\", line 704, in _cudnn_gru\n\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  -input rank(-1) <= split_dim < input rank (1), but got 1\n\t [[{{node sequential_2_1/sequential_1_1/gru_1_1/split_1}}]]\n\t [[sequential_2_1/text_vectorization_1/UnicodeSplit/UnicodeEncode/UnicodeEncode/RaggedFromTensor/Reshape/_14]]\n  (1) INVALID_ARGUMENT:  -input rank(-1) <= split_dim < input rank (1), but got 1\n\t [[{{node sequential_2_1/sequential_1_1/gru_1_1/split_1}}]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_one_step_on_data_distributed_3644]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mset_seed(\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m      2\u001b[0m input_text \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTo be or not to be\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mextend_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m extended_string \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m      5\u001b[0m extended_string\n",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m, in \u001b[0;36mextend_text\u001b[0;34m(text, n_chars, temperature)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextend_text\u001b[39m(text, n_chars\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_chars):\n\u001b[0;32m----> 3\u001b[0m         text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mnext_char\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m, in \u001b[0;36mnext_char\u001b[0;34m(text, temperature)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mnext_char\u001b[39m(text, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     y_proba \u001b[38;5;241m=\u001b[39m \u001b[43mshakespeare_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m      3\u001b[0m     rescaled_logits \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mlog(y_proba) \u001b[38;5;241m/\u001b[39m temperature\n\u001b[1;32m      4\u001b[0m     char_id \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mcategorical(rescaled_logits, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node sequential_2_1/sequential_1_1/gru_1_1/split_1 defined at (most recent call last):\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n\n  File \"/tmp/ipykernel_7933/3505144283.py\", line 3, in <module>\n\n  File \"/tmp/ipykernel_7933/3562896888.py\", line 3, in extend_text\n\n  File \"/tmp/ipykernel_7933/4243554360.py\", line 2, in next_char\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 566, in predict\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 260, in one_step_on_data_distributed\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 250, in one_step_on_data\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 105, in predict_step\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/layer.py\", line 936, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/operation.py\", line 58, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/sequential.py\", line 220, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py\", line 183, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/function.py\", line 177, in _run_through_graph\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py\", line 648, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/layer.py\", line 936, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/operation.py\", line 58, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/sequential.py\", line 220, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py\", line 183, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/function.py\", line 177, in _run_through_graph\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py\", line 648, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/layer.py\", line 936, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/operation.py\", line 58, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/rnn/gru.py\", line 601, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py\", line 406, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/rnn/gru.py\", line 568, in inner_loop\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/rnn.py\", line 484, in gru\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/rnn.py\", line 704, in _cudnn_gru\n\nDetected at node sequential_2_1/sequential_1_1/gru_1_1/split_1 defined at (most recent call last):\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n\n  File \"/tmp/ipykernel_7933/3505144283.py\", line 3, in <module>\n\n  File \"/tmp/ipykernel_7933/3562896888.py\", line 3, in extend_text\n\n  File \"/tmp/ipykernel_7933/4243554360.py\", line 2, in next_char\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 566, in predict\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 260, in one_step_on_data_distributed\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 250, in one_step_on_data\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 105, in predict_step\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/layer.py\", line 936, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/operation.py\", line 58, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/sequential.py\", line 220, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py\", line 183, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/function.py\", line 177, in _run_through_graph\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py\", line 648, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/layer.py\", line 936, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/operation.py\", line 58, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/sequential.py\", line 220, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py\", line 183, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/function.py\", line 177, in _run_through_graph\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py\", line 648, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/layer.py\", line 936, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/operation.py\", line 58, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/rnn/gru.py\", line 601, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py\", line 406, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/rnn/gru.py\", line 568, in inner_loop\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/rnn.py\", line 484, in gru\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/rnn.py\", line 704, in _cudnn_gru\n\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  -input rank(-1) <= split_dim < input rank (1), but got 1\n\t [[{{node sequential_2_1/sequential_1_1/gru_1_1/split_1}}]]\n\t [[sequential_2_1/text_vectorization_1/UnicodeSplit/UnicodeEncode/UnicodeEncode/RaggedFromTensor/Reshape/_14]]\n  (1) INVALID_ARGUMENT:  -input rank(-1) <= split_dim < input rank (1), but got 1\n\t [[{{node sequential_2_1/sequential_1_1/gru_1_1/split_1}}]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_one_step_on_data_distributed_3644]"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(50)\n",
    "input_text = tf.constant(['To be or not to be'])\n",
    "output = extend_text(input_text, temperature=0.01)\n",
    "extended_string = output.numpy().item()\n",
    "extended_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops! It only outputs space \" \" prediction let's maybe try with different temperature values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 00:55:54.546412: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: -input rank(-1) <= split_dim < input rank (1), but got 1\n",
      "\t [[{{node sequential_2_1/sequential_1_1/gru_1_1/split_1}}]]\n",
      "2025-07-23 00:55:54.546503: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 7858462850924658090\n",
      "2025-07-23 00:55:54.546532: I tensorflow/core/framework/local_rendezvous.cc:430] Local rendezvous send item cancelled. Key hash: 7994855678121314662\n",
      "2025-07-23 00:55:54.546569: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 13125160337373383403\n",
      "2025-07-23 00:55:54.546620: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 17520575464057185985\n",
      "2025-07-23 00:55:54.546651: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 1100417879316850649\n",
      "2025-07-23 00:55:54.546712: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 83755775956897098\n",
      "2025-07-23 00:55:54.546742: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 12983520393139297892\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node sequential_2_1/sequential_1_1/gru_1_1/split_1 defined at (most recent call last):\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n\n  File \"/tmp/ipykernel_7933/3505144283.py\", line 3, in <module>\n\n  File \"/tmp/ipykernel_7933/3562896888.py\", line 3, in extend_text\n\n  File \"/tmp/ipykernel_7933/4243554360.py\", line 2, in next_char\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 566, in predict\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 260, in one_step_on_data_distributed\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 250, in one_step_on_data\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 105, in predict_step\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/layer.py\", line 936, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/operation.py\", line 58, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/sequential.py\", line 220, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py\", line 183, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/function.py\", line 177, in _run_through_graph\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py\", line 648, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/layer.py\", line 936, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/operation.py\", line 58, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/sequential.py\", line 220, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py\", line 183, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/function.py\", line 177, in _run_through_graph\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py\", line 648, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/layer.py\", line 936, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/operation.py\", line 58, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/rnn/gru.py\", line 601, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py\", line 406, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/rnn/gru.py\", line 568, in inner_loop\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/rnn.py\", line 484, in gru\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/rnn.py\", line 704, in _cudnn_gru\n\nDetected at node sequential_2_1/sequential_1_1/gru_1_1/split_1 defined at (most recent call last):\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n\n  File \"/tmp/ipykernel_7933/3505144283.py\", line 3, in <module>\n\n  File \"/tmp/ipykernel_7933/3562896888.py\", line 3, in extend_text\n\n  File \"/tmp/ipykernel_7933/4243554360.py\", line 2, in next_char\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 566, in predict\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 260, in one_step_on_data_distributed\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 250, in one_step_on_data\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 105, in predict_step\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/layer.py\", line 936, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/operation.py\", line 58, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/sequential.py\", line 220, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py\", line 183, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/function.py\", line 177, in _run_through_graph\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py\", line 648, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/layer.py\", line 936, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/operation.py\", line 58, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/sequential.py\", line 220, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py\", line 183, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/function.py\", line 177, in _run_through_graph\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py\", line 648, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/layer.py\", line 936, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/operation.py\", line 58, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/rnn/gru.py\", line 601, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py\", line 406, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/rnn/gru.py\", line 568, in inner_loop\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/rnn.py\", line 484, in gru\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/rnn.py\", line 704, in _cudnn_gru\n\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  -input rank(-1) <= split_dim < input rank (1), but got 1\n\t [[{{node sequential_2_1/sequential_1_1/gru_1_1/split_1}}]]\n\t [[sequential_2_1/text_vectorization_1/UnicodeSplit/UnicodeEncode/UnicodeEncode/RaggedFromTensor/Reshape/_14]]\n  (1) INVALID_ARGUMENT:  -input rank(-1) <= split_dim < input rank (1), but got 1\n\t [[{{node sequential_2_1/sequential_1_1/gru_1_1/split_1}}]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_one_step_on_data_distributed_3644]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mset_seed(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m      2\u001b[0m input_text \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTo be or not to be\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mextend_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m extended_string \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m      5\u001b[0m extended_string\n",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m, in \u001b[0;36mextend_text\u001b[0;34m(text, n_chars, temperature)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextend_text\u001b[39m(text, n_chars\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_chars):\n\u001b[0;32m----> 3\u001b[0m         text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mnext_char\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m, in \u001b[0;36mnext_char\u001b[0;34m(text, temperature)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mnext_char\u001b[39m(text, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     y_proba \u001b[38;5;241m=\u001b[39m \u001b[43mshakespeare_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m      3\u001b[0m     rescaled_logits \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mlog(y_proba) \u001b[38;5;241m/\u001b[39m temperature\n\u001b[1;32m      4\u001b[0m     char_id \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mcategorical(rescaled_logits, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node sequential_2_1/sequential_1_1/gru_1_1/split_1 defined at (most recent call last):\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n\n  File \"/tmp/ipykernel_7933/3505144283.py\", line 3, in <module>\n\n  File \"/tmp/ipykernel_7933/3562896888.py\", line 3, in extend_text\n\n  File \"/tmp/ipykernel_7933/4243554360.py\", line 2, in next_char\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 566, in predict\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 260, in one_step_on_data_distributed\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 250, in one_step_on_data\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 105, in predict_step\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/layer.py\", line 936, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/operation.py\", line 58, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/sequential.py\", line 220, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py\", line 183, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/function.py\", line 177, in _run_through_graph\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py\", line 648, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/layer.py\", line 936, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/operation.py\", line 58, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/sequential.py\", line 220, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py\", line 183, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/function.py\", line 177, in _run_through_graph\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py\", line 648, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/layer.py\", line 936, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/operation.py\", line 58, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/rnn/gru.py\", line 601, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py\", line 406, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/rnn/gru.py\", line 568, in inner_loop\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/rnn.py\", line 484, in gru\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/rnn.py\", line 704, in _cudnn_gru\n\nDetected at node sequential_2_1/sequential_1_1/gru_1_1/split_1 defined at (most recent call last):\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n\n  File \"/tmp/ipykernel_7933/3505144283.py\", line 3, in <module>\n\n  File \"/tmp/ipykernel_7933/3562896888.py\", line 3, in extend_text\n\n  File \"/tmp/ipykernel_7933/4243554360.py\", line 2, in next_char\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 566, in predict\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 260, in one_step_on_data_distributed\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 250, in one_step_on_data\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 105, in predict_step\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/layer.py\", line 936, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/operation.py\", line 58, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/sequential.py\", line 220, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py\", line 183, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/function.py\", line 177, in _run_through_graph\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py\", line 648, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/layer.py\", line 936, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/operation.py\", line 58, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/sequential.py\", line 220, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py\", line 183, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/function.py\", line 177, in _run_through_graph\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py\", line 648, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/layer.py\", line 936, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/operation.py\", line 58, in __call__\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/rnn/gru.py\", line 601, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py\", line 406, in call\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/rnn/gru.py\", line 568, in inner_loop\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/rnn.py\", line 484, in gru\n\n  File \"/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/rnn.py\", line 704, in _cudnn_gru\n\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  -input rank(-1) <= split_dim < input rank (1), but got 1\n\t [[{{node sequential_2_1/sequential_1_1/gru_1_1/split_1}}]]\n\t [[sequential_2_1/text_vectorization_1/UnicodeSplit/UnicodeEncode/UnicodeEncode/RaggedFromTensor/Reshape/_14]]\n  (1) INVALID_ARGUMENT:  -input rank(-1) <= split_dim < input rank (1), but got 1\n\t [[{{node sequential_2_1/sequential_1_1/gru_1_1/split_1}}]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_one_step_on_data_distributed_3644]"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "input_text = tf.constant(['To be or not to be'])\n",
    "output = extend_text(input_text, temperature=1)\n",
    "extended_string = output.numpy().item()\n",
    "extended_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 266ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b\"To be or not to begb.tznogllookzk!l:eg3c& ;c-qs!pg:3?av3az's-esvztpa\""
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(50)\n",
    "input_text = tf.constant(['To be or not to be'])\n",
    "output = extend_text(input_text, temperature=100)\n",
    "extended_string = output.numpy().item()\n",
    "extended_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shakespeare seems to be suffering from a heatwave. To generate more convincing text, a common technique is to sample only from the top k characters, or only from the smallest set of top characters whose total probability exceeds some threshold (this is called *nucleus sampling*). Alternatively, you could try using *beam search*, which we will discuss later in this chapter, or using more GRU layers and more neurons per layer, training for longer, and adding some regularization if needed. Also note that the model is currently incapable of learning patterns longer than ***length***, which is just 100 characters. You could try making this window larger, but it will also make training harder, and even LSTM and GRU cells cannot handle very long sequences. An alternative approach is to use a stateful RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Stateful RNN**\n",
    "Until now, we have only used *stateless RNNs*: at each training iteration the model starts with a hidden state full of zeros, then it updates this state at each time step, and after the last time step, it throws it away as it is not needed anymore. What if we instructed the RNN to preserve this final state after processing a training batch and use it as the initial state for the next training batch? This way the model could learn long-term patterns despite only backpropagating through short sequences. This is called a *stateful RNN*. Let’s go over how to build one.\n",
    "\n",
    "First, note that a stateful RNN only makes sense if each input sequence in a batch starts exactly where the corresponding sequence in the previous batch left off. So the first thing we need to do to build a stateful RNN is to use sequential and nonoverlapping input sequences (rather than the shuffled and overlapping sequences we used to train stateless RNNs). When creating the ***tf.data.Dataset***, we must therefore use ***shift=length*** (instead of ***shift=1***) when calling the ***window()*** method. Moreover, we must not call the ***shuffle()*** method.\n",
    "\n",
    "Unfortunately, batching is much harder when preparing a dataset for a stateful RNN than it is for a stateless RNN. Indeed, if we were to call ***batch(32)***, then 32 consecutive windows would be put in the same batch, and the following batch would not continue each of these windows where it left off. The first batch would contain windows 1 to 32 and the second batch would contain windows 33 to 64, so if you consider, say, the first window of each batch (i.e., windows 1 and 33), you can see that they are not consecutive. The simplest solution to this problem is to just use a batch size of 1. The following ***to_dataset_for_stateful_rnn()*** custom utility function uses this strategy to prepare a dataset for a stateful RNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataset_for_stateful_rnn(sequence, length):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "    ds = ds.window(length + 1, shift=length, drop_remainder=True)\n",
    "    ds.flat_map(lambda window: window.batch(length + 1))\n",
    "    ds.batch(1, drop_remainder=True)\n",
    "    ds.map(lambda window: (window[:, :-1], window[:, 1:]))\n",
    "    return ds.prefetch(1)\n",
    "\n",
    "train = int((dataset_size*0.9)//1)\n",
    "remain = (dataset_size - train) // 2\n",
    "tf.random.set_seed(50)\n",
    "stateful_train_set = to_dataset(encoded[:train], length=length, seed=50)\n",
    "stateful_valid_set = to_dataset(encoded[train:(train+remain)], length=length)\n",
    "stateful_test_set = to_dataset(encoded[-remain:], length=length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Figure 16-2](#fig162) summarizes the main steps of this function.\n",
    "\n",
    "<span id=fig162></span>\n",
    "![Preparing a dataset of consecutive sequence fragments for a stateful RNN](f162.png)\n",
    "\n",
    "Batching is harder, but it is not impossible. For example, we could chop Shakespeare’s text into 32 texts of equal length, create one dataset of consecutive input sequences for each of them, and finally use ***tf.data.Dataset.zip(datasets).map(lambda *windows: tf.stack(windows))*** to create proper consecutive batches, where the n input sequence in a batch starts off exactly where the n input sequence ended in the previous batch (see the notebook for the full code).\n",
    "\n",
    "Now, let’s create the stateful RNN. We need to set the ***stateful*** argument to ***True*** when creating each recurrent layer, and because the stateful RNN needs to know the batch size (since it will preserve a state for each input sequence in the batch). Therefore we must set the ***batch_input_shape*** argument in the first layer. Note that we can leave the second dimension unspecified, since the input sequences could have any length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "_orig_sign = K.sign\n",
    "def _patched_sign(x):\n",
    "    if x.dtype.is_integer:\n",
    "        return tf.cast(tf.sign(tf.cast(x, tf.float32)), x.dtype)\n",
    "    return _orig_sign(x)\n",
    "K.sign = _patched_sign\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 01:04:57.531176: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "{{function_node __wrapped__Sign_device_/job:localhost/replica:0/task:0/device:GPU:0}} JIT compilation failed. [Op:Sign] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mInput(batch_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m), dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m     13\u001b[0m x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mEmbedding(input_dim\u001b[38;5;241m=\u001b[39mn_tokens, output_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)(inputs)\n\u001b[0;32m---> 14\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGRU\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstateful\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m outputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(n_tokens, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)(x)\n\u001b[1;32m     17\u001b[0m model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mModel(inputs, outputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/numpy.py:2106\u001b[0m, in \u001b[0;36msign\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   2104\u001b[0m     x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcast(tf\u001b[38;5;241m.\u001b[39msign(x), ori_dtype)\n\u001b[0;32m-> 2106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msign\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mUnknownError\u001b[0m: {{function_node __wrapped__Sign_device_/job:localhost/replica:0/task:0/device:GPU:0}} JIT compilation failed. [Op:Sign] name: "
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.Input(batch_shape=(1, None), dtype=tf.int32)\n",
    "x = tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16)(inputs)\n",
    "x = tf.keras.layers.GRU(128, return_sequences=True, stateful=True, )(x)\n",
    "outputs = tf.keras.layers.Dense(n_tokens, activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of each epoch, we need to reset the states before we go back to the beginning of the text. For this, we can use a small custom Keras callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResetStatesCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        for layer in self.model.layers:\n",
    "            if hasattr(layer, 'reset_states'):\n",
    "                layer.reset_states()\n",
    "        print(f\"\\nStates reset at the end of epoch {epoch + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can compile the model and train it using our callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling GRU.call().\n\n\u001b[1mInput tensor `functional_13_1/gru_13_1/ReadVariableOp:0` enters the loop with shape (1, 128), but has shape (None, 128) after one iteration. To allow the shape to vary across iterations, use the `shape_invariants` argument of tf.while_loop to specify a less-specific shape.\u001b[0m\n\nArguments received by GRU.call():\n  • sequences=tf.Tensor(shape=(None, None, 16), dtype=float32)\n  • initial_state=None\n  • mask=None\n  • training=True",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[133], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnadam\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      2\u001b[0m metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m----> 3\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(stateful_train_set, validation_data\u001b[38;5;241m=\u001b[39mstateful_valid_set, \n\u001b[0;32m      4\u001b[0m epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[ResetStatesCallback(), model_ckpt])\n",
      "File \u001b[1;32md:\\CONTENTS\\APPLICATIONS\\New Folder\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32md:\\CONTENTS\\APPLICATIONS\\New Folder\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling GRU.call().\n\n\u001b[1mInput tensor `functional_13_1/gru_13_1/ReadVariableOp:0` enters the loop with shape (1, 128), but has shape (None, 128) after one iteration. To allow the shape to vary across iterations, use the `shape_invariants` argument of tf.while_loop to specify a less-specific shape.\u001b[0m\n\nArguments received by GRU.call():\n  • sequences=tf.Tensor(shape=(None, None, 16), dtype=float32)\n  • initial_state=None\n  • mask=None\n  • training=True"
     ]
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam', \n",
    "metrics=[\"accuracy\"])\n",
    "history = model.fit(stateful_train_set, validation_data=stateful_valid_set, \n",
    "epochs=10, callbacks=[ResetStatesCallback(), model_ckpt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (1, 100)\n",
      "Target shape: (1, 100)\n",
      "Epoch 1/10\n",
      "  10038/Unknown \u001b[1m484s\u001b[0m 48ms/step - accuracy: 0.3874 - loss: 2.1183"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\keras\\src\\trainers\\epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "States reset at the end of epoch 1\n",
      "\u001b[1m10038/10038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m493s\u001b[0m 49ms/step - accuracy: 0.3874 - loss: 2.1183 - val_accuracy: 0.4993 - val_loss: 1.6770\n",
      "Epoch 2/10\n",
      "\u001b[1m10038/10038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.5236 - loss: 1.5838\n",
      "States reset at the end of epoch 2\n",
      "\u001b[1m10038/10038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m453s\u001b[0m 44ms/step - accuracy: 0.5236 - loss: 1.5838 - val_accuracy: 0.5228 - val_loss: 1.5949\n",
      "Epoch 3/10\n",
      "\u001b[1m10037/10038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.5488 - loss: 1.4885\n",
      "States reset at the end of epoch 3\n",
      "\u001b[1m10038/10038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m478s\u001b[0m 48ms/step - accuracy: 0.5488 - loss: 1.4885 - val_accuracy: 0.5321 - val_loss: 1.5615\n",
      "Epoch 4/10\n",
      "\u001b[1m10038/10038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.5595 - loss: 1.4454\n",
      "States reset at the end of epoch 4\n",
      "\u001b[1m10038/10038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m490s\u001b[0m 49ms/step - accuracy: 0.5595 - loss: 1.4454 - val_accuracy: 0.5360 - val_loss: 1.5445\n",
      "Epoch 5/10\n",
      "\u001b[1m10037/10038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.5661 - loss: 1.4195\n",
      "States reset at the end of epoch 5\n",
      "\u001b[1m10038/10038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m534s\u001b[0m 53ms/step - accuracy: 0.5661 - loss: 1.4195 - val_accuracy: 0.5408 - val_loss: 1.5324\n",
      "Epoch 6/10\n",
      "\u001b[1m10038/10038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.5704 - loss: 1.4023\n",
      "States reset at the end of epoch 6\n",
      "\u001b[1m10038/10038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m464s\u001b[0m 46ms/step - accuracy: 0.5704 - loss: 1.4023 - val_accuracy: 0.5435 - val_loss: 1.5255\n",
      "Epoch 7/10\n",
      "\u001b[1m10038/10038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.5734 - loss: 1.3904\n",
      "States reset at the end of epoch 7\n",
      "\u001b[1m10038/10038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m521s\u001b[0m 52ms/step - accuracy: 0.5734 - loss: 1.3904 - val_accuracy: 0.5458 - val_loss: 1.5242\n",
      "Epoch 8/10\n",
      "\u001b[1m10038/10038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.5752 - loss: 1.3817\n",
      "States reset at the end of epoch 8\n",
      "\u001b[1m10038/10038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m494s\u001b[0m 49ms/step - accuracy: 0.5752 - loss: 1.3817 - val_accuracy: 0.5443 - val_loss: 1.5233\n",
      "Epoch 9/10\n",
      "\u001b[1m10038/10038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.5766 - loss: 1.3750\n",
      "States reset at the end of epoch 9\n",
      "\u001b[1m10038/10038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m497s\u001b[0m 49ms/step - accuracy: 0.5766 - loss: 1.3750 - val_accuracy: 0.5430 - val_loss: 1.5197\n",
      "Epoch 10/10\n",
      "\u001b[1m10038/10038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.5783 - loss: 1.3692\n",
      "States reset at the end of epoch 10\n",
      "\u001b[1m10038/10038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m490s\u001b[0m 49ms/step - accuracy: 0.5783 - loss: 1.3692 - val_accuracy: 0.5433 - val_loss: 1.5207\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def to_dataset_for_stateful_rnn(sequence, length):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "    # 1. Create windows of size length+1, stepping by length, drop remainders\n",
    "    ds = ds.window(size=length + 1, shift=length, drop_remainder=True)\n",
    "    # 2. Turn each window into a dense tensor of shape (length+1,)\n",
    "    ds = ds.flat_map(lambda w: w.batch(length + 1))\n",
    "    # 3. Batch into groups of 1 window, forcing the batch dimension to 1\n",
    "    ds = ds.batch(1, drop_remainder=True)\n",
    "    # 4. Split into (inputs, targets); each will be shape (1, length)\n",
    "    ds = ds.map(lambda window: (window[:, :-1], window[:, 1:]))\n",
    "    return ds.prefetch(1)\n",
    "\n",
    "# Splits\n",
    "train_end = int(dataset_size * 0.9)\n",
    "val_end   = train_end + (dataset_size - train_end) // 2\n",
    "\n",
    "tf.random.set_seed(50)\n",
    "stateful_train_set = to_dataset_for_stateful_rnn(encoded[:train_end], length)\n",
    "stateful_valid_set = to_dataset_for_stateful_rnn(encoded[train_end:val_end], length)\n",
    "stateful_test_set  = to_dataset_for_stateful_rnn(encoded[val_end:], length)\n",
    "\n",
    "# Verify shapes\n",
    "for inp, tgt in stateful_train_set.take(1):\n",
    "    print(\"Input shape:\", inp.shape)   # should be (1, length)\n",
    "    print(\"Target shape:\", tgt.shape)  # should be (1, length)\n",
    "\n",
    "# Build model\n",
    "inputs = tf.keras.Input(batch_shape=(1, None), dtype=tf.int32)\n",
    "x = tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16)(inputs)\n",
    "x = tf.keras.layers.GRU(128, return_sequences=True, stateful=True)(x)\n",
    "outputs = tf.keras.layers.Dense(n_tokens, activation='softmax')(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "class ResetStatesCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        for layer in self.model.layers:\n",
    "            if hasattr(layer, 'reset_states'):\n",
    "                layer.reset_states()\n",
    "        print(f\"\\nStates reset at the end of epoch {epoch + 1}\")\n",
    "\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='nadam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    stateful_train_set,\n",
    "    validation_data=stateful_valid_set,\n",
    "    epochs=10,\n",
    "    callbacks=[ResetStatesCallback(), model_ckpt]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
      "To be or not to be\n"
     ]
    }
   ],
   "source": [
    "text = \"To be or not to b\"\n",
    "input_text = tf.constant([text])\n",
    "\n",
    "try:\n",
    "    y_proba = shakespeare_model.predict(input_text)[0, -1]\n",
    "    y_pred = tf.argmax(y_proba) # Choose the most probable character ID\n",
    "    predicted_char = text_vec_layer.get_vocabulary()[y_pred + 2]\n",
    "    print(text + predicted_char)\n",
    "except Exception as e:\n",
    "    print(\"Prediction failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'To be or not to be so\\nmay be so man the words that i shall be so\\nmay be a shame the words that i shall be so\\nmay be so'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probas = tf.math.log([[0.5, 0.4, 0.1]]) # probas = 50%, 40%, and 10%\n",
    "tf.random.set_seed(50)\n",
    "tf.random.categorical(log_probas, num_samples=8) # draw 8 samples\n",
    "\n",
    "def next_char(text, temperature=1):\n",
    "    y_proba = shakespeare_model.predict([text])[0, -1:]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0, 0]\n",
    "    return text_vec_layer.get_vocabulary()[char_id + 2]\n",
    "\n",
    "def extend_text(text, n_chars=100, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text\n",
    "\n",
    "tf.random.set_seed(50)\n",
    "input_text = tf.constant(['To be or not to be'])\n",
    "output = extend_text(input_text, temperature=0.01)\n",
    "extended_string = output.numpy().item()\n",
    "extended_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting i see now it is able to give meaningful words, but there not in proper and continuation series as they keep repeating for some of the words with no proper meaning which we will look after it on the as you move on *sentiment analysis* part in this chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### **TIP**\n",
    "> After this model is trained, it will only be possible to use it to make predictions for batches of the same size as were used during training. To avoid this restriction, create an identical stateless model, and copy the stateful model’s weights to this model.\n",
    "\n",
    "Interestingly, although a char-RNN model is just trained to predict the next character, this seemingly simple task actually requires it to learn some higher-level tasks as well. For example, to find the next character after “Great movie, I really”, it’s helpful to understand that the sentence is positive, so what follows is more likely to be the letter “l” (for “loved”) rather than “h” (for “hated”). In fact, a [2017 paper](https://homl.info/sentimentneuron)  by Alec Radford and other OpenAI researchers describes how the authors trained a big char-RNN-like model on a large dataset, and found that one of the neurons acted as an excellent sentiment analysis classifier: although the model was trained without any labels, the sentiment neuron—as they called it—reached state-of-the-art performance on sentiment analysis benchmarks. This foreshadowed and motivated unsupervised pretraining in NLP. \n",
    "\n",
    "But before we explore unsupervised pretraining, let’s turn our attention to word-level models and how to use them in a supervised fashion for sentiment analysis. In the process, you will learn how to handle sequences of variable lengths using masking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Sentiment Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating text can be fun and instructive, but in real-life projects, one of the most common applications of NLP is text classification—especially sentiment analysis. If image classification on the MNIST dataset is the “Hello world!” of computer vision, then sentiment analysis on the IMDb reviews dataset is the “Hello world!” of natural language processing. The IMDb dataset consists of 50,000 movie reviews in English (25,000 for training, 25,000 for testing) extracted from the famous [Internet Movie Database](https://imdb.com), along with a simple binary target for each review indicating whether it is negative (0) or positive (1). Just like MNIST, the IMDb reviews dataset is popular for good reasons: it is simple enough to be tackled on a laptop in a reasonable amount of time, but challenging enough to be fun and rewarding.\n",
    "\n",
    "Let’s load the IMDb dataset using the TensorFlow Datasets library (introduced in Chapter 13). We’ll use the first 90% of the training set for training, and the remaining 10% for validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_datasets in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (4.9.9)\n",
      "Requirement already satisfied: absl-py in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from tensorflow_datasets) (2.1.0)\n",
      "Requirement already satisfied: array_record>=0.5.0 in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from tensorflow_datasets) (0.7.2)\n",
      "Requirement already satisfied: dm-tree in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from tensorflow_datasets) (0.1.9)\n",
      "Requirement already satisfied: etils>=1.6.0 in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow_datasets) (1.13.0)\n",
      "Requirement already satisfied: immutabledict in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from tensorflow_datasets) (4.2.1)\n",
      "Requirement already satisfied: numpy in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from tensorflow_datasets) (2.1.3)\n",
      "Requirement already satisfied: promise in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from tensorflow_datasets) (2.3)\n",
      "Requirement already satisfied: protobuf>=3.20 in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from tensorflow_datasets) (4.21.12)\n",
      "Requirement already satisfied: psutil in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from tensorflow_datasets) (5.9.0)\n",
      "Requirement already satisfied: pyarrow in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from tensorflow_datasets) (21.0.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from tensorflow_datasets) (2.32.3)\n",
      "Requirement already satisfied: simple_parsing in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from tensorflow_datasets) (0.1.7)\n",
      "Requirement already satisfied: tensorflow-metadata in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from tensorflow_datasets) (1.17.2)\n",
      "Requirement already satisfied: termcolor in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from tensorflow_datasets) (2.1.0)\n",
      "Requirement already satisfied: toml in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from tensorflow_datasets) (0.10.2)\n",
      "Requirement already satisfied: tqdm in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from tensorflow_datasets) (4.67.1)\n",
      "Requirement already satisfied: wrapt in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from tensorflow_datasets) (1.14.1)\n",
      "Requirement already satisfied: fsspec in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow_datasets) (2025.7.0)\n",
      "Requirement already satisfied: importlib_resources in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow_datasets) (6.5.2)\n",
      "Requirement already satisfied: typing_extensions in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow_datasets) (4.12.2)\n",
      "Requirement already satisfied: zipp in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow_datasets) (3.23.0)\n",
      "Requirement already satisfied: einops in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow_datasets) (0.8.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow_datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow_datasets) (2025.4.26)\n",
      "Requirement already satisfied: attrs>=18.2.0 in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from dm-tree->tensorflow_datasets) (24.3.0)\n",
      "Requirement already satisfied: six in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from promise->tensorflow_datasets) (1.17.0)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from simple_parsing->tensorflow_datasets) (0.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Variant folder /home/jaxon/tensorflow_datasets/imdb_reviews/plain_text/1.0.0 has no dataset_info.json\n",
      "/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /home/jaxon/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Completed...:   0%|          | 0/1 [02:40<?, ? url/s]"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "raw_train_set, raw_valid_set, raw_test_set = tfds.load(\n",
    "    name=\"imdb_reviews\", \n",
    "    split=['train[:90%]', 'train[90%:]', 'test'], \n",
    "    as_supervised=True\n",
    ")\n",
    "tf.random.set_seed(50)\n",
    "train_set = raw_train_set.shuffle(5000, seed=50).batch(32).prefetch(1)\n",
    "valid_set = raw_valid_set.batch(32).prefetch(1)\n",
    "test_set = raw_test_set.batch(32).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### **TIP**\n",
    ">  Keras also includes a function for loading the IMDb dataset, if you prefer: ***tf.keras.datasets.imdb.load_data()***. The reviews are already preprocessed as sequences of word IDs.\n",
    "\n",
    "Let's inspect a few reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\n",
      "Label: 0\n",
      "I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.\n",
      "Label: 0\n",
      "Mann photographs the Alberta Rocky Mountains in a superb fashion, and Jimmy Stewart and Walter Brennan give enjoyable performances as they always seem to do. <br /><br />But come on Hollywood - a Mountie telling the people of Dawson City, Yukon to elect themselves a marshal (yes a marshal!) and to enforce the law themselves, then gunfighters battling it out on the streets for control of the town? <br /><br />Nothing even remotely resembling that happened on the Canadian side of the border during the Klondike gold rush. Mr. Mann and company appear to have mistaken Dawson City for Deadwood, the Canadian North for the American Wild West.<br /><br />Canadian viewers be prepared for a Reefer Madness type of enjoyable howl with this ludicrous plot, or, to shake your head in disgust.\n",
      "Label: 0\n",
      "This is the kind of film for a snowy Sunday afternoon when the rest of the world can go ahead with its own business as you descend into a big arm-chair and mellow for a couple of hours. Wonderful performances from Cher and Nicolas Cage (as always) gently row the plot along. There are no rapids to cross, no dangerous waters, just a warm and witty paddle through New York life at its best. A family film in every sense and one that deserves the praise it received.\n",
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "for review, label in raw_train_set.take(4):\n",
    "    print(review.numpy().decode(\"utf-8\"))\n",
    "    print(\"Label:\", label.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some reviews are easy to classify. For example, the first review includes the words “terrible movie” in the very first sentence. But in many cases things are not that simple. For example, the third review starts off positively, even though it’s ultimately a negative review (label 0).\n",
    "\n",
    "To build a model for this task, we need to preprocess the text, but this time we will chop it into words instead of characters. For this, we can use the ***tf.keras***.\n",
    "\n",
    "***layers.TextVectorization*** layer again. Note that it uses spaces to identify word boundaries, which will not work well in some languages. For example, Chinese writing does not use spaces between words, Vietnamese uses spaces even within words, and German often attaches multiple words together, without spaces. Even in English, spaces are not always the best way to tokenize text: think of “San Francisco” or “#ILoveDeepLearning”.\n",
    "\n",
    "Fortunately, there are solutions to address these issues. In a [2016 paper](https://homl.info/rarewords),  Rico Sennrich et al. from the University of Edinburgh explored several methods to tokenize and detokenize text at the subword level. This way, even if your model encounters a rare word it has never seen before, it can still reasonably guess what it means. For example, even if the model never saw the word “smartest” during training, if it learned the word “smart” and it also learned that the suffix “est” means “the most”, it can infer the meaning of “smartest”. One of the techniques the authors evaluated is *byte pair encoding* (**BPE**). BPE works by splitting the whole training set into individual characters (including spaces), then repeatedly merging the most frequent adjacent pairs until the vocabulary reaches the desired size.\n",
    "\n",
    "A [2018 paper](https://homl.info/subword)  by Taku Kudo at Google further improved subword tokenization, often removing the need for language-specific preprocessing prior to tokenization. Moreover, the paper proposed a novel regularization technique called *subword regularization*, which improves accuracy and robustness by introducing some randomness in tokenization during training: for example, “New England” may be tokenized as “New” + “England”, or “New” + “Eng” + “land”, or simply “New England” (just one token). Google’s [SentencePiece](https://github.com/google/sentencepiece) project provides an open source implementation, which is described in a [paper](https://homl.info/sentencepiece)  by Taku Kudo and John Richardson.\n",
    "\n",
    "The [TensorFlow Text](https://homl.info/tftext) library also implements various tokenization strategies, including [WordPiece](https://homl.info/wordpiece)  (a variant of BPE), and last but not least, the [Tokenizers library by Hugging Face](https://homl.info/tokenizers) implements a wide range of extremely fast tokenizers.\n",
    "\n",
    "However, for the IMDb task in English, using spaces for token boundaries should be good enough. So let’s go ahead with creating a ***TextVectorization*** layer and adapting it to the training set. We will limit the vocabulary to 1,000 tokens, including the most frequent 998 words plus a padding token and a token for unknown words, since it’s unlikely that very rare words will be important for this task, and limiting the vocabulary size will reduce the number of parameters the model needs to learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "text_vec_layer = tf.keras.layers.TextVectorization(max_tokens=vocab_size)\n",
    "text_vec_layer.adapt(train_set.map(lambda reviews, labels: reviews))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create the model and train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 611ms/step - accuracy: 0.6445 - loss: 0.6194"
     ]
    },
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode character '\\x96' in position 3182: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 20\u001b[0m\n\u001b[0;32m     12\u001b[0m early_stop \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(\n\u001b[0;32m     13\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     14\u001b[0m     patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, \n\u001b[0;32m     15\u001b[0m     restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     17\u001b[0m model_ckpt \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment_text_masking.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 20\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(train_set, validation_data\u001b[38;5;241m=\u001b[39mvalid_set, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[early_stop, model_ckpt])\n",
      "File \u001b[1;32md:\\CONTENTS\\APPLICATIONS\\New Folder\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32md:\\CONTENTS\\APPLICATIONS\\New Folder\\Lib\\encodings\\cp1252.py:19\u001b[0m, in \u001b[0;36mIncrementalEncoder.encode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m codecs\u001b[38;5;241m.\u001b[39mcharmap_encode(\u001b[38;5;28minput\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,encoding_table)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode character '\\x96' in position 3182: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "embed_size = 128\n",
    "tf.random.set_seed(50)\n",
    "model = tf.keras.Sequential([\n",
    "    text_vec_layer, \n",
    "    tf.keras.layers.Embedding(vocab_size, embed_size, mask_zero=True), \n",
    "    tf.keras.layers.GRU(128), \n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=2, \n",
    "    restore_best_weights=True)\n",
    "\n",
    "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"sentiment_text_masking.keras\", monitor=\"val_accuracy\", save_best_only=True)\n",
    "\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=10, callbacks=[early_stop, model_ckpt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 112ms/step - accuracy: 0.4976 - loss: 0.6933\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6934395432472229, 0.49939998984336853]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first layer is the ***TextVectorization*** layer we just prepared, followed by an ***Embedding*** layer that will convert word IDs into embeddings. The embedding matrix needs to have one row per token in the vocabulary (***vocab_size***) and one column per embedding dimension (this example uses 128 dimensions, but this is a hyperparameter you could tune). Next we use a ***GRU*** layer and a ***Dense*** layer with a single neuron and the sigmoid activation function, since this is a binary classification task: the model’s output will be the estimated probability that the review expresses a positive sentiment regarding the movie. We then compile the model, and we fit it on the dataset we prepared earlier for a couple of epochs (or you can train for longer to get better results).\n",
    "\n",
    "Sadly, if you run this code, you will generally find that the model fails to learn anything at all: the accuracy remains close to 50%, no better than random chance. Why is that? The reviews have different lengths, so when the ***TextVectorization*** layer converts them to sequences of token IDs, it pads the shorter sequences using the padding token (with ID 0) to make them as long as the longest sequence in the batch. As a result, most sequences end with many padding tokens—often dozens or even hundreds of them. Even though we’re using a ***GRU*** layer, which is much better than a ***SimpleRNN*** layer, its short-term memory is still not great, so when it goes through manypadding tokens, it ends up forgetting what the review was about! One solution is to feed the model with batches of equal-length sentences (which also speeds up training). Another solution is to make the RNN ignore the padding tokens. This can be done using masking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Masking**\n",
    "Making the model ignore padding tokens is trivial using Keras: simply add ***mask_zero=True*** when creating the ***Embedding*** layer. This means that padding tokens (whose ID is 0) will be ignored by all downstream layers. That’s all! If you retrain the previous model for a few epochs, you will find that the validation accuracy quickly reaches over 80%.\n",
    "\n",
    "The way this works is that the ***Embedding*** layer creates a *mask tensor* equal to ***tf.math.not_equal(inputs, 0)***: it is a Boolean tensor with the same shape as the inputs, and it is equal to ***False*** anywhere the token IDs are 0, or ***True*** otherwise. This mask tensor is then automatically propagated by the model to the next layer. If that layer’s ***call()*** method has a ***mask*** argument, then it automatically receives the mask. This allows the layer to ignore the appropriate time steps. Each layer may handle the mask differently, but in general they simply ignore masked time steps (i.e., time steps for which the mask is ***False***). For example, when a recurrent layer encounters a masked time step, it simply copies the output from the previous time step.\n",
    "\n",
    "Next, if the layer’s ***supports_masking*** attribute is ***True***, then the mask is automatically propagated to the next layer. It keeps propagating this way for as long as the layers have ***supports_masking=True***. As an example, a recurrent layer’s ***supports_ masking*** attribute is True when ***return_sequences=True***, but it’s ***False*** when ***return_sequences=False*** since there’s no need for a mask anymore in this case. So if you have a model with several recurrent layers with ***return_sequences=True***, followed by a recurrent layer with ***return_sequences=False***, then the mask will automatically propagate up to the last recurrent layer: that layer will use the mask to ignore masked steps, but it will not propagate the mask any further. Similarly, if you set ***mask_zero=True*** when creating the ***Embedding*** layer in the sentiment analysis model we just built, then the ***GRU*** layer will receive and use the mask automatically, but it will not propagate it any further, since ***return_sequences*** is not set to ***True***.\n",
    "\n",
    "> #### **TIP**\n",
    "> Some layers need to update the mask before propagating it to the next layer: they do so by implementing the ***compute_mask()*** method, which takes two arguments: the inputs and the previous mask. It then computes the updated mask and returns it. The default implementation of ***compute_mask()*** just returns the previous mask unchanged.\n",
    "\n",
    "Many Keras layers support masking: ***SimpleRNN, GRU, LSTM, Bidirectional, Dense, TimeDistributed, Add***, and a few others (all in the ***tf.keras.layers package***). However, convolutional layers (including ***Conv1D***) do not support masking—it’s not obvious how they would do so anyway.\n",
    "\n",
    "If the mask propagates all the way to the output, then it gets applied to the losses as well, so the masked time steps will not contribute to the loss (their loss will be 0). This assumes that the model outputs sequences, which is not the case in our sentiment analysis model.\n",
    "\n",
    "> #### **WARNING**\n",
    ">  The ***LSTM*** and ***GRU*** layers have an optimized implementation for GPUs, based on Nvidia’s cuDNN library. However, this implementation only supports masking if all the padding tokens are at the end of the sequences. It also requires you to use the default values for several hyperparameters: ***activation, recurrent_activation, recurrent_dropout, unroll, use_bias***, and ***reset_after***. If that’s not the case, then these layers will fall back to the (much slower) default GPU implementation.\n",
    "\n",
    "If you want to implement your own custom layer with masking support, you should add a ***mask*** argument to the ***call()*** method, and obviously make the method use the mask. Additionally, if the mask must be propagated to the next layers, then you should set ***self.supports_masking=True*** in the constructor. If the mask must be updated before it is propagated, then you must implement the ***compute_mask()*** method.\n",
    "\n",
    "If your model does not start with an ***Embedding*** layer, you may use the ***tf.keras.layers.Masking*** layer instead: by default, it sets the mask to ***tf.math.reduce_any(tf.math.not_equal(X, 0), axis=-1)***, meaning that time steps where the last dimension is full of zeros will be masked out in subsequent layers.\n",
    "\n",
    "Using masking layers and automatic mask propagation works best for simple models. It will not always work for more complex models, such as when you need to mix Conv1D layers with recurrent layers. In such cases, you will need to explicitly compute the mask and pass it to the appropriate layers, using either the functional API or the subclassing API. For example, the following model is equivalent to the previous model, except it is built using the functional API and handles masking manually. It also adds a bit of dropout since the previous model was overfitting slightly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\CONTENTS\\APPLICATIONS\\New Folder\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\CONTENTS\\APPLICATIONS\\New Folder\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
    "token_ids = text_vec_layer(inputs)\n",
    "mask = tf.keras.layers.Lambda(\n",
    "    lambda x: tf.math.not_equal(x, 0), \n",
    ") (token_ids)\n",
    "x = tf.keras.layers.Embedding(vocab_size, embed_size)(token_ids)\n",
    "x = tf.keras.layers.GRU(128, dropout=0.2)(x, mask=mask)\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\") (x)\n",
    "model = tf.keras.Model(inputs=[inputs], outputs=[outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last approach to masking is to feed the model with ragged tensors.  In practice, all you need to do is to set ***ragged=True*** when creating the ***TextVectorization*** layer, so that the input sequences are represented as ragged tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[86, 18], [11, 7, 1, 116, 217]]>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec_layer_ragged = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=vocab_size, ragged=True\n",
    ")\n",
    "text_vec_layer_ragged.adapt(train_set.map(lambda reviews, labels: reviews))\n",
    "text_vec_layer_ragged([\"Great movie!\", \"This is DiCaprio's best role.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this ragged tensor representation with the regular tensor representation, which uses padding tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 5), dtype=int64, numpy=\n",
       "array([[ 86,  18,   0,   0,   0],\n",
       "       [ 11,   7,   1, 116, 217]], dtype=int64)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vec_layer([\"Great movie!\", \"This is DiCaprio's best role.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally a full picture of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "if hasattr(sys.stdout, \"reconfigure\"):\n",
    "    sys.stdout.reconfigure(encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\CONTENTS\\APPLICATIONS\\New Folder\\keras\\src\\models\\functional.py:238: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['keras_tensor_103']\n",
      "Received: inputs=Tensor(shape=(None,))\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling GRU.call().\n\n\u001b[1mCannot index into an inner ragged dimension.\u001b[0m\n\nArguments received by GRU.call():\n  • sequences=tf.Tensor(shape=(None, None, 128), dtype=float32)\n  • initial_state=None\n  • mask=None\n  • training=True",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 22\u001b[0m\n\u001b[0;32m     16\u001b[0m model_ckpt \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment_text_masking.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnadam\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     20\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 22\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(train_set, validation_data\u001b[38;5;241m=\u001b[39mvalid_set, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, \n\u001b[0;32m     23\u001b[0m                     callbacks\u001b[38;5;241m=\u001b[39m[early_stop, model_ckpt], verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32md:\\CONTENTS\\APPLICATIONS\\New Folder\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32md:\\CONTENTS\\APPLICATIONS\\New Folder\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling GRU.call().\n\n\u001b[1mCannot index into an inner ragged dimension.\u001b[0m\n\nArguments received by GRU.call():\n  • sequences=tf.Tensor(shape=(None, None, 128), dtype=float32)\n  • initial_state=None\n  • mask=None\n  • training=True"
     ]
    }
   ],
   "source": [
    "embed_size = 128\n",
    "tf.random.set_seed(50)\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
    "token_ids = text_vec_layer_ragged(inputs)\n",
    "x = tf.keras.layers.Embedding(vocab_size, embed_size)(token_ids)\n",
    "x = tf.keras.layers.GRU(128, dropout=0.2)(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\") (x)\n",
    "model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=2, \n",
    "    restore_best_weights=True)\n",
    "\n",
    "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"sentiment_text_masking.keras\", monitor=\"val_accuracy\", save_best_only=True)\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=10, \n",
    "                    callbacks=[early_stop, model_ckpt], verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras’s recurrent layers have built-in support for ragged tensors, so there’s nothing else you need to do: just use this ***TextVectorization*** layer in your model. There’s no need to pass ***mask_zero=True*** or handle masks explicitly—it’s all implemented for you. That’s convenient! However, as of early 2022, the support for ragged tensors in Keras is still fairly recent, so there are a few rough edges. For example, it is currently not possible to use ragged tensors as targets when running on the GPU (but this may be resolved by the time you read these lines).\n",
    "\n",
    "Whichever masking approach you prefer, after training this model for a few epochs, it will become quite good at judging whether a review is positive or not. If you use the ***tf.keras.callbacks.TensorBoard()*** callback, you can visualize the embeddings in TensorBoard as they are being learned: it is fascinating to see words like “awesome” and “amazing” gradually cluster on one side of the embedding space, while words like “awful” and “terrible” cluster on the other side. Some words are not as positive as you might expect (at least with this model), such as the word “good”, presumably because many negative reviews contain the phrase “not good”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Reusing Pretrained Embeddings and Language Models**\n",
    "It’s impressive that the model is able to learn useful word embeddings based on just 25,000 movie reviews. Imagine how good the embeddings would be if we had billions of reviews to train on! Unfortunately, we don’t, but perhaps we can reuse word embeddings trained on some other (very) large text corpus (e.g., Amazon reviews, available on TensorFlow Datasets), even if it is not composed of movie reviews? After all, the word “amazing” generally has the same meaning whether you use it to talk about movies or anything else. Moreover, perhaps embeddings would be useful for sentiment analysis even if they were trained on another task: since words like “awesome” and “amazing” have a similar meaning, they will likely cluster in the embedding space even for tasks such as predicting the next word in a sentence. If all positive words and all negative words form clusters, then this will be helpful for sentiment analysis. So, instead of training word embeddings, we could just download and use pretrained embeddings, such as Google’s [Word2vec embeddings](https://homl.info/word2vec), Stanford’s [GloVe embeddings](https://homl.info/glove), or Facebook’s [FastText embeddings](https://fasttext.cc).\n",
    "\n",
    "Using pretrained word embeddings was popular for several years, but this approach has its limits. In particular, a word has a single representation, no matter the context. For example, the word “right” is encoded the same way in “left and right” and “right and wrong”, even though it means two very different things. To address this limitation, a [2018 paper](https://homl.info/elmo) by Matthew Peters introduced *Embeddings from Language Models* (**ELMo**): these are contextualized word embeddings learned from the internal states of a deep bidirectional language model. Instead of just using pretrained embeddings in your model, you reuse part of a pretrained language model. \n",
    "\n",
    "At roughly the same time, the [Universal Language Model Fine-Tuning (ULMFiT) paper](https://homl.info/ulmfit) by Jeremy Howard and Sebastian Ruder demonstrated the effectiveness of unsupervised pretraining for NLP tasks: the authors trained an LSTM language model on a huge text corpus using selfsupervised learning (i.e., generating the labels automatically from the data), then they fine-tuned it on various tasks. Their model outperformed the state of the art on six text classification tasks by a large margin (reducing the error rate by 18–24% in most cases). Moreover, the authors showed a pretrained model fine-tuned on just 100 labeled examples could achieve the same performance as one trained from scratch on 10,000 examples. Before the ULMFiT paper, using pretrained models was only the norm in computer vision; in the context of NLP, pretraining was limited to word embeddings. This paper marked the beginning of a new era in NLP: today, reusing pretrained language models is the norm.\n",
    "\n",
    "For example, let’s build a classifier based on the Universal Sentence Encoder, a model architecture introduced in a 2018 paper by a team of Google researchers. This model is based on the transformer architecture, which we will look at later in this chapter. Conveniently, the model is available on TensorFlow Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-hub\n",
      "  Using cached tensorflow_hub-0.16.1-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from tensorflow-hub) (1.23.5)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from tensorflow-hub) (3.20.3)\n",
      "Collecting tf-keras>=2.14.1 (from tensorflow-hub)\n",
      "  Using cached tf_keras-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting tensorflow<2.20,>=2.19 (from tf-keras>=2.14.1->tensorflow-hub)\n",
      "  Using cached tensorflow-2.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (3.3.0)\n",
      "Requirement already satisfied: packaging in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (24.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (78.1.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (2.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (1.48.2)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub)\n",
      "  Using cached tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub)\n",
      "  Using cached keras-3.10.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting numpy>=1.12.0 (from tensorflow-hub)\n",
      "  Using cached numpy-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (0.37.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (2025.4.26)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (3.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (0.7.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (0.45.1)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub)\n",
      "  Using cached rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub)\n",
      "  Using cached namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub)\n",
      "  Using cached optree-0.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (3.0.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras>=2.14.1->tensorflow-hub)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached tensorflow_hub-0.16.1-py2.py3-none-any.whl (30 kB)\n",
      "Using cached tf_keras-2.19.0-py3-none-any.whl (1.7 MB)\n",
      "Using cached tensorflow-2.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.8 MB)\n",
      "Downloading numpy-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m490.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hUsing cached keras-3.10.0-py3-none-any.whl (1.4 MB)\n",
      "Using cached namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (405 kB)\n",
      "Using cached rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, optree, numpy, mdurl, tensorboard, markdown-it-py, rich, keras, tensorflow, tf-keras, tensorflow-hub\n",
      "\u001b[2K  Attempting uninstall: numpym━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/11\u001b[0m [optree]\n",
      "\u001b[2K    Found existing installation: numpy 1.23.5━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/11\u001b[0m [optree]\n",
      "\u001b[2K    Uninstalling numpy-1.23.5:[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/11\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled numpy-1.23.5━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/11\u001b[0m [numpy]\n",
      "\u001b[2K  Attempting uninstall: tensorboard━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/11\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: tensorboard 2.12.1━━━━━━━━━━━\u001b[0m \u001b[32m 2/11\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling tensorboard-2.12.1:\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/11\u001b[0m [tensorboard]\n",
      "\u001b[2K      Successfully uninstalled tensorboard-2.12.1━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/11\u001b[0m [tensorboard]\n",
      "\u001b[2K  Attempting uninstall: keras\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/11\u001b[0m [rich]own-it-py]\n",
      "\u001b[2K    Found existing installation: keras 2.12.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/11\u001b[0m [rich]\n",
      "\u001b[2K    Uninstalling keras-2.12.0:━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/11\u001b[0m [keras]\n",
      "\u001b[2K      Successfully uninstalled keras-2.12.0\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/11\u001b[0m [keras]\n",
      "\u001b[2K  Attempting uninstall: tensorflow[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/11\u001b[0m [keras]\n",
      "\u001b[2K    Found existing installation: tensorflow 2.12.0━━━━━━━━━━━━\u001b[0m \u001b[32m 7/11\u001b[0m [keras]\n",
      "\u001b[2K    Uninstalling tensorflow-2.12.0:━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m 8/11\u001b[0m [tensorflow]\n",
      "\u001b[2K      Successfully uninstalled tensorflow-2.12.0[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m 8/11\u001b[0m [tensorflow]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/11\u001b[0m [tensorflow-hub]m [tf-keras]w]\n",
      "\u001b[1A\u001b[2KSuccessfully installed keras-3.10.0 markdown-it-py-3.0.0 mdurl-0.1.2 namex-0.1.0 numpy-2.1.3 optree-0.16.0 rich-14.0.0 tensorboard-2.19.0 tensorflow-2.19.0 tensorflow-hub-0.16.1 tf-keras-2.19.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 32\u001b[0m\n\u001b[1;32m     28\u001b[0m model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mModel(inputs, outputs)\n\u001b[1;32m     30\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnadam\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     31\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 32\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\u001b[43mtrain_set\u001b[49m, validation_data\u001b[38;5;241m=\u001b[39mvalid_set, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, \n\u001b[1;32m     33\u001b[0m           callbacks\u001b[38;5;241m=\u001b[39m[early_stop, model_ckpt], verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_set' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "os.environ['TFHUB_CACHE_DIR'] = 'my_tfhub_cache'\n",
    "\n",
    "\n",
    "raw_use_layer = hub.KerasLayer(\n",
    "    \"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "    trainable=True,\n",
    "    dtype=tf.string,\n",
    "    input_shape=[]     # <<< this line is the key\n",
    ")\n",
    "\n",
    "class UseWrapper(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Embed the pre-built Hublayer\n",
    "        self._use = raw_use_layer\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return self._use(inputs)\n",
    "\n",
    "inputs = tf.keras.Input(shape=(), dtype=tf.string)\n",
    "embeddings = UseWrapper()(inputs)\n",
    "x = tf.keras.layers.Dense(64, activation=\"relu\") (embeddings)\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\") (x)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='nadam', \n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_set, validation_data=valid_set, epochs=10, \n",
    "          callbacks=[early_stop, model_ckpt], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\CONTENTS\\APPLICATIONS\\New Folder\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\CONTENTS\\APPLICATIONS\\New Folder\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "704/704 - 27s - 38ms/step - accuracy: 0.8401 - loss: 0.3797 - val_accuracy: 0.8544 - val_loss: 0.3277\n",
      "Epoch 2/10\n",
      "704/704 - 24s - 34ms/step - accuracy: 0.8599 - loss: 0.3252 - val_accuracy: 0.8508 - val_loss: 0.3240\n",
      "Epoch 3/10\n",
      "704/704 - 24s - 34ms/step - accuracy: 0.8634 - loss: 0.3202 - val_accuracy: 0.8520 - val_loss: 0.3231\n",
      "Epoch 4/10\n",
      "704/704 - 24s - 35ms/step - accuracy: 0.8661 - loss: 0.3157 - val_accuracy: 0.8492 - val_loss: 0.3304\n",
      "Epoch 5/10\n",
      "704/704 - 24s - 34ms/step - accuracy: 0.8684 - loss: 0.3120 - val_accuracy: 0.8492 - val_loss: 0.3339\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x259851a42c0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# 1) Point TF‑Hub cache (optional)\n",
    "os.environ['TFHUB_CACHE_DIR'] = 'my_tfhub_cache'\n",
    "\n",
    "# 2) Define a custom Keras Layer for USE\n",
    "class USELayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, trainable=True, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # Load once, outside of any tf.function tracing\n",
    "        self.embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "        self.trainable = trainable\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs is a 1-D tf.Tensor of dtype string\n",
    "        return self.embed(inputs)\n",
    "\n",
    "    def get_config(self):\n",
    "        base = super().get_config()\n",
    "        return {**base, \"trainable\": self.trainable}\n",
    "\n",
    "# 3) Build the Functional model\n",
    "inputs = tf.keras.layers.Input(shape=(), dtype=tf.string, name=\"text_input\")\n",
    "x = USELayer(name=\"use\")(inputs)                 # now a true Layer\n",
    "x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# 4) Compile & fit with ASCII logs\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer='nadam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.fit(\n",
    "    train_set,\n",
    "    validation_data=valid_set,\n",
    "    epochs=10,\n",
    "    callbacks=[early_stop, model_ckpt],\n",
    "    verbose=2    # simple one‑line/per‑epoch output avoids Unicode issues\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### **TIP**\n",
    "> This model is quite large—close to 1 GB in size—so it may take a while to download. By default, TensorFlow Hub modules are saved to a temporary directory, and they get downloaded again and again every time you run your program. To avoid that, you must set the ***TFHUB_CACHE_DIR*** environment variable to a directory of your choice: the modules will then be saved there, and only downloaded once.\n",
    "\n",
    "Note that the last part of the TensorFlow Hub module URL specifies that we want version 4 of the model. This versioning ensures that if a new module version is released on TF Hub, it will not break our model. Conveniently, if you just enter this URL in a web browser, you will get the documentation for this module.\n",
    "\n",
    "Also note that we set ***trainable=True*** when creating the ***hub.KerasLayer***. This way, the pretrained Universal Sentence Encoder is fine-tuned during training: some of its weights are tweaked via backprop. Not all TensorFlow Hub modules are fine-tunable, so make sure to check the documentation for each pretrained module you’re interested in.\n",
    "\n",
    "After training, this model should reach a validation accuracy of over 90%. That’s actually really good: if you try to perform the task yourself, you will probably do only marginally better since many reviews contain both positive and negative comments. Classifying these ambiguous reviews is like flipping a coin.\n",
    "\n",
    "So far we have looked at text generation using a char-RNN, and sentiment analysis with word-level RNN models (based on trainable embeddings) and using a powerful pretrained language model from TensorFlow Hub. In the next section, we will explore another important NLP task: *neural machine translation* (**NMT**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **An Encoder-Decoder Network for Neural Machine Translation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s begin with a simple [NMT model](https://homl.info/103) that will translate English sentences to Spanish\n",
    "\n",
    "In short, the architecture is as follows: English sentences are fed as inputs to the encoder, and thedecoder outputs the Spanish translations. Note that the Spanish translations are also used as inputsto the decoder during training, but shifted back by one step. In other words, during training the decoder is given as input the word that it should have output at the previous step, regardless of what it actually output. This is called teacher forcing—a technique that significantly speeds up training and improves the model’s performance. For the very first word, the decoder is given the start-ofsequence (SOS) token, and the decoder is expected to end the sentence with an end-of-sequence (EOS) token.\n",
    "\n",
    "Each word is initially represented by its ID (e.g., 854 for the word “soccer”). Next, an Embedding layer returns the word embedding. These word embeddings are then fed to the encoder and the decoder.\n",
    "\n",
    "At each step, the decoder outputs a score for each word in the output vocabulary (i.e., Spanish), then the softmax activation function turns these scores into probabilities. For example, at the first step the word “Me” may have a probability of 7%, “Yo” may have a probability of 1%, and so on. The word with the highest probability is output. This is very much like a regular classification task, and indeed you can train the model using the \"sparse_categorical_crossentropy\" loss, much like we did in the char-RNN model.\n",
    "\n",
    "Note that at inference time (after training), you will not have the target sentence to feed to the decoder. Instead, you need to feed it the word that it has just output at the previous step, Let’s build and train this model! First, we need to download a dataset of English/Spanish sentence pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZIP file is here: /home/jaxon/.keras/datasets/spa-eng.zip\n",
      "Extracted to: /home/jaxon/.keras/datasets/spa-eng\n",
      "Reading from: /home/jaxon/.keras/datasets/spa-eng/spa-eng/spa.txt\n",
      "Go.\tVe.\n",
      "Go.\tVete.\n",
      "Go.\tVaya.\n",
      "Go.\tVáyase.\n",
      "Hi.\tHola.\n",
      "Run!\t¡Corre!\n",
      "Run.\tCorred.\n",
      "Who?\t¿Quién?\n",
      "Fire!\t¡Fuego!\n",
      "Fire!\t¡Incendio!\n",
      "Fire!\t¡Disparad!\n",
      "Help!\t¡Ayuda!\n",
      "Help!\t¡Socorro! ¡Auxilio!\n",
      "Help!\t¡Auxilio!\n",
      "Jump!\t¡\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TF_XLA_FLAGS\"] = \"--tf_xla_enable_xla_devices=false\"\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.config.optimizer.set_jit(False)\n",
    "\n",
    "\n",
    "from zipfile import ZipFile\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Download\n",
    "url = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
    "zip_path = Path(tf.keras.utils.get_file(\"spa-eng.zip\", origin=url))\n",
    "print(\"ZIP file is here:\", zip_path)\n",
    "\n",
    "# 2) Unzip manually\n",
    "extract_dir = zip_path.parent / \"spa-eng\"\n",
    "extract_dir.mkdir(exist_ok=True)    # make sure the folder exists\n",
    "with ZipFile(zip_path, 'r') as zf:\n",
    "    zf.extractall(extract_dir)\n",
    "print(\"Extracted to:\", extract_dir)\n",
    "\n",
    "# 3) Find and read spa.txt (wherever it landed)\n",
    "candidates = list(extract_dir.rglob(\"spa.txt\"))\n",
    "if not candidates:\n",
    "    raise FileNotFoundError(f\"spa.txt not found under {extract_dir}\")\n",
    "txt_path = candidates[0]\n",
    "print(\"Reading from:\", txt_path)\n",
    "text = txt_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "# quick sanity-check\n",
    "print(text[:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "text = text.replace(\"¡\", \"\").replace(\"¿\", \"\")\n",
    "pairs = [line.split(\"\\t\") for line in text.splitlines()]\n",
    "np.random.shuffle(pairs)\n",
    "sentences_en, sentences_es = zip(*pairs) # separates the pairs into 2 lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take a look at the first three sentence pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We had no water to drink. => No teníamos agua que beber.\n",
      "I like listening to music. => Me gusta escuchar música.\n",
      "She was kissed by him. => Él la besó.\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(sentences_en[i], \"=>\", sentences_es[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let’s create two TextVectorization layers—one per language—and adapt them to the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1753366097.992260    5422 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1730 MB memory:  -> device: 0, name: NVIDIA GeForce MX150, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 1000\n",
    "max_length = 50\n",
    "text_vec_layer_en = tf.keras.layers.TextVectorization(\n",
    "    vocab_size, output_sequence_length=max_length\n",
    ")\n",
    "text_vec_layer_es = tf.keras.layers.TextVectorization(\n",
    "    vocab_size, output_sequence_length=max_length\n",
    ")\n",
    "text_vec_layer_en.adapt(sentences_en)\n",
    "text_vec_layer_es.adapt([f\"start-fseq {s} endofseq\" for s in sentences_es])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few things to note here:\n",
    "- We limit the vocabulary size to 1,000, which is quite small. That’s because the training set is not very large, and because using a small value will speed up training. State-of-the-art translation models typically use a much larger vocabulary (e.g., 30,000), a much larger training set (gigabytes), and a much larger model (hundreds or even thousands of megabytes). For example, check out the Opus-MT models by the University of Helsinki, or the M2M-100 model by Facebook.\n",
    "  \n",
    "- Since all sentences in the dataset have a maximum of 50 words, we set output_sequence_length to 50: this way the input sequences will automatically be padded with zeros until they are all 50 tokens long. If there was any sentence longer than 50tokens in the training set, it would be cropped to 50 tokens.\n",
    "\n",
    "- For the Spanish text, we add “startofseq” and “endofseq” to each sentence when adapting the TextVectorization layer: we will use these words as SOS and EOS tokens. You could use any other words, as long as they are not actual Spanish words. Let’s inspect the first 10 tokens in both vocabularies. They start with the padding token, the unknown token, the SOS and EOS tokens (only in the Spanish vocabulary), then the actual words, sorted by decreasing frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', np.str_('the'), np.str_('i'), np.str_('to'), np.str_('you'), np.str_('tom'), np.str_('a'), np.str_('is'), np.str_('he')] ['', '[UNK]', np.str_('startfseq'), np.str_('endofseq'), np.str_('de'), np.str_('que'), np.str_('a'), np.str_('no'), np.str_('tom'), np.str_('la')]\n"
     ]
    }
   ],
   "source": [
    "print(text_vec_layer_en.get_vocabulary()[:10], \n",
    "text_vec_layer_es.get_vocabulary()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let’s create the training set and the validation set (you could also create a test set if you needed it). We will use the first 100,000 sentence pairs for training, and the rest for validation. The decoder’s inputs are the Spanish sentences plus an SOS token prefix. The targets are the Spanish sentences plus an EOS suffix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 17:08:23.455413: W external/local_xla/xla/service/gpu/llvm_gpu_backend/default/nvptx_libdevice_path.cc:40] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.\n",
      "Searched for CUDA in the following directories:\n",
      "  ./cuda_sdk_lib\n",
      "  ipykernel_launcher.runfiles/cuda_nvcc\n",
      "  ipykern/cuda_nvcc\n",
      "  \n",
      "  /usr/local/cuda\n",
      "  /opt/cuda\n",
      "  /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/tensorflow/python/platform/../../../nvidia/cuda_nvcc\n",
      "  /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/tensorflow/python/platform/../../../../nvidia/cuda_nvcc\n",
      "  /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/tensorflow/python/platform/../../cuda\n",
      "  /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/tensorflow/python/platform/../../../../../..\n",
      "  /home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/tensorflow/python/platform/../../../../../../..\n",
      "  .\n",
      "You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.\n"
     ]
    }
   ],
   "source": [
    "x_train = tf.constant(sentences_en[:100_000])\n",
    "x_valid = tf.constant(sentences_en[100_000:])\n",
    "x_train_dec = tf.constant([f\"startofseq {s}\" for s in sentences_es[:100_000]])\n",
    "x_valid_dec = tf.constant([f\"{s} endofseq\" for s in sentences_es[100_000:]])\n",
    "y_train = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[:100_000]])\n",
    "y_valid = text_vec_layer_es([f\"{s} endofseq\" for s in sentences_es[100_000:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, we’re now ready to build our translation model. We will use the functional API for that since the model is not sequential. It requires two text inputs—one for the encoder and one for the decoder so let’s start with that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
    "decoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to encode these sentences using the ***TextVectorization*** layers we prepared earlier, followed by an ***Embedding*** layer for each language, with ***mask_zero=True*** to ensure masking is handled automatically. The embedding size is a hyperparameter you can tune, as always:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
    "decoder_input_ids = text_vec_layer_es(decoder_inputs)\n",
    "encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size, mask_zero=True)\n",
    "decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size, embed_size, mask_zero=True)\n",
    "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **TIP**\n",
    "> When the languages share many words, you may get better performance using the same embedding layer for both the encoder and the decoder.\n",
    "\n",
    "Now let's create the encoder and pass it the embedded inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.19.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import RNN, LSTMCell\n",
    "\n",
    "# assume encoder_embeddings is already defined:\n",
    "# batch_size, timesteps, features = 32, 10, 64\n",
    "# encoder_embeddings = tf.random.uniform((batch_size, timesteps, features))\n",
    "\n",
    "with tf.device(\"/CPU:0\"):\n",
    "    # build a generic (non‑cuDNN) RNN wrapping an LSTMCell\n",
    "    cpu_encoder = RNN(\n",
    "        LSTMCell(512),\n",
    "        return_state=True,\n",
    "        return_sequences=False\n",
    "    )\n",
    "    # call it exactly like your old LSTM\n",
    "    encoder_outputs, state_h, state_c = cpu_encoder(encoder_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import RNN, LSTMCell, TimeDistributed, Dense\n",
    "\n",
    "with tf.device(\"/CPU:0\"):\n",
    "    cpu_decoder = RNN(\n",
    "        LSTMCell(512), \n",
    "        return_state=True, \n",
    "        return_sequences=True\n",
    "    )\n",
    "    decoder_seq, dec_h, dec_c = cpu_decoder(decoder_embeddings, initial_state=[state_h, state_c])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can pass the decoder’s outputs through a Dense layer with the softmax activation function to get the word probabilities for each step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
    "y_proba = output_layer(decoder_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **OPTIMIZING THE OUTPUT LAYER**\n",
    "> When the output vocabulary is large, outputting a probability for each and every possible wordcan be quite slow. If the target vocabulary contained, say, 50,000 Spanish words instead of1,000, then the decoder would output 50,000-dimensional vectors, and computing the softmaxfunction over such a large vector would be very computationally intensive. To avoid this, onesolution is to look only at the logits output by the model for the correct word and for a randomsample of incorrect words, then compute an approximation of the loss based only on theselogits. This sampled softmax technique was introduced in 2015 by Sébastien Jean et al. InTensorFlow you can use the tf.nn.sampled_softmax_loss() function for this duringtraining and use the normal softmax function at inference time (sampled softmax cannot beused at inference time because it requires knowing the target).Another thing you can do to speed up training—which is compatible with sampled softmax—is to tie the weights of the output layer to the transpose of the decoder’s embedding matrix (you will see how to tie weights in Chapter 17). This significantly reduces the number of model parameters, which speeds up training and may sometimes improve the model’s accuracy as well, especially if you don’t have a lot of training data. The embedding matrix is equivalent to one-hot encoding followed by a linear layer with no bias term and no activation function that maps the one-hot vectors to the embedding space. The output layer does the reverse. So, if the model can find an embedding matrix whose transpose is close to its inverse (such a matrix is called an orthogonal matrix), then there’s no need to learn a separate set of weights for the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4317s\u001b[0m 1s/step - accuracy: 0.0509 - loss: 3.5252 - val_accuracy: 0.0330 - val_loss: 5.0494\n",
      "Epoch 2/10\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2180s\u001b[0m 698ms/step - accuracy: 0.0789 - loss: 1.9470 - val_accuracy: 0.0324 - val_loss: 5.0158\n",
      "Epoch 3/10\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2051s\u001b[0m 656ms/step - accuracy: 0.0911 - loss: 1.4606 - val_accuracy: 0.0287 - val_loss: 5.2889\n",
      "Epoch 4/10\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2238s\u001b[0m 716ms/step - accuracy: 0.0985 - loss: 1.1988 - val_accuracy: 0.0296 - val_loss: 5.5027\n",
      "Epoch 5/10\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2317s\u001b[0m 741ms/step - accuracy: 0.1037 - loss: 1.0211 - val_accuracy: 0.0284 - val_loss: 5.8300\n",
      "Epoch 6/10\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2320s\u001b[0m 742ms/step - accuracy: 0.1078 - loss: 0.8804 - val_accuracy: 0.0280 - val_loss: 6.1498\n",
      "Epoch 7/10\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2333s\u001b[0m 747ms/step - accuracy: 0.1119 - loss: 0.7625 - val_accuracy: 0.0281 - val_loss: 6.4223\n",
      "Epoch 8/10\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2281s\u001b[0m 730ms/step - accuracy: 0.1156 - loss: 0.6566 - val_accuracy: 0.0275 - val_loss: 6.7051\n",
      "Epoch 9/10\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2973s\u001b[0m 951ms/step - accuracy: 0.1183 - loss: 0.5720 - val_accuracy: 0.0266 - val_loss: 7.1540\n",
      "Epoch 10/10\n",
      "\u001b[1m3125/3125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2678s\u001b[0m 857ms/step - accuracy: 0.1217 - loss: 0.4952 - val_accuracy: 0.0268 - val_loss: 7.5018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7fc97288f400>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs], \n",
    "                       outputs=[y_proba])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", \n",
    "              metrics=[\"accuracy\"], jit_compile=False)\n",
    "model.fit((x_train, x_train_dec), y_train, epochs=10, \n",
    "          validation_data=((x_valid, x_valid_dec), y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('eng_spanish_translator.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence_en):\n",
    "    translation = \"\"\n",
    "    for word_idx in range(max_length):\n",
    "        x = np.array([sentence_en]) # encoder input\n",
    "        x_dec = np.array([\"startofseq \" + translation]) # decoder input\n",
    "        y_proba = model.predict((x, x_dec))[0, word_idx] # last token's probas\n",
    "        predicted_word_id = np.argmax(y_proba)\n",
    "        predicted_word = text_vec_layer_es.get_vocabulary()[predicted_word_id]\n",
    "        if predicted_word == \"endofseq\":\n",
    "            break\n",
    "        translation += \" \" + predicted_word\n",
    "        return translation.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 17:09:09.302736: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-24 17:09:09.302772: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: UNKNOWN: JIT compilation failed.\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "{{function_node __wrapped__Sign_device_/job:localhost/replica:0/task:0/device:GPU:0}} JIT compilation failed. [Op:Sign] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load or define the model (adjust path)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meng_spanish_translator.keras\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Define parameters\u001b[39;00m\n\u001b[1;32m      8\u001b[0m max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m  \u001b[38;5;66;03m# Must match training\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/saving/saving_api.py:189\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    186\u001b[0m         is_keras_zip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_keras_zip \u001b[38;5;129;01mor\u001b[39;00m is_keras_dir \u001b[38;5;129;01mor\u001b[39;00m is_hf:\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msaving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format\u001b[38;5;241m.\u001b[39mload_model_from_hdf5(\n\u001b[1;32m    197\u001b[0m         filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m\n\u001b[1;32m    198\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:370\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    366\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid filename: expected a `.keras` extension. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    367\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    368\u001b[0m     )\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filepath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_model_from_fileobj\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:447\u001b[0m, in \u001b[0;36m_load_model_from_fileobj\u001b[0;34m(fileobj, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m zf\u001b[38;5;241m.\u001b[39mopen(_CONFIG_FILENAME, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    445\u001b[0m     config_json \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m--> 447\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43m_model_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m all_filenames \u001b[38;5;241m=\u001b[39m zf\u001b[38;5;241m.\u001b[39mnamelist()\n\u001b[1;32m    452\u001b[0m extract_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:436\u001b[0m, in \u001b[0;36m_model_from_config\u001b[0;34m(config_json, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# Construct the model from the configuration file in the archive.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ObjectSharingScope():\n\u001b[0;32m--> 436\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/saving/serialization_lib.py:718\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m custom_obj_scope, safe_mode_scope:\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 718\u001b[0m         instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    720\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    721\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m could not be deserialized properly. Please\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    722\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ensure that components that are Python object\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    726\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mconfig=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mException encountered: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    727\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/model.py:651\u001b[0m, in \u001b[0;36mModel.from_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_functional_config \u001b[38;5;129;01mand\u001b[39;00m revivable_as_functional:\n\u001b[1;32m    647\u001b[0m     \u001b[38;5;66;03m# Revive Functional model\u001b[39;00m\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;66;03m# (but not Functional subclasses with a custom __init__)\u001b[39;00m\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m functional_from_config\n\u001b[0;32m--> 651\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunctional_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;66;03m# Either the model has a custom __init__, or the config\u001b[39;00m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;66;03m# does not contain all the information necessary to\u001b[39;00m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;66;03m# revive a Functional model. This happens when the user creates\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;66;03m# In this case, we fall back to provide all config into the\u001b[39;00m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;66;03m# constructor of the class.\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py:560\u001b[0m, in \u001b[0;36mfunctional_from_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;66;03m# First, we create all layers and enqueue nodes to be processed\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_data \u001b[38;5;129;01min\u001b[39;00m functional_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayers\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 560\u001b[0m     \u001b[43mprocess_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# Then we process nodes in order of layer depth.\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m# Nodes that cannot yet be processed (if the inbound node\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# does not yet exist) are re-enqueued, and the process\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;66;03m# is repeated until all nodes are processed.\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m unprocessed_nodes:\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/models/functional.py:527\u001b[0m, in \u001b[0;36mfunctional_from_config.<locals>.process_layer\u001b[0;34m(layer_data)\u001b[0m\n\u001b[1;32m    523\u001b[0m     layer \u001b[38;5;241m=\u001b[39m saving_utils\u001b[38;5;241m.\u001b[39mmodel_from_config(\n\u001b[1;32m    524\u001b[0m         layer_data, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects\n\u001b[1;32m    525\u001b[0m     )\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 527\u001b[0m     layer \u001b[38;5;241m=\u001b[39m \u001b[43mserialization_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, Operation):\n\u001b[1;32m    531\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    532\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected object from deserialization, expected a layer or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moperation, got a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(layer)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    534\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/saving/serialization_lib.py:718\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m custom_obj_scope, safe_mode_scope:\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 718\u001b[0m         instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    720\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    721\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m could not be deserialized properly. Please\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    722\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ensure that components that are Python object\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    726\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mconfig=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mException encountered: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    727\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:475\u001b[0m, in \u001b[0;36mRNN.from_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_config\u001b[39m(\u001b[38;5;28mcls\u001b[39m, config, custom_objects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 475\u001b[0m     cell \u001b[38;5;241m=\u001b[39m \u001b[43mserialization_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    478\u001b[0m     layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(cell, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/saving/serialization_lib.py:730\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m build_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuild_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m build_config \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m instance\u001b[38;5;241m.\u001b[39mbuilt:\n\u001b[0;32m--> 730\u001b[0m     \u001b[43minstance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuild_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    731\u001b[0m     instance\u001b[38;5;241m.\u001b[39mbuilt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    732\u001b[0m compile_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompile_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/layer.py:481\u001b[0m, in \u001b[0;36mLayer.build_from_config\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config:\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config:\n\u001b[0;32m--> 481\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_shape\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config:\n\u001b[1;32m    483\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/layer.py:232\u001b[0m, in \u001b[0;36mLayer.__new__.<locals>.build_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_open_name_scope():\n\u001b[1;32m    231\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_path \u001b[38;5;241m=\u001b[39m current_path()\n\u001b[0;32m--> 232\u001b[0m     \u001b[43moriginal_build_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m# Record build config.\u001b[39;00m\n\u001b[1;32m    234\u001b[0m signature \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(original_build_method)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/rnn/lstm.py:157\u001b[0m, in \u001b[0;36mLSTMCell.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    149\u001b[0m input_dim \u001b[38;5;241m=\u001b[39m input_shape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_weight(\n\u001b[1;32m    151\u001b[0m     shape\u001b[38;5;241m=\u001b[39m(input_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munits \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m),\n\u001b[1;32m    152\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkernel\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    155\u001b[0m     constraint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_constraint,\n\u001b[1;32m    156\u001b[0m )\n\u001b[0;32m--> 157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurrent_kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_weight\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munits\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrecurrent_kernel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecurrent_initializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregularizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecurrent_regularizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconstraint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecurrent_constraint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_bias:\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_forget_bias:\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/layer.py:575\u001b[0m, in \u001b[0;36mLayer.add_weight\u001b[0;34m(self, shape, initializer, dtype, trainable, autocast, regularizer, constraint, aggregation, overwrite_with_gradient, name)\u001b[0m\n\u001b[1;32m    573\u001b[0m initializer \u001b[38;5;241m=\u001b[39m initializers\u001b[38;5;241m.\u001b[39mget(initializer)\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, caller\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 575\u001b[0m     variable \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVariable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m        \u001b[49m\u001b[43minitializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautocast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautocast\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43maggregation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maggregation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;66;03m# Will be added to layer.losses\u001b[39;00m\n\u001b[1;32m    585\u001b[0m variable\u001b[38;5;241m.\u001b[39mregularizer \u001b[38;5;241m=\u001b[39m regularizers\u001b[38;5;241m.\u001b[39mget(regularizer)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/common/variables.py:206\u001b[0m, in \u001b[0;36mVariable.__init__\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(initializer):\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_shape(shape)\n\u001b[0;32m--> 206\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_with_initializer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitializer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize(initializer)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/core.py:52\u001b[0m, in \u001b[0;36mVariable._initialize_with_initializer\u001b[0;34m(self, initializer)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_initialize_with_initializer\u001b[39m(\u001b[38;5;28mself\u001b[39m, initializer):\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minitializer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/core.py:42\u001b[0m, in \u001b[0;36mVariable._initialize\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVariable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrainable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43maggregation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_aggregation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggregation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynchronization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_synchronization\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynchronization\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/core.py:52\u001b[0m, in \u001b[0;36mVariable._initialize_with_initializer.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_initialize_with_initializer\u001b[39m(\u001b[38;5;28mself\u001b[39m, initializer):\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43minitializer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/initializers/random_initializers.py:707\u001b[0m, in \u001b[0;36mOrthogonal.__call__\u001b[0;34m(self, shape, dtype)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Make Q uniform\u001b[39;00m\n\u001b[1;32m    706\u001b[0m d \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mdiag(r)\n\u001b[0;32m--> 707\u001b[0m q \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msign\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_rows \u001b[38;5;241m<\u001b[39m num_cols:\n\u001b[1;32m    709\u001b[0m     q \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mtranspose(q)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/ops/numpy.py:5204\u001b[0m, in \u001b[0;36msign\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   5202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors((x,)):\n\u001b[1;32m   5203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Sign()\u001b[38;5;241m.\u001b[39msymbolic_call(x)\n\u001b[0;32m-> 5204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msign\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/sparse.py:387\u001b[0m, in \u001b[0;36melementwise_unary.<locals>.sparse_wrapper\u001b[0;34m(x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mIndexedSlices(\n\u001b[1;32m    384\u001b[0m         func(x\u001b[38;5;241m.\u001b[39mvalues, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), x\u001b[38;5;241m.\u001b[39mindices, x\u001b[38;5;241m.\u001b[39mdense_shape\n\u001b[1;32m    385\u001b[0m     )\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/tensorflow/numpy.py:2106\u001b[0m, in \u001b[0;36msign\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   2104\u001b[0m     x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcast(tf\u001b[38;5;241m.\u001b[39msign(x), ori_dtype)\n\u001b[0;32m-> 2106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msign\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mUnknownError\u001b[0m: {{function_node __wrapped__Sign_device_/job:localhost/replica:0/task:0/device:GPU:0}} JIT compilation failed. [Op:Sign] name: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Load or define the model (adjust path)\n",
    "model = tf.keras.models.load_model(\"eng_spanish_translator.keras\")\n",
    "\n",
    "# Define parameters\n",
    "max_length = 50  # Must match training\n",
    "vocab_en = [\"\", \"[UNK]\", \"i\", \"like\", \"soccer\", \"to\", \"play\"]  # Example; replace with real vocab\n",
    "vocab_es = [\"\", \"[UNK]\", \"inicio\", \"fin\", \"me\", \"gusta\", \"fútbol\"]  # Example\n",
    "\n",
    "# Initialize TextVectorization layers\n",
    "text_vec_layer_en = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=10000,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length\n",
    ")\n",
    "text_vec_layer_es = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=10000,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length\n",
    ")\n",
    "text_vec_layer_en.set_vocabulary(vocab_en)\n",
    "text_vec_layer_es.set_vocabulary(vocab_es)\n",
    "\n",
    "def translate(sentence_en):\n",
    "    encoder_input = text_vec_layer_en([sentence_en])  # Shape: (1, max_length)\n",
    "    start_token_id = text_vec_layer_es([\"startofseq\"])[0][0].numpy()\n",
    "    translation_ids = [start_token_id]\n",
    "    x_dec = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        [translation_ids], maxlen=max_length, padding=\"post\"\n",
    "    )\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        predictions = model.predict([encoder_input, x_dec])\n",
    "        predicted_id = tf.argmax(predictions[0, len(translation_ids) - 1], axis=-1).numpy()\n",
    "        if predicted_id == text_vec_layer_es.get_vocabulary().index(\"endofseq\", 1):\n",
    "            break\n",
    "        translation_ids.append(predicted_id)\n",
    "        x_dec = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            [translation_ids], maxlen=max_length, padding=\"post\"\n",
    "        )\n",
    "\n",
    "    translation = \" \".join(\n",
    "        text_vec_layer_es.get_vocabulary()[id] for id in translation_ids[1:]\n",
    "    )\n",
    "    return translation.replace(\"endofseq\", \"\").strip()\n",
    "\n",
    "# Test\n",
    "try:\n",
    "    result = translate(\"I like soccer\")\n",
    "    print(\"Translation:\", result)\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function simply keeps predicting one word at a time, gradually completing the translation, and it stops once it reaches the EOS token. Let's give it a try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid dtype: object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI like soccer\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[56], line 6\u001b[0m, in \u001b[0;36mtranslate\u001b[0;34m(sentence_en)\u001b[0m\n\u001b[1;32m      4\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([sentence_en]) \u001b[38;5;66;03m# encoder input\u001b[39;00m\n\u001b[1;32m      5\u001b[0m x_dec \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstartofseq \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m translation]) \u001b[38;5;66;03m# decoder input\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m y_proba \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_dec\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m, word_idx] \u001b[38;5;66;03m# last token's probas\u001b[39;00m\n\u001b[1;32m      7\u001b[0m predicted_word_id \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_proba)\n\u001b[1;32m      8\u001b[0m predicted_word \u001b[38;5;241m=\u001b[39m text_vec_layer_es\u001b[38;5;241m.\u001b[39mget_vocabulary()[predicted_word_id]\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/optree/ops.py:766\u001b[0m, in \u001b[0;36mtree_map\u001b[0;34m(func, tree, is_leaf, none_is_leaf, namespace, *rests)\u001b[0m\n\u001b[1;32m    764\u001b[0m leaves, treespec \u001b[38;5;241m=\u001b[39m _C\u001b[38;5;241m.\u001b[39mflatten(tree, is_leaf, none_is_leaf, namespace)\n\u001b[1;32m    765\u001b[0m flat_args \u001b[38;5;241m=\u001b[39m [leaves] \u001b[38;5;241m+\u001b[39m [treespec\u001b[38;5;241m.\u001b[39mflatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rests]\n\u001b[0;32m--> 766\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtreespec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflat_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid dtype: object"
     ]
    }
   ],
   "source": [
    "message = \"I like soccer\"\n",
    "translate(tf.convert_to_tensor(message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hurray, it works! Well, at least it does with very short sentences. If you try playing with this model for a while, you will find that it’s not bilingual yet, and in particular it really struggles with longer sentences. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid dtype: str7904",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mI like soccer and also going to the beach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 6\u001b[0m, in \u001b[0;36mtranslate\u001b[0;34m(sentence_en)\u001b[0m\n\u001b[1;32m      4\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([sentences_en]) \u001b[38;5;66;03m# encoder input\u001b[39;00m\n\u001b[1;32m      5\u001b[0m x_dec \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstartofseq \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m translation]) \u001b[38;5;66;03m# decoder input\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m y_proba \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_dec\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m, word_idx] \u001b[38;5;66;03m# last token's probas\u001b[39;00m\n\u001b[1;32m      7\u001b[0m predicted_word_id \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_proba)\n\u001b[1;32m      8\u001b[0m predicted_word \u001b[38;5;241m=\u001b[39m text_vec_layer_es\u001b[38;5;241m.\u001b[39mget_vocabulary()[predicted_word_id]\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/optree/ops.py:766\u001b[0m, in \u001b[0;36mtree_map\u001b[0;34m(func, tree, is_leaf, none_is_leaf, namespace, *rests)\u001b[0m\n\u001b[1;32m    764\u001b[0m leaves, treespec \u001b[38;5;241m=\u001b[39m _C\u001b[38;5;241m.\u001b[39mflatten(tree, is_leaf, none_is_leaf, namespace)\n\u001b[1;32m    765\u001b[0m flat_args \u001b[38;5;241m=\u001b[39m [leaves] \u001b[38;5;241m+\u001b[39m [treespec\u001b[38;5;241m.\u001b[39mflatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rests]\n\u001b[0;32m--> 766\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtreespec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflat_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid dtype: str7904"
     ]
    }
   ],
   "source": [
    "translate(\"I like soccer and also going to the beach\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The translation says “I like soccer and sometimes even the bus”. So how can you improve it? One way is to increase the training set size and add more LSTM layers in both the encoder and thedecoder. But this will only get you so far, so let’s look at more sophisticated techniques, starting with bidirectional recurrent layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Bidirectional RNNs**\n",
    "At each time step, a regular recurrent layer only looks at past and present inputs before generating its output. In other words, it is causal, meaning it cannot look into the future. This type of RNN makes sense when forecasting time series, or in the decoder of a sequence-to-sequence (seq2seq) model. But for tasks like text classification, or in the encoder of a seq2seq model, it is often preferable to look ahead at the next words before encoding a given word.\n",
    "\n",
    "For example, consider the phrases “the right arm”, “the right person”, and “the right to criticize”: to properly encode the word “right”, you need to look ahead. One solution is to run two recurrent layers on the same inputs, one reading the words from left to right and the other reading them from right to left, then combine their outputs at each time step, typically by concatenating them. This is what a ***bidirectional recurrent layer*** does.\n",
    "\n",
    "To implement a bidirectional recurrent layer in Keras, just wrap a recurrent layer in a ***tf.keras.layers.Bidirectional*** layer. For example, the following Bidirectional layer could be used as the encoder in our translation model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.LSTM(256, return_state=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### **NOTE**\n",
    "> The ***Bidirectional*** layer will create a clone of the GRU layer (but in the reverse direction), and it will run both and concatenate their outputs. So although the ***GRU*** layer has 10 units, the ***Bidirectional*** layer will output 20 values per time step.\n",
    "\n",
    "There’s just one problem. This layer will now return four states instead of two: the final short-term and long-term states of the forward ***LSTM*** layer, and the final short-term and long-term states of the backward ***LSTM*** layer. We cannot use this quadruple state directly as the initial state of the decoder’s ***LSTM*** layer, since it expects just two states (short-term and long-term). We cannot make the decoder bidirectional, since it must remain causal: otherwise it would cheat during training and it would not work. Instead, we can concatenate the two short-term states, and also concatenate the two long-term states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.ops`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m encoder_outputs, \u001b[38;5;241m*\u001b[39mencoder_state \u001b[38;5;241m=\u001b[39m cpu_encoder(encoder_embeddings)\n\u001b[0;32m----> 2\u001b[0m encoder_state \u001b[38;5;241m=\u001b[39m [\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_state\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;66;03m# short-term (0 & 2)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m                  tf\u001b[38;5;241m.\u001b[39mconcat(encoder_state[\u001b[38;5;241m1\u001b[39m::\u001b[38;5;241m2\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/common/keras_tensor.py:156\u001b[0m, in \u001b[0;36mKerasTensor.__tf_tensor__\u001b[0;34m(self, dtype, name)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__tf_tensor__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor cannot be used as input to a TensorFlow function. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor is a symbolic placeholder for a shape and dtype, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mused when constructing Keras Functional models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor Keras Functions. You can only use it as input to a Keras layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a Keras operation (from the namespaces `keras.layers` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand `keras.ops`). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are likely doing something like:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = Input(...)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_fn(x)  # Invalid.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat you should do instead is wrap `tf_fn` in a layer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass MyLayer(Layer):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    172\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    def call(self, x):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        return tf_fn(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = MyLayer()(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    175\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    176\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.ops`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"
     ]
    }
   ],
   "source": [
    "encoder_outputs, *encoder_state = cpu_encoder(encoder_embeddings)\n",
    "encoder_state = [tf.concat(encoder_state[::2], axis=-1), # short-term (0 & 2)\n",
    "                 tf.concat(encoder_state[1::2], axis=-1)] # long-term (1 & 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s look at another popular technique that can greatly improve the performance of a translation model at inference time: beam search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Beam Search**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you have trained an encoder–decoder model, and you use it to translate the sentence “I like soccer” to Spanish. You are hoping that it will output the proper translation “me gusta el fútbol”, but unfortunately it outputs “me gustan los jugadores”, which means “I like the players”. Looking at the training set, you notice many sentences such as “I like cars”, which translates to “me gustan los autos”, so it wasn’t absurd for the model to output “me gustan los” after seeing “I like”. Unfortunately, in this case it was a mistake since “soccer” is singular. The model could not go back and fix it, so it tried to complete the sentence as best it could, in this case using the word “jugadores”. How can we give the model a chance to go back and fix mistakes it made earlier? One of the most common solutions is beam search: it keeps track of a short list of the k most promising sentences (say, the top three), and at each decoder step it tries to extend them by one word, keeping only the k most likely sentences. The parameter ***k*** is called the ***beam width.***\n",
    "\n",
    "For example, suppose you use the model to translate the sentence “I like soccer” using beam search with a beam width of 3 (see Figure 16-6). At the first decoder step, the model will output an estimated probability for each possible first word in the translated sentence. Suppose the top three words are “me” (75% estimated probability), “a” (3%), and “como” (1%). That’s our short list so far. Next, we use the model to find the next word for each sentence. For the first sentence (“me”), perhaps the model outputs a probability of 36% for the word “gustan”, 32% for the word “gusta”, 16% for the word “encanta”, and so on. Note that these are actually conditional probabilities, given that the sentence starts with “me”. For the second sentence (“a”), the model might output a conditional probability of 50% for the word “mi”, and so on. Assuming the vocabulary has 1,000 words, we will end up with 1,000 probabilities per sentence. \n",
    "\n",
    "Next, we compute the probabilities of each of the 3,000 two-word sentences we considered (3 × 1,000). We do this by multiplying the estimated conditional probability of each word by the estimated probability of the sentence it completes. For example, the estimated probability of the sentence “me” was 75%, while the estimated conditional probability of the word “gustan” (given that the first word is “me”) was 36%, so the estimated probability of the sentence “me gustan” is 75% × 36% = 27%. After computing the probabilities of all 3,000 two-word sentences, we keep only the top 3. In this example they all start with the word “me”: “me gustan” (27%), “me gusta” (24%), and “me encanta” (12%). Right now, the sentence “me gustan” is winning, but “me gusta” has not been eliminated.\n",
    "\n",
    "Then we repeat the same process: we use the model to predict the next word in each of these three sentences, and we compute the probabilities of all 3,000 three-word sentences we considered. Perhaps the top three are now “me gustan los” (10%), “me gusta el” (8%), and “me gusta mucho” (2%). At the next step we may get “me gusta el fútbol” (6%), “me gusta mucho el” (1%), and “me gusta el deporte” (0.2%). Notice that “me gustan” was eliminated, and the correct translation is now ahead. We boosted our encoder–decoder model’s performance without any extra training, simply by using it more wisely.\n",
    "\n",
    "> #### **TIP**\n",
    "> The TensorFlow Addons library includes a full seq2seq API that lets you build encoder–decoder models with attention, including beam search, and more. However, its documentation is currently very limited. Implementing beam search is a good exercise, so give it a try! Check out this chapter’s notebook for a possible solution.\n",
    "\n",
    "With all this, you can get reasonably good translations for fairly short sentences. Unfortunately, this model will be really bad at translating long sentences. Once again, the problem comes from the limited short-term memory of RNNs. ***Attention mechanisms*** are the game-changing innovation that addressed this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Attention Mechanisms**\n",
    "Consider the path from the word “soccer” to its translation “fútbol” back in Figure 16-3: it is quite long! This means that a representation of this word (along with all the other words) needs to be carried over many steps before it is actually used. Can’t we make this path shorter?\n",
    "\n",
    "This was the core idea in a landmark 2014 paper by Dzmitry Bahdanau et al., where the authors introduced a technique that allowed the decoder to focus on the appropriate words (as encoded by the encoder) at each time step. For example, at the time step where the decoder needs to output the word “fútbol”, it will focus its attention on the word “soccer”. This means that the path from an input word to its translation is now much shorter, so the short-term memory limitations of RNNs have much less impact. Attention mechanisms revolutionized neural machine translation (and deep learning in general), allowing a significant improvement in the state of the art, especially for long sentences (e.g., over 30 words).\n",
    "\n",
    "> #### **NOTE**\n",
    "> The most common metric used in NMT is the ***bilingual evaluation understudy*** (**BLEU**) score, which compares each translation produced by the model with several good translations produced by humans: it counts the number of n-grams (sequences of n words) that appear in any of the target translations and adjusts the score to take into account the frequency of the produced n-grams in the target translations.\n",
    "\n",
    "One of the common attention mechanisms are:\n",
    "- ***Bahdanau attention*** (named after the 2014 paper’s first author). Since it concatenates the encoder output with the decoder’s previous hidden state, it is sometimes called ***concatenative attention*** (or ***additive attention***).\n",
    "\n",
    "- Another common attention mechanism, known as ***Luong attention*** or ***multiplicative attention***, was proposed shortly after, in 2015, by Minh-Thang Luong et al. Because the goal of the alignment model is to measure the similarity between one of the encoder’s outputs and the decoder’s previous hidden state, the authors proposed to simply compute the dot product (see Chapter 4) of these two vectors, as this is often a fairly good similarity measure, and modern hardware can compute it very efficiently. For this to be possible, both vectors must have the same dimensionality. The dot product gives a score, and all the scores (at a given decoder time step) go through a softmax layer to give the final weights, just like in Bahdanau attention. Another simplification Luong et al.\n",
    "\n",
    "Keras provides a ***tf.keras.layers.Attention*** layer for Luong attention, and an ***AdditiveAttention*** layer for Bahdanau attention. Let’s add Luong attention to our encoder– decoder model. Since we will need to pass all the encoder’s outputs to the ***Attention*** layer, we first need to set ***return_sequences=True*** when creating the encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.LSTM(256, return_sequences=True, return_state=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to create the attention layer and pass it the decoder’s states and the encoder’s outputs. However, to access the decoder’s states at each step we would need to write a custom memory cell. For simplicity, let’s use the decoder’s outputs instead of its states: in practice this works well too, and it’s much easier to code. Then we just pass the attention layer’s outputs directly to the output layer, as suggested in the Luong attention paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_layer = tf.keras.layers.Attention()\n",
    "attention_outputs = attention_layer([decoder_seq, encoder_outputs])\n",
    "output_layer = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
    "y_proba = output_layer(attention_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that’s it! If you train this model, you will find that it now handles much longer sentences. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid dtype: str7904",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mI like soccer and also going to the beach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 6\u001b[0m, in \u001b[0;36mtranslate\u001b[0;34m(sentence_en)\u001b[0m\n\u001b[1;32m      4\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([sentences_en]) \u001b[38;5;66;03m# encoder input\u001b[39;00m\n\u001b[1;32m      5\u001b[0m x_dec \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstartofseq \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m translation]) \u001b[38;5;66;03m# decoder input\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m y_proba \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_dec\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m, word_idx] \u001b[38;5;66;03m# last token's probas\u001b[39;00m\n\u001b[1;32m      7\u001b[0m predicted_word_id \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_proba)\n\u001b[1;32m      8\u001b[0m predicted_word \u001b[38;5;241m=\u001b[39m text_vec_layer_es\u001b[38;5;241m.\u001b[39mget_vocabulary()[predicted_word_id]\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/optree/ops.py:766\u001b[0m, in \u001b[0;36mtree_map\u001b[0;34m(func, tree, is_leaf, none_is_leaf, namespace, *rests)\u001b[0m\n\u001b[1;32m    764\u001b[0m leaves, treespec \u001b[38;5;241m=\u001b[39m _C\u001b[38;5;241m.\u001b[39mflatten(tree, is_leaf, none_is_leaf, namespace)\n\u001b[1;32m    765\u001b[0m flat_args \u001b[38;5;241m=\u001b[39m [leaves] \u001b[38;5;241m+\u001b[39m [treespec\u001b[38;5;241m.\u001b[39mflatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rests]\n\u001b[0;32m--> 766\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtreespec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflat_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid dtype: str7904"
     ]
    }
   ],
   "source": [
    "translate(\"I like soccer and also going to the beach\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In short, the attention layer provides a way to focus the attention of the model on part of the inputs. But there’s another way to think of this layer: it acts as a differentiable memory retrieval mechanism.\n",
    "\n",
    "For example, let’s suppose the encoder analyzed the input sentence “I like soccer”, and it managed to understand that the word “I” is the subject and the word “like” is the verb, so it encoded this information in its outputs for these words. Now suppose the decoder has already translated the subject, and it thinks that it should translate the verb next. For this, it needs to fetch the verb from the input sentence. This is analogous to a dictionary lookup: it’s as if the encoder had created a dictionary {\"subject”: “They”, “verb”: “played”, …} and the decoder wanted to look up the value that corresponds to the key “verb”.\n",
    "\n",
    "However, the model does not have discrete tokens to represent the keys (like “subject” or “verb”); instead, it has vectorized representations of these concepts that it learned during training, so the query it will use for the lookup will not perfectly match any key in the dictionary. The solution is to compute a similarity measure between the query and each key in the dictionary, and then use the softmax function to convert these similarity scores to weights that add up to 1. As we saw earlier, that’s exactly what the attention layer does. If the key that represents the verb is by far the most similar to the query, then that key’s weight will be close to 1.\n",
    "\n",
    "Next, the attention layer computes a weighted sum of the corresponding values: if the weight of the “verb” key is close to 1, then the weighted sum will be very close to the representation of the word “played”.\n",
    "\n",
    "This is why the Keras Attention and AdditiveAttention layers both expect a list as input, containing two or three items: the queries, the keys, and optionally the values. If you do not pass any values, then they are automatically equal to the keys. So, looking at the previous code example again, the decoder outputs are the queries, and the encoder outputs are both the keys and the values. For each decoder output (i.e., each query), the attention layer returns a weighted sum of the encoder outputs (i.e., the keys/values) that are most similar to the decoder output.\n",
    "\n",
    "The bottom line is that an attention mechanism is a trainable memory retrieval system. It is so powerful that you can actually build state-of-the-art models using only attention mechanisms. Enter the transformer architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Attention Is All You Need: The Original Transformer Architecture**\n",
    "\n",
    "\n",
    "In a groundbreaking 2017 paper, a team of Google researchers suggested that “Attention Is All You Need”. They created an architecture called the transformer, which significantly improved the state-of-the-art in NMT without using any recurrent or convolutional layers, just attention mechanisms (plus embedding layers, dense layers, normalization layers, and a few other bits and pieces). Because the model is not recurrent, it doesn’t suffer as much from the vanishing or exploding gradients problems as RNNs, it can be trained in fewer steps, it’s easier to parallelize across multiple GPUs, and it can better capture long-range patterns than RNNs.\n",
    "\n",
    "If you use the transformer for NMT, then during training you must feed the English sentences to the encoder and the corresponding Spanish translations to the decoder, with an extra SOS token inserted at the start of each sentence. At inference time, you must call the transformer multiple times, producing the translations one word at a time and feeding the partial translations to the decoder at each round, just like we did earlier in the translate() function.\n",
    "\n",
    "The encoder’s role is to gradually transform the inputs—word representations of the English sentence—until each word’s representation perfectly captures the meaning of the word, in the context of the sentence. For example, if you feed the encoder with the sentence “I like soccer”, then the word “like” will start off with a rather vague representation, since this word could mean different things in different contexts: think of “I like soccer” versus “It’s like that”. But after going through the encoder, the word’s representation should capture the correct meaning of “like” in the given sentence (i.e., to be fond of), as well as any other information that may be required for translation (e.g., it’s a verb).\n",
    "\n",
    "The decoder’s role is to gradually transform each word representation in the translated sentence into a word representation of the next word in the translation. For example, if the sentence to translate is “I like soccer”, and the decoder’s input sentence is “<SOS> me gusta el fútbol”, then after going through the decoder, the word representation of the word “el” will end up transformed into a representation of the word “fútbol”. Similarly, the representation of the word “fútbol” will be transformed into a representation of the EOS token.\n",
    "\n",
    "After going through the decoder, each word representation goes through a final Dense layer with a softmax activation function, which will hopefully output a high probability for the correct next word and a low probability for all other words. The predicted sentence should be “me gusta el fútbol <EOS>\n",
    "\n",
    "#### **Positional encodings**\n",
    "A positional encoding is a dense vector that encodes the position of a word within a sentence: the i positional encoding is added to the word embedding of the i word in the sentence. The easiest way to implement this is to use an Embedding layer and make it encode all the positions from 0 to the maximum sequence length in the batch, then add the result to the word embeddings. The rules of broadcasting will ensure that the positional encodings get applied to every input sequence. For example, here is how to add positional encodings to the encoder and decoder inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.ops`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m embed_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[1;32m      4\u001b[0m pos_embed_layer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mEmbedding(max_length, embed_size)\n\u001b[0;32m----> 5\u001b[0m batch_max_len_enc \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_embeddings\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      6\u001b[0m encoder_in \u001b[38;5;241m=\u001b[39m encoder_embeddings \u001b[38;5;241m+\u001b[39m pos_embed_layer(tf\u001b[38;5;241m.\u001b[39mrange(batch_max_len_dec))\n\u001b[1;32m      7\u001b[0m batch_max_len_dec \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mshape(decoder_embeddings)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/common/keras_tensor.py:156\u001b[0m, in \u001b[0;36mKerasTensor.__tf_tensor__\u001b[0;34m(self, dtype, name)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__tf_tensor__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor cannot be used as input to a TensorFlow function. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor is a symbolic placeholder for a shape and dtype, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mused when constructing Keras Functional models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor Keras Functions. You can only use it as input to a Keras layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a Keras operation (from the namespaces `keras.layers` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand `keras.ops`). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are likely doing something like:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = Input(...)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_fn(x)  # Invalid.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat you should do instead is wrap `tf_fn` in a layer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass MyLayer(Layer):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    172\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    def call(self, x):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        return tf_fn(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = MyLayer()(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    175\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    176\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.ops`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"
     ]
    }
   ],
   "source": [
    "max_length = 50 # max length of the whole training set.\n",
    "\n",
    "embed_size = 128\n",
    "pos_embed_layer = tf.keras.layers.Embedding(max_length, embed_size)\n",
    "batch_max_len_enc = tf.shape(encoder_embeddings)[1]\n",
    "encoder_in = encoder_embeddings + pos_embed_layer(tf.range(batch_max_len_dec))\n",
    "batch_max_len_dec = tf.shape(decoder_embeddings)[1]\n",
    "decoder_in = decoder_embeddings + pos_embed_layer(tf.range(batch_max_len_dec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this implementation assumes that the embeddings are represented as regular tensors, notragged tensors. The encoder and the decoder share the same Embedding layer for the positional encodings, since they have the same embedding size (this is often the case).\n",
    "\n",
    "There is no ***PositionalEncoding*** layer in TensorFlow, but it is not too hard to create one. For efficiency reasons, we precompute the positional encoding matrix in the constructor. The ***call()*** method just truncates this encoding matrix to the max length of the input sequences, and it adds them to the inputs. We also set supports_masking=True to propagate the input’s automatic mask to the next layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_length, embed_size, dtype=tf.float32, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        assert embed_size % 2 == 0, \"embed_size must be even\"\n",
    "        p, i = np.meshgrid(np.arange(max_length), \n",
    "                           2 * np.arange(embed_size // 2))\n",
    "        pos_emb = np.empty((1, max_length, embed_size))\n",
    "        pos_emb[0, :, ::2] = np.sin(p / 10_000 ** (i / embed_size)).T\n",
    "        pos_emb[0, :, 1::2] = np.cos(p / 10_000 ** (i / embed_size)).T\n",
    "        self.pos_encodings = tf.constant(pos_emb.astype(self.dtype))\n",
    "        self.supports_masking = True\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        batch_max_length = tf.shape(inputs)[1]\n",
    "        return inputs + self.pos_encodings[:, :batch_max_length]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s use this layer to add the positional encoding to the encoder’s inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embed_layer = PositionalEncoding(max_length, embed_size)\n",
    "encoder_in = pos_embed_layer(encoder_embeddings)\n",
    "decoder_in = pos_embed_layer(decoder_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s look deeper into the heart of the transformer model, at the multi-head attention layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Multi-head attention**\n",
    "It is just a bunch of scaled dot-product attention layers, each preceded by a linear transformation of the values, keys, and queries (i.e., a time-distributed dense layer with no activation function). All the outputs are simply concatenated, and they go through a final linear transformation (again, time-distributed).\n",
    " \n",
    "But why? What is the intuition behind this architecture? Well, consider once again the word “like” in the sentence “I like soccer”. The encoder was smart enough to encode the fact that it is a verb. But the word representation also includes its position in the text, thanks to the positional encodings, and it probably includes many other features that are useful for its translation, such as the fact that it is in the present tense. In short, the word representation encodes many different characteristics of the word. If we just used a single scaled dot-product attention layer, we would only be able to query all of these characteristics in one shot.\n",
    "\n",
    "This is why the multi-head attention layer applies multiple different linear transformations of the values, keys, and queries: this allows the model to apply many different projections of the word representation into different subspaces, each focusing on a subset of the word’s characteristics. Perhaps one of the linear layers will project the word representation into a subspace where all that remains is the information that the word is a verb, another linear layer will extract just the fact that it is present tense, and so on. Then the scaled dot-product attention layers implement the lookup phase, and finally we concatenate all the results and project them back to the original space.\n",
    "\n",
    "Keras includes a tf.keras.layers.MultiHeadAttention layer, so we now have everything we need to build the rest of the transformer. Let’s start with the full encoder, which is exactly like in Figure 16-8, except we use a stack of two blocks (N = 2) instead of six, since we don’t have a huge training set, and we add a bit of dropout as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.ops`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m dropout_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m      4\u001b[0m n_units \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m \u001b[38;5;66;03m# for the first dense layer in each feedforward block\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m encoder_pad_mask \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_equal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m[:, tf\u001b[38;5;241m.\u001b[39mnewaxis]\n\u001b[1;32m      6\u001b[0m Z \u001b[38;5;241m=\u001b[39m encoder_in\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N):\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/common/keras_tensor.py:156\u001b[0m, in \u001b[0;36mKerasTensor.__tf_tensor__\u001b[0;34m(self, dtype, name)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__tf_tensor__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor cannot be used as input to a TensorFlow function. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor is a symbolic placeholder for a shape and dtype, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mused when constructing Keras Functional models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor Keras Functions. You can only use it as input to a Keras layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a Keras operation (from the namespaces `keras.layers` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand `keras.ops`). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are likely doing something like:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = Input(...)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_fn(x)  # Invalid.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat you should do instead is wrap `tf_fn` in a layer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass MyLayer(Layer):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    172\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    def call(self, x):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        return tf_fn(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = MyLayer()(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    175\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    176\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.ops`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"
     ]
    }
   ],
   "source": [
    "N = 2 # instead of 6\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1\n",
    "n_units = 128 # for the first dense layer in each feedforward block\n",
    "encoder_pad_mask = tf.math.not_equal(encoder_input_ids, 0)[:, tf.newaxis]\n",
    "Z = encoder_in\n",
    "for _ in range(N):\n",
    "    skip = Z\n",
    "    attn_layer = tf.keras.layers.MultiHeadAttention(\n",
    "    num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
    "    Z = attn_layer(Z, value=Z, attention_mask=encoder_pad_mask)\n",
    "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
    "    skip = Z\n",
    "    Z = tf.keras.layers.Dense(n_units, activation=\"relu\")(Z)\n",
    "    Z = tf.keras.layers.Dense(embed_size)(Z)\n",
    "    Z = tf.keras.layers.Dropout(dropout_rate)(Z)\n",
    "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code should be mostly straightforward, except for one thing: masking. As of the time of writing, the MultiHeadAttention layer does not support automatic masking, so we must handle it manually. How can we do that?\n",
    "\n",
    "The MultiHeadAttention layer accepts an attention_mask argument, which is a Boolean tensor of shape [batch size, max query length, max value length]: for every token in every query sequence, this mask indicates which tokens in the corresponding value sequence should be attended to. We want to tell the MultiHeadAttention layer to ignore all the padding tokens in the values. So, we first compute the padding mask using tf.math.not_equal(encoder_input_ids, 0). This returns a Boolean tensor of shape [batch size, max sequence length]. We then insert a second axis using [:, tf.newaxis], to get a mask of shape [batch size, 1, max sequence length]. This allows us to use this mask as the attention_mask when calling the MultiHeadAttention layer: thanks to broadcasting, the same mask will be used for all tokens in each query. This way, the padding tokens in the values will be ignored correctly.\n",
    "\n",
    "However, the layer will compute outputs for every single query token, including the padding tokens. We need to mask the outputs that correspond to these padding tokens. Recall that we used mask_zero in the Embedding layers, and we set supports_masking to True in the PositionalEncoding layer, so the automatic mask was propagated all the way to the MultiHeadAttention layer’s inputs (encoder_in). We can use this to our advantage in the skip connection: indeed, the Add layer supports automatic masking, so when we add Z and skip (which is initially equal to encoder_in), the outputs get automatically masked correctly. Yikes! Masking required much more explanation than code.\n",
    "\n",
    "Now on to the decoder! Once again, masking is going to be the only tricky part, so let’s start with that. The first multi-head attention layer is a self-attention layer, like in the encoder, but it is a masked multi-head attention layer, meaning it is causal: it should ignore all tokens in the future. So, we need two masks: a padding mask and a causal mask. Let’s create them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.ops`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m decoder_pad_mask \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_equal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m[:, tf\u001b[38;5;241m.\u001b[39mnewaxis]\n\u001b[1;32m      2\u001b[0m causal_mask \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mband_part( \u001b[38;5;66;03m# creates a lower triangular matrix\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     tf\u001b[38;5;241m.\u001b[39mones((batch_max_len_dec, batch_max_len_dec), tf\u001b[38;5;241m.\u001b[39mbool), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/backend/common/keras_tensor.py:156\u001b[0m, in \u001b[0;36mKerasTensor.__tf_tensor__\u001b[0;34m(self, dtype, name)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__tf_tensor__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor cannot be used as input to a TensorFlow function. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor is a symbolic placeholder for a shape and dtype, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mused when constructing Keras Functional models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor Keras Functions. You can only use it as input to a Keras layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a Keras operation (from the namespaces `keras.layers` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand `keras.ops`). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are likely doing something like:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = Input(...)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_fn(x)  # Invalid.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat you should do instead is wrap `tf_fn` in a layer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    171\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass MyLayer(Layer):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    172\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    def call(self, x):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        return tf_fn(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = MyLayer()(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    175\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    176\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.ops`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"
     ]
    }
   ],
   "source": [
    "decoder_pad_mask = tf.math.not_equal(decoder_input_ids, 0)[:, tf.newaxis]\n",
    "causal_mask = tf.linalg.band_part( # creates a lower triangular matrix\n",
    "    tf.ones((batch_max_len_dec, batch_max_len_dec), tf.bool), -1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The padding mask is exactly like the one we created for the encoder, except it’s based on the decoder’s inputs rather than the encoder’s. The causal mask is created using the tf.linalg.band_part() function, which takes a tensor and returns a copy with all the values outside a diagonal band set to zero. With these arguments, we get a square matrix of size batch_max_len_dec (the max length of the input sequences in the batch), with 1s in the lowerleft triangle and 0s in the upper right. If we use this mask as the attention mask, we will get exactly what we want: the first query token will only attend to the first value token, the second will only attend to the first two, the third will only attend to the first three, and so on. In other words, query tokens cannot attend to any value token in the future.\n",
    "\n",
    "Let’s now build the decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Z' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mZ\u001b[49m \u001b[38;5;66;03m# let's save the encoder's final outputs\u001b[39;00m\n\u001b[1;32m      2\u001b[0m Z \u001b[38;5;241m=\u001b[39m decoder_in \u001b[38;5;66;03m# the decoder starts with its own inputs\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Z' is not defined"
     ]
    }
   ],
   "source": [
    "encoder_outputs = Z # let's save the encoder's final outputs\n",
    "Z = decoder_in # the decoder starts with its own inputs\n",
    "for _ in range(N):\n",
    "    skip = Z\n",
    "    attn_layer = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
    "    Z = attn_layer(Z, value=Z, attention_mask=causal_mask & decoder_pad_mask)\n",
    "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
    "    skip = Z\n",
    "    attn_layer = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, key_dim=embed_size, dropout=dropout_rate)\n",
    "    Z = attn_layer(Z, value=encoder_outputs, attention_mask=encoder_pad_mask)\n",
    "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))\n",
    "    skip = Z\n",
    "    Z = tf.keras.layers.Dense(n_units, activation=\"relu\")(Z)\n",
    "    Z = tf.keras.layers.Dense(embed_size)(Z)\n",
    "    Z = tf.keras.layers.LayerNormalization()(tf.keras.layers.Add()([Z, skip]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first attention layer, we use causal_mask & decoder_pad_mask to mask both the padding tokens and future tokens. The causal mask only has two dimensions: it’s missing the batch dimension, but that’s okay since broadcasting ensures that it gets copied across all the instances in the batch.\n",
    "\n",
    "For the second attention layer, there’s nothing special. The only thing to note is that we are usingencoder_pad_mask, not decoder_pad_mask, because this attention layer uses the encoder’s final outputs as its values.\n",
    "\n",
    "We’re almost done. We just need to add the final output layer, create the model, compile it, and train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Z' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m Y_proba \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(vocab_size, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m)(\u001b[43mZ\u001b[49m)\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mModel(inputs\u001b[38;5;241m=\u001b[39m[encoder_inputs, decoder_inputs],\n\u001b[1;32m      3\u001b[0m                        outputs\u001b[38;5;241m=\u001b[39m[Y_proba])\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnadam\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Z' is not defined"
     ]
    }
   ],
   "source": [
    "Y_proba = tf.keras.layers.Dense(vocab_size, activation=\"softmax\")(Z)\n",
    "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
    "                       outputs=[Y_proba])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit((X_train, X_train_dec), Y_train, epochs=10,\n",
    "          validation_data=((X_valid, X_valid_dec), Y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### **TIP**\n",
    "> The Keras team has created a new [*Keras NLP project*](https://github.com/keras-team/keras-cv), including an API to build a transformer more easily. You may also be interested in the new [*Keras CV project for computer vision*](https://github.com/keras-team/keras-cv).\n",
    "\n",
    "But the field didn’t stop there. Let’s now explore some of the recent advances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **An Avalanche of Transformer Models**\n",
    "The year 2018 has been called the “ImageNet moment for NLP”. Since then, progress has been astounding, with larger and larger transformer-based architectures trained on immense datasets.\n",
    "\n",
    "First, the [*GPT paper*](https://homl.info/gpt) by Alec Radford and other OpenAI researchers once again demonstrated the effectiveness of unsupervised pretraining, like the ELMo and ULMFiT papers before it, but this time using a transformer-like architecture. The authors pretrained a large but fairly simple architecture composed of a stack of 12 transformer modules using only masked multi-head attention layers, like in the original transformer’s decoder. They trained it on a very large dataset, using the same autoregressive technique we used for our Shakespearean char-RNN: just predict the next token. This is a form of self-supervised learning. Then they fine-tuned it on various language tasks, using only minor adaptations for each task. The tasks were quite diverse: they included text classification, entailment (whether sentence A imposes, involves, or implies sentence B as a necessary consequence), similarity (e.g., “Nice weather today” is very similar to “It is sunny”), and question answering (given a few paragraphs of text giving some context, the model must answer some multiple-choice questions).\n",
    "\n",
    "Then Google’s [*BERT paper*](https://homl.info/bert) came out: it also demonstrated the effectiveness of self-supervised pretraining on a large corpus, using a similar architecture to GPT but with nonmasked multi-head attention layers only, like in the original transformer’s encoder. This means that the model is naturally bidirectional; hence the B in BERT (Bidirectional Encoder Representations from Transformers). Most importantly, the authors proposed two pretraining tasks that explain most of the model’s strength:\n",
    "- ***Masked language model (MLM)***\n",
    "  Each word in a sentence has a 15% probability of being masked, and the model is trained to predict the masked words. For example, if the original sentence is “She had fun at the birthday party”, then the model may be given the sentence “She <mask> fun at the <mask> party” and it must predict the words “had” and “birthday” (the other outputs will be ignored). To be more precise, each selected word has an 80% chance of being masked, a 10% chance of being replaced by a random word (to reduce the discrepancy between pretraining and fine-tuning, since the model will not see <mask> tokens during fine-tuning), and a 10% chance of being left alone (to bias the model toward the correct answer).\n",
    "\n",
    "- ***Next sentence prediction (NSP)***\n",
    "  The model is trained to predict whether two sentences are consecutive or not. For example, it should predict that “The dog sleeps” and “It snores loudly” are consecutive sentences, while “The dog sleeps” and “The Earth orbits the Sun” are not consecutive. Later research showed that NSP was not as important as was initially thought, so it was dropped in most later architectures.\n",
    "\n",
    "The model is trained on these two tasks simultaneously. For the NSP task, the authors inserted a class token (<CLS>) at the start of every input, and the corresponding output token represents the model’s prediction: sentence B follows sentence A, or it does not. The two input sentences are concatenated, separated only by a special separation token (<SEP>), and they are fed as input to the model. To help the model know which sentence each input token belongs to, a segment embedding is added on top of each token’s positional embeddings: there are just two possible segment embeddings, one for sentence A and one for sentence B. For the MLM task, some input words are masked (as we just saw) and the model tries to predict what those words were. The loss is only computed on the NSP prediction and the masked tokens, not on the unmasked ones.\n",
    "\n",
    "After this unsupervised pretraining phase on a very large corpus of text, the model is then finetuned on many different tasks, changing very little for each task. For example, for text classificationsuch as sentiment analysis, all output tokens are ignored except for the first one, corresponding to the class token, and a new output layer replaces the previous one, which was just a binary classification layer for NSP.\n",
    "\n",
    "In February 2019, just a few months after BERT was published, Alec Radford, Jeffrey Wu, and other OpenAI researchers published the [GPT-2 paper](https://homl.info/gpt2), which proposed a very similar architecture to GPT, but larger still (with over 1.5 billion parameters!). The researchers showed that the new and improved GPT model could perform zero-shot learning (ZSL), meaning it could achieve good performance on many tasks without any fine-tuning. This was just the start of a race toward larger and larger models: Google’s [Switch Transformers](https://homl.info/switch) (introduced in January 2021) used 1 trillion parameters, and soon much larger models came out, such as the Wu Dao 2.0 model by the Beijing Academy of Artificial Intelligence (BAII), announced in June 2021.\n",
    "\n",
    "An unfortunate consequence of this trend toward gigantic models is that only well-funded organizations can afford to train such models: it can easily cost hundreds of thousands of dollars or more. And the energy required to train a single model corresponds to an American household’s electricity consumption for several years; it’s not eco-friendly at all. Many of these models are just too big to even be used on regular hardware: they wouldn’t fit in RAM, and they would be horribly slow. Lastly, some are so costly that they are not released publicly.\n",
    "\n",
    "Luckily, ingenious researchers are finding new ways to downsize transformers and make them more data-efficient. For example, the [DistilBERT model](https://homl.info/distilbert), introduced in October 2019 by Victor Sanh et al. from Hugging Face, is a small and fast transformer model based on BERT. It is available on Hugging Face’s excellent model hub, along with thousands of others—you’ll see an example later in this chapter.\n",
    "\n",
    "DistilBERT was trained using distillation (hence the name): this means transferring knowledge from a teacher model to a student one, which is usually much smaller than the teacher model. This is typically done by using the teacher’s predicted probabilities for each training instance as targets for the student. Surprisingly, distillation often works better than training the student from scratch on the same dataset as the teacher! Indeed, the student benefits from the teacher’s more nuanced labels.\n",
    "\n",
    "Many more transformer architectures came out after BERT, almost on a monthly basis, often improving on the state of the art across all NLP tasks: XLNet (June 2019), RoBERTa (July 2019), StructBERT (August 2019), ALBERT (September 2019), T5 (October 2019), ELECTRA (March 2020), GPT3 (May 2020), DeBERTa (June 2020), Switch Transformers (January 2021), Wu Dao 2.0 (June 2021), Gopher (December 2021), GPT-NeoX-20B (February 2022), Chinchilla (March 2022), OPT (May 2022), and the list goes on and on. Each of these models brought new ideas and techniques, but I particularly like the [T5 paper](https://homl.info/t5) by Google researchers: it frames all NLP tasks as text-to-text, using an encoder–decoder transformer. For example, to translate “I like soccer” to Spanish, you can just call the model with the input sentence “translate English to Spanish: I like soccer” and it outputs “me gusta el fútbol”. To summarize a paragraph, you just enter “summarize:” followed by the paragraph, and it outputs the summary. For classification, you only need to change the prefix to “classify:” and the model outputs the class name, as text. This simplifies using the model, and it also makes it possible to pretrain it on even more tasks.\n",
    "\n",
    "Last but not least, in April 2022, Google researchers used a new large-scale training platform named Pathways (which we will briefly discuss in Chapter 19) to train a humongous language model named the [Pathways Language Model (PaLM)](https://homl.info/palm), with a whopping 540 billion parameters, using over 6,000 TPUs. Other than its incredible size, this model is a standard transformer, using decoders only (i.e., with masked multi-head attention layers), with just a few tweaks (see the paper for details). This model achieved incredible performance on all sorts of NLP tasks, particularly in natural language understanding (NLU). It’s capable of impressive feats, such as explaining jokes, giving detailed step-by-step answers to questions, and even coding. This is in part due to the model’s size, but also thanks to a technique called [Chain of thought prompting](https://homl.info/cpt), which was introduced a couple months earlier by another team of Google researchers.\n",
    "\n",
    "In question answering tasks, regular prompting typically includes a few examples of questions and answers, such as: “Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? A: 11.” The prompt then continues with the actual question, such as “Q: John takes care of 10 dogs. Each dog takes .5 hours a day to walk and take care of their business. How many hours a week does he spend taking care of dogs? A:”, and the model’s job is to append the answer: in this case, “35.”\n",
    "\n",
    "But with chain of thought prompting, the example answers include all the reasoning steps that lead to the conclusion. For example, instead of “A: 11”, the prompt contains “A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11.” This encourages the model to give a detailed answer to the actual question, such as “John takes care of 10 dogs. Each dog takes .5 hours a day to walk and take care of their business. So that is 10 × .5 = 5 hours a day. 5 hours a day × 7 days a week = 35 hours a week. The answer is 35 hours a week.” This is an actual example from the paper!\n",
    "\n",
    "Not only does the model give the right answer much more frequently than using regular prompting we’re encouraging the model to think things through—but it also provides all the reasoning steps, which can be useful to better understand the rationale behind a model’s answer. Transformers have taken over NLP, but they didn’t stop there: they soon expanded to computer vision as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Vision Transformers**\n",
    "One of the first applications of attention mechanisms beyond NMT was in generating image captions using [visual attention](https://homl.info/visualattention): a convolutional neural network first processes the image and outputs some feature maps, then a decoder RNN equipped with an attention mechanism generates the caption, one word at a time.\n",
    "\n",
    "At each decoder time step (i.e., each word), the decoder uses the attention model to focus on just the right part of the image.\n",
    "\n",
    "> #### **EXPLAINABILITY**\n",
    "> One extra benefit of attention mechanisms is that they make it easier to understand what led the model to produce its output. This is called explainability. It can be especially useful when the model makes a mistake: for example, if an image of a dog walking in the snow is labeled as “a wolf walking in the snow”, then you can go back and check what the model focused on when it output the word “wolf”. You may find that it was paying attention not only to the dog, but also to the snow, hinting at a possible explanation: perhaps the way the model learned to distinguish dogs from wolves is by checking whether or not there’s a lot of snow around. You can then fix this by training the model with more images of wolves without snow, and dogs with snow. This example comes from a great [2016 paper](https://homl.info/explainclass) by Marco Tulio Ribeiro et al. that uses a different approach to explainability: learning an interpretable model locally around a classifier’s prediction. In some applications, explainability is not just a tool to debug a model; it can be a legal requirement—think of a system deciding whether or not it should grant you a loan.\n",
    "\n",
    "When transformers came out in 2017 and people started to experiment with them beyond NLP, they were first used alongside CNNs, without replacing them. Instead, transformers were generally used to replace RNNs, for example, in image captioning models. Transformers became slightly more visual in a [2020 paper](https://homl.info/detr) by Facebook researchers, which proposed a hybrid CNN–transformer architecture for object detection. Once again, the CNN first processes the input images and outputs a set of feature maps, then these feature maps are converted to sequences and fed to a transformer, which outputs bounding box predictions. But again, most of the visual work is still done by the CNN.\n",
    "\n",
    "Then, in October 2020, a team of Google researchers released [a paper](https://homl.info/vit) that introduced a fully transformer-based vision model, called a vision transformer (ViT). The idea is surprisingly simple: just chop the image into little 16 × 16 squares, and treat the sequence of squares as if it were a sequence of word representations. To be more precise, the squares are first flattened into 16 × 16 × 3 = 768-dimensional vectors—the 3 is for the RGB color channels—then these vectors go through a linear layer that transforms them but retains their dimensionality. The resulting sequence of vectors can then be treated just like a sequence of word embeddings: this means adding positional embeddings, and passing the result to the transformer. That’s it! This model beat the state of the art on ImageNet image classification, but to be fair the authors had to use over 300 million additional images for training. This makes sense since transformers don’t have as many inductive biases as convolution neural nets, so they need extra data just to learn things that CNNs implicitly assume.\n",
    "\n",
    "> #### **NOTE**\n",
    "> An inductive bias is an implicit assumption made by the model, due to its architecture. For example, linear models implicitly assume that the data is, well, linear. CNNs implicitly assume that patterns learned in one location will likely be useful in other locations as well. RNNs implicitly assume that the inputs are ordered, and that recent tokens are more important than older ones. The more inductive biases a model has, assuming they are correct, the less training data the model will require. But if the implicit assumptions are wrong, then the model may perform poorly even if it is trained on a large dataset.\n",
    "\n",
    "Just two months later, a team of Facebook researchers released [a paper](https://homl.info/deit) that introduced dataefficient image transformers (DeiTs). Their model achieved competitive results on ImageNet without requiring any additional data for training. The model’s architecture is virtually the same as the original ViT, but the authors used a distillation technique to transfer knowledge from state-ofthe-art CNN models to their model.\n",
    "\n",
    "Then, in March 2021, DeepMind released an important [paper](https://homl.info/perceiver) that introduced the Perceiver architecture. It is a multimodal transformer, meaning you can feed it text, images, audio, or virtually any other modality. Until then, transformers had been restricted to fairly short sequences because of the performance and RAM bottleneck in the attention layers. This excluded modalities such as audio or video, and it forced researchers to treat images as sequences of patches, rather than sequences of pixels. The bottleneck is due to self-attention, where every token must attend to every other token: if the input sequence has M tokens, then the attention layer must compute an M × M matrix, which can be huge if M is very large. The Perceiver solves this problem by gradually improving a fairly short latent representation of the inputs, composed of N tokens—typically just a few hundred. (The word latent means hidden, or internal.) The model uses cross-attention layers only, feeding them the latent representation as the queries, and the (possibly large) inputs as the values. This only requires computing an M × N matrix, so the computational complexity is linear with regard to M, instead of quadratic. After going through several cross-attention layers, if everything goes well, the latent representation ends up capturing everything that matters in the inputs. The authors also suggested sharing the weights between consecutive cross-attention layers: if you do that, then the Perceiver effectively becomes an RNN. Indeed, the shared cross-attention layers can be seen as the same memory cell at different time steps, and the latent representation corresponds to the cell’s context vector. The same inputs are repeatedly fed to the memory cell at every time step. It looks like RNNs are not dead after all!\n",
    "\n",
    "Just a month later, Mathilde Caron et al. introduced [DINO](https://homl.info/dino), an impressive vision transformer trained entirely without labels, using self-supervision, and capable of high-accuracy semantic segmentation. The model is duplicated during training, with one network acting as a teacher and the other acting as a student. Gradient descent only affects the student, while the teacher’s weights are just an exponential moving average of the student’s weights. The student is trained to match the teacher’s predictions: since they’re almost the same model, this is called self-distillation. At each training step, the input images are augmented in different ways for the teacher and the student, so they don’t see the exact same image, but their predictions must match. This forces them to come up with high-level representations. To prevent mode collapse, where both the student and the teacher would always output the same thing, completely ignoring the inputs, DINO keeps track of a moving average of the teacher’s outputs, and it tweaks the teacher’s predictions to ensure that they remain centered on zero, on average. DINO also forces the teacher to have high confidence in its predictions: this is called sharpening. Together, these techniques preserve diversity in the teacher’s outputs.\n",
    "\n",
    "In a [2021 paper](https://homl.info/scalingvits), Google researchers showed how to scale ViTs up or down, depending on the amount of data. They managed to create a huge 2 billion parameter model that reached over 90.4% top-1 accuracy on ImageNet. Conversely, they also trained a scaled-down model that reached over 84.8% top-1 accuracy on ImageNet, using only 10,000 images: that’s just 10 images per class! And progress in visual transformers has continued steadily to this day. For example, in March 2022, a [paper](https://homl.info/modelsoups) by Mitchell Wortsman et al. demonstrated that it’s possible to first train multiple transformers, then average their weights to create a new and improved model. This is similar to an ensemble (see Chapter 7), except there’s just one model in the end, which means there’s no inference time penalty.\n",
    "\n",
    "The latest trend in transformers consists in building large multimodal models, often capable of zeroshot or few-shot learning. For example, [OpenAI’s 2021 CLIP paper](https://homl.info/clip) proposed a large transformer model pretrained to match captions with images: this task allows it to learn excellent image representations, and the model can then be used directly for tasks such as image classification using simple text prompts such as “a photo of a cat”. Soon after, OpenAI announced [DALL·E](https://homl.info/dalle), capable of generating amazing images based on text prompts. The [DALL·E 2](https://homl.info/dalle), which generates even higher quality images using a diffusion model (see Chapter 17).\n",
    "\n",
    "> #### **NOTE**\n",
    "> astounding advances have led some researchers to claim that human-level AI is near, that “scale is all you need”, and that some of these models may be “slightly conscious”. Others point out that despite the amazing progress, these models still lack the reliability and adaptability of human intelligence, our ability to reason symbolically, to generalize based on a single example, and more.\n",
    "\n",
    "As you can see, transformers are everywhere! And the good news is that you generally won’t have to implement transformers yourself since many excellent pretrained models are readily available for download via TensorFlow Hub or Hugging Face’s model hub. You’ve already seen how to use a model from TF Hub, so let’s close this chapter by taking a quick look at Hugging Face’s ecosystem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Hugging Face's Transformers Library**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s impossible to talk about transformers today without mentioning Hugging Face, an AI company that has built a whole ecosystem of easy-to-use open source tools for NLP, vision, and beyond. The\n",
    "central component of their ecosystem is the Transformers library, which allows you to easily download a pretrained model, including its corresponding tokenizer, and then fine-tune it on your own dataset, if needed. Plus, the library supports TensorFlow, PyTorch, and JAX (with the Flax library).\n",
    "\n",
    "The simplest way to use the Transformers library is to use the ***transformers.pipeline()*** function: you just specify which task you want, such as sentiment analysis, and it downloads a default pretrained model, ready to be used—it really couldn’t be any simpler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m----> 3\u001b[0m classifier \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msentiment-analysis\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m result \u001b[38;5;241m=\u001b[39m classifier(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe actors were very convincing\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/transformers/pipelines/__init__.py:1020\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m   1015\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1016\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYou cannot use both `pipeline(... torch_dtype=..., model_kwargs=\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:...})` as those\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1017\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m arguments might conflict, use only one.)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1018\u001b[0m         )\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1020\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(torch_dtype, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[43mtorch\u001b[49m, torch_dtype):\n\u001b[1;32m   1021\u001b[0m         torch_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(torch, torch_dtype)\n\u001b[1;32m   1022\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch_dtype\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", framework=\"tf\")\n",
    "result = classifier(\"The actors were very convincing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes a lot of Mbs\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification, pipeline\n",
    "\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "Classifier = pipeline(\n",
    "    \"sentiment_analysis\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    framework=\"tf\"\n",
    ")\n",
    "\n",
    "print(Classifier(\"The actors were very convincing\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### **BIAS AND FAIRNESS**\n",
    "> As the output suggests, this specific classifier loves Indians, but is severely biased against Iraqis. You can try this code with your own country or city. Such an undesirable bias generally comes in large part from the training data itself: in this case, there were plenty of negative sentences related to the wars in Iraq in the training data. This bias was then amplified during the fine-tuning process since the model was forced to choose between just two classes: positive or negative. If you add a neutral class when fine-tuning, then the country bias mostly disappears. But the training data is not the only source of bias: the model’s architecture, the type of loss or regularization used for training, the optimizer; all of these can affect what the model ends up learning. Even a mostly unbiased model can be used in a biased way, much like survey questions can be biased.\n",
    ">\n",
    "> Understanding bias in AI and mitigating its negative effects is still an area of active research, but one thing is certain: you should pause and think before you rush to deploy a model to production. Ask yourself how the model could do harm, even indirectly. For example, if the model’s predictions are used to decide whether or not to give someone a loan, the process should be fair. So, make sure you evaluate the model’s performance not just on average over the whole test set, but across various subsets as well: for example, you may find that although the model works very well on average, its performance is abysmal for some categories of people. You may also want to run counterfactual tests: for example, you may want to check that the model’s predictions do not change when you simply switch someone’s gender.\n",
    ">\n",
    "> If the model works well on average, it’s tempting to push it to production and move on to something else, especially if it’s just one component of a much larger system. But in general, if you don’t fix such issues, no one else will, and your model may end up doing more harm than good. The solution depends on the problem: it may require rebalancing the dataset, fine-tuning on a different dataset, switching to another pretrained model, tweaking the model’s architecture or hyperparameters, etc.\n",
    "\n",
    "The ***pipeline()*** function uses the default model for the given task. For example, for text classification tasks such as sentiment analysis, at the time of writing, it defaults to distilbertbase-uncased-finetuned-sst-2-english—a DistilBERT model with an uncased tokenizer, trained on English Wikipedia and a corpus of English books, and fine-tuned on the Stanford Sentiment Treebank v2 (SST 2) task. It’s also possible to manually specify a different model. For example, you could use a DistilBERT model fine-tuned on the Multi-Genre Natural Language Inference (MultiNLI) task, which classifies two sentences into three classes: contradiction, neutral, or entailment. Here is how:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline() function uses the default model for the given task. For example, for text classification tasks such as sentiment analysis, at the time of writing, it defaults to distilbertbase-uncased-finetuned-sst-2-english—a DistilBERT model with an uncased tokenizer, trained on English Wikipedia and a corpus of English books, and fine-tuned on the Stanford Sentiment Treebank v2 (SST 2) task. It’s also possible to manually specify a different model. For example, you could use a DistilBERT model fine-tuned on the Multi-Genre Natural Language Inference (MultiNLI) task, which classifies two sentences into three classes: contradiction, neutral, or entailment. Here is how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "model_name = \"huggingface/distilbert-base-uncased-finetuned-mnli\"\n",
    "classifier_mnli = pipeline(\"text-classification\", model=model_name)\n",
    "classifier_mnli(\"She loves me. [SEP] She loves me not.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### **TIP**\n",
    "> You can find the available models at https://huggingface.co/models, and the list of tasks at https://huggingface.co/tasks.\n",
    "\n",
    "The pipeline API is very simple and convenient, but sometimes you will need more control. For such cases, the Transformers library provides many classes, including all sorts of tokenizers, models, configurations, callbacks, and much more. For example, let’s load the same DistilBERT model, along with its corresponding tokenizer, using the TFAutoModelForSequenceClassification and AutoTokenizer classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let’s tokenize a couple of pairs of sentences. In this code, we activate padding and specify that we want TensorFlow tensors instead of Python lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids = tokenizer([\"I like soccer. [SEP] We all love soccer!\",\n",
    "                        \"Joe lived for a very long time. [SEP] Joe is old.\"],\n",
    "                        padding=True, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### **TIP**\n",
    "> Instead of passing \"Sentence 1 [SEP] Sentence 2\" to the tokenizer, you can equivalently pass it a tuple: (\"Sentence 1\", \"Sentence 2\").\n",
    "\n",
    "The output is a dictionary-like instance of the BatchEncoding class, which contains the sequences of token IDs, as well as a mask containing 0s for the padding tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you set ***return_token_type_ids=True*** when calling the tokenizer, you will also get an extra tensor that indicates which sentence each token belongs to. This is needed by some models, but not DistilBERT.\n",
    "\n",
    "Next, we can directly pass this BatchEncoding object to the model; it returns a ***TFSequenceClassifierOutput*** object containing its predicted class logits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(token_ids)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can apply the softmax activation function to convert these logits to class probabilities, and use the ***argmax()*** function to predict the class with the highest probability for each input sentence pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_probas = tf.keras.activations.softmax(outputs.logits)\n",
    "Y_probas\n",
    "\n",
    "Y_pred = tf.argmax(Y_probas, axis=1)\n",
    "Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the model correctly classifies the first sentence pair as neutral (the fact that I like soccer does not imply that everyone else does) and the second pair as an entailment (Joe must indeed be quite old).\n",
    "\n",
    "If you wish to fine-tune this model on your own dataset, you can train the model as usual with Keras since it’s just a regular Keras model with a few extra methods. However, because the model outputs logits instead of probabilities, you must use the ***tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)*** loss instead of the usual \"***sparse_categorical_crossentropy***\" loss. Moreover, the modeldoes not support BatchEncoding inputs during training, so you must use its data attribute to get a regular dictionary instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [(\"Sky is blue\", \"Sky is red\"), (\"I love her\", \"She loves me\")]\n",
    "X_train = tokenizer(sentences, padding=True, return_tensors=\"tf\").data\n",
    "y_train = tf.constant([0, 2])\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(loss=loss, optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging Face has also built a Datasets library that you can use to easily download a standard dataset (such as IMDb) or a custom one, and use it to fine-tune your model. It’s similar to TensorFlow Datasets, but it also provides tools to perform common preprocessing tasks on the fly, such as masking. The list of datasets is available at https://huggingface.co/datasets.\n",
    "\n",
    "This should get you started with Hugging Face’s ecosystem. To learn more, you can head over to https://huggingface.co/docs for the documentation, which includes many tutorial notebooks, videos, the full API, and more. I also recommend you check out the O’Reilly book Natural Language Processing with Transformers: Building Language Applications with Hugging Face by Lewis Tunstall, Leandro von Werra, and Thomas Wolf—all from the Hugging Face team.\n",
    "\n",
    "In the next chapter we will discuss how to learn deep representations in an unsupervised way using autoencoders, and we will use generative adversarial networks to produce images and more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
