{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d19d3ee6",
   "metadata": {},
   "source": [
    "# **Reinforcement Learning**\n",
    "Reinforcement learning (RL) is one of the most exciting fields of machine learning today, and also one of the oldest. It has been around since the 1950s, producing many interesting applications over the years, particularly in games (e.g., TD-Gammon, a Backgammon-playing program) and in machine control, but seldom making the headline news. However, a revolution took place [in 2013](https://homl.info/dqn), when researchers from a British startup called DeepMind demonstrated a system that could learn to play just about any Atari game from scratch, eventually [outperforming humans](https://homl.info/dqn2) in most of them, using only raw pixels as inputs and without any prior knowledge of the rules of the games. This was the first of a series of amazing feats, culminating with the victory of their system AlphaGo against Lee Sedol, a legendary professional player of the game of Go, in March 2016 and against Ke Jie, the world champion, in May 2017. No program had ever come close to beating a master of this game, let alone the world champion. Today the whole field of RL is boiling with new ideas, with a wide range of applications.\n",
    "\n",
    "So how did DeepMind (bought by Google for over $500 million in 2014) achieve all this? With hindsight it seems rather simple: they applied the power of deep learning to the field of reinforcement learning, and it worked beyond their wildest dreams. In this chapter I will first explain what reinforcement learning is and what it’s good at, then present two of the most important techniques in deep reinforcement learning: policy gradients and deep Qnetworks, including a discussion of Markov decision processes. Let’s get started!\n",
    "\n",
    "## **Learning to Optimize Rewards**\n",
    "In reinforcement learning, a software agent makes observations and takes actions within an environment, and in return it receives rewards from the environment. Its objective is to learn to act in a way that will maximize its expected rewards over time. If you don’t mind a bit of anthropomorphism, you can think of positive rewards as pleasure, and negative rewards as pain (the term “reward” is a bit misleading in this case). In short, the agent acts in the environment and learns by trial and error to maximize its pleasure and minimize its pain.\n",
    "\n",
    "\n",
    "This is quite a broad setting, which can apply to a wide variety of tasks. Here are a few examples (see Figure 18-1 from the book):\n",
    "- The agent can be the program controlling a robot. In this case, the environment is the real world, the agent observes the environment through a set of sensors such as cameras and touch sensors, and its actions consist of sending signals to activate motors. It may be programmed to get positive rewards whenever it approaches the target destination, and negative rewards whenever it wastes time or goes in the wrong direction.\n",
    "\n",
    "- The agent can be the program controlling Ms. Pac-Man. In this case, the environment is a simulation of the Atari game, the actions are the nine possible joystick positions (upper left, down, center, and so on), the observations are screenshots, and the rewards are just the game points.\n",
    "\n",
    "- Similarly, the agent can be the program playing a board game such as Go. It only gets a reward if it wins.\n",
    "\n",
    "- The agent does not have to control a physically (or virtually) moving thing. For example, it can be a smart thermostat, getting positive rewards whenever it is close to the target temperature and saves energy, and negative rewards when humans need to tweak the temperature, so the agent must learn to anticipate human needs.\n",
    "\n",
    "- The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the monetary gains and losses.\n",
    "\n",
    "Note that there may not be any positive rewards at all; for example, the agent may move around in a maze, getting a negative reward at every time step, so it had better find the exit as quickly as possible! There are many other examples of tasks to which reinforcement learning is well suited, such as self-driving cars, recommender systems, placing ads on a web page, or controlling where an image classification system should focus its attention.\n",
    "\n",
    "## **Policy Search**\n",
    "The algorithm a software agent uses to determine its actions is called its policy. The policy could be a neural network taking observations as inputs and outputting the action to take (see Figure 18-2) from the book.\n",
    "\n",
    "The policy can be any algorithm you can think of, and it does not have to be deterministic. In fact, in some cases it does not even have to observe the environment! For example, consider a robotic vacuum cleaner whose reward is the amount of dust it picks up in 30 minutes. Its policy could be to move forward with some probability p every second, or randomly rotate left or right with probability 1 – p. The rotation angle would be a random angle between –r and +r. Since this policy involves some randomness, it is called a stochastic policy. The robot will have an erratic trajectory, which guarantees that it will eventually get to any place it can reach and pick up all the dust. The question is, how much dust will it pick up in 30 minutes?\n",
    "\n",
    "How would you train such a robot? There are just two policy parameters you can tweak: the probability p and the angle range r. One possible learning algorithm could be to try out many different values for these parameters, and pick the combination that performs best (see Figure 18-3) from the book. This is an example of policy search, in this case using a brute-force approach. When the policy space is too large (which is generally the case), finding a good set of parameters this way is like searching for a needle in a gigantic haystack.\n",
    "\n",
    "Another way to explore the policy space is to use genetic algorithms. For example, you could randomly create a first generation of 100 policies and try them out, then “kill” the 80 worst policies and make the 20 survivors produce 4 offspring each. An offspring is a copy of its parent plus some random variation. The surviving policies plus their offspring together constitute the second generation. You can continue to iterate through generations this way until you find a good policy\n",
    "\n",
    "Yet another approach is to use optimization techniques, by evaluating the gradients of the rewards with regard to the policy parameters, then tweaking these parameters by following the gradients toward higher rewards. We will discuss this approach, called policy gradients (PG), in more detail later in this chapter. Going back to the vacuum cleaner robot, you could slightly increase p and evaluate whether doing so increases the amount of dust picked up by the robot in 30 minutes; if it does, then increase p some more, or else reduce p. We will implement a popular PG algorithm using TensorFlow, but before we do, we need to create an environment for the agent to live in—so it’s time to introduce OpenAI Gym.\n",
    "\n",
    "## **Introduction to OpenAI Gym**\n",
    "One of the challenges of reinforcement learning is that in order to train an agent, you first need to have a working environment. If you want to program an agent that will learn to play an Atari game, you will need an Atari game simulator. If you want to program a walking robot, then the environment is the real world, and you can directly train your robot in that environment. However, this has its limits: if the robot falls off a cliff, you can’t just click Undo. You can’t speed up time either—adding more computing power won’t make the robot move any faster—and it’s generally too expensive to train 1,000 robots in parallel. In short, training is hard and slow in the real world, so you generally need a simulated environment at least for bootstrap training. For example, you might use a library like [PyBullet](https://pybullet.org) or [MuJoCo](https://mujoco.org) for 3D physics simulation.\n",
    "\n",
    "[OpenAI Gym](https://gym.openai.com) is a toolkit that provides a wide variety of simulated environments (Atari games, board games, 2D and 3D physical simulations, and so on), that you can use to train agents, compare them, or develop new RL algorithms.\n",
    "\n",
    "OpenAI Gym is preinstalled on Colab, but it’s an older version, so you’ll need to replace it with the latest one. You also need to install a few of its dependencies. If you are coding on your own machine instead of Colab, and you followed the installation instructions at https://homl.info/install, then you can skip this step; otherwise, enter these commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b63d51e0",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Only run these commands on Colab or Kaggle!\n",
    "%pip install -q -U gym\n",
    "%pip install -q -U gym[classic_control,box2d,atari,accept-rom-license]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e3d2fd",
   "metadata": {},
   "source": [
    "The first %pip command upgrades Gym to the latest version. The -q option stands for quiet: it makes the output less verbose. The -U option stands for upgrade. The second %pip command installs the libraries required to run various kinds of environments. This includes classic environments from control theory–the science of controlling dynamical systems–such as balancing a pole on a cart. It also includes environments based on the Box2D library— a 2D physics engine for games. Lastly, it includes environments based on the Arcade Learning Environment (ALE), which is an emulator for Atari 2600 games. Several Atari game ROMs are downloaded automatically, and by running this code you agree with Atari’s ROM licenses.\n",
    "\n",
    "With that, you're ready to use OpenAI Gym. Let's import it and make an environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35c4c133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca5b44d",
   "metadata": {},
   "source": [
    "Here, we’ve created a CartPole environment. This is a 2D simulation in which a cart can be accelerated left or right in order to balance a pole placed on top of it (see Figure 18-4) from the book. This is a classic control task.\n",
    "\n",
    "After the environment is created, you must initialize it using the reset() method, optionally specifying a random seed. This returns the first observation. Observations depend on the type of environment. For the CartPole environment, each observation is a 1D NumPy array containing four floats representing the cart’s horizontal position (0.0 = center), its velocity (positive means right), the angle of the pole (0.0 = vertical), and its angular velocity (positive means clockwise). The reset() method also returns a dictionary that may contain extra environment-specific information. This can be useful for debugging or for training. For example, in many Atari environments, it contains the number of lives left. However, in the CartPole environment, this dictionary is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbe00dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, info = env.reset(seed=50)\n",
    "obs\n",
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4febe74",
   "metadata": {},
   "source": [
    "Let’s call the ***render()*** method to render this environment as an image. Since we set ***render_mode=\"rgb_array\"*** when creating the environment, the image will be returned as a NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2569edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 600, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = env.render()\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f30745",
   "metadata": {},
   "source": [
    "You can then use Matplotlib’s ***imshow()*** function to display this image, as usual. Now let’s ask the environment what actions are possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fe6bd04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0d02d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02940961  0.22841987  0.00573734 -0.24382275]\n",
      "1.0\n",
      "False\n",
      "False\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "action = 1 # accelerate right\n",
    "obs, reward, done, truncated, info = env.step(action)\n",
    "print(obs)\n",
    "print(reward)\n",
    "print(done)\n",
    "print(truncated)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8072d17",
   "metadata": {},
   "source": [
    "The step() method executes the desired action and returns five values:\n",
    "***obs***\n",
    "- This is the new observation. The cart is now moving toward the right (obs[1] > 0). The pole is still tilted toward the right (obs[2] > 0), but its angular velocity is now negative (obs[3] < 0), so it will likely be tilted toward the left after the next step.\n",
    "\n",
    "- ***reward***\n",
    "  In this environment, you get a reward of 1.0 at every step, no matter what you do, so the goal is to keep the episode running for as long as possible.\n",
    "\n",
    "- ***done***\n",
    "  This value will be True when the episode is over. This will happen when the pole tilts too much, or goes off the screen, or after 200 steps (in this last case, you have won). After that, the environment must be reset before it can be used again.\n",
    "\n",
    "- ***truncated***\n",
    "  This value will be True when an episode is interrupted early, for example by an environment wrapper that imposes a maximum number of steps per episode (see Gym’s documentation for more details on environment wrappers). Some RL algorithms treat truncated episodes differently from episodes finished normally (i.e., when done is True), but in this chapter we will treat them identically.\n",
    "\n",
    "- ***info***\n",
    "  This environment-specific dictionary may provide extra information, just like the one returned by the ***reset()*** method.\n",
    "\n",
    "> #### **TIP**\n",
    "> Once you have finished using an environment, you should call its close() method to free resources.\n",
    "\n",
    "Let’s hardcode a simple policy that accelerates left when the pole is leaning toward the left and accelerates right when the pole is leaning toward the right. We will run this policy to see the average rewards it gets over 500 episodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0799ac8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_policy(obs):\n",
    "    angle = obs[2]\n",
    "    return 0 if angle < 0 else 1\n",
    "\n",
    "totals = []\n",
    "for episode in range(500):\n",
    "    episode_rewards = 0\n",
    "    obs, info = env.reset(seed=episode)\n",
    "    for step in range(200):\n",
    "        action = basic_policy(obs)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        episode_rewards += reward\n",
    "        if done or truncated:\n",
    "            break\n",
    "        \n",
    "        totals.append(episode_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e41458",
   "metadata": {},
   "source": [
    "This code is self-explanatory. Let's look at the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e70661d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21.713696004717676, 13.151677912691767, 1.0, 62.0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean(totals), np.std(totals), min(totals), max(totals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96f89af",
   "metadata": {},
   "source": [
    "Even with 500 tries, this policy never managed to keep the pole upright for more than 63 consecutive steps. Not great. If you look at the simulation in this chapter’s notebook, you will see that the cart oscillates left and right more and more strongly until the pole tilts too much. Let’s see if a neural network can come up with a better policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52609e59",
   "metadata": {},
   "source": [
    "## **Neural Network Policies**\n",
    "Let’s create a neural network policy. This neural network will take an observation as input, and it will output the action to be executed, just like the policy we hardcoded earlier. More precisely, it will estimate a probability for each action, and then we will select an action randomly, according to the estimated probabilities (see Figure 18-5). In the case of the CartPole environment, there are just two possible actions (left or right), so we only need one output neuron. It will output the probability p of action 0 (left), and of course the probability of action 1 (right) will be 1 – p. For example, if it outputs 0.7, then we will pick action 0 with 70% probability, or action 1 with 30% probability.\n",
    "\n",
    "You may wonder why we are picking a random action based on the probabilities given by the neural network, rather than just picking the action with the highest score. This approach lets the agent find the right balance between exploring new actions and exploiting the actions that are known to work well. Here’s an analogy: suppose you go to a restaurant for the first time, and all the dishes look equally appealing, so you randomly pick one. If it turns out to be good, you can increase the probability that you’ll order it next time, but you shouldn’t increase that probability up to 100%, or else you will never try out the other dishes, some of which may be even better than the one you tried. This exploration/exploitation dilemma is central in reinforcement learning.\n",
    "\n",
    "Also note that in this particular environment, the past actions and observations can safely be ignored, since each observation contains the environment’s full state. If there were some hidden state, then you might need to consider past actions and observations as well. For example, if the environment only revealed the position of the cart but not its velocity, you would have to consider not only the current observation but also the previous observation in order to estimate the current velocity. Another example is when the observations are noisy; in that case, you generally want to use the past few observations to estimate the most likely current state. The CartPole problem is thus as simple as can be; the observations are noise-free, and they contain the environment’s full state.\n",
    "\n",
    "Here is the code to build a basic neural network policy using Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "beee7250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(5, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e76a72f",
   "metadata": {},
   "source": [
    "We use a Sequential model to define the policy network. The number of inputs is the size of the observation space—which in the case of CartPole is 4—and we have just five hidden units because it’s a fairly simple task. Finally, we want to output a single probability—the probability of going left—so we have a single output neuron using the sigmoid activation function. If there were more than two possible actions, there would be one output neuron per action, and we would use the softmax activation function instead.\n",
    "\n",
    "OK, we now have a neural network policy that will take observations and output action probabilities. But how do we train it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440c13ed",
   "metadata": {},
   "source": [
    "## **Evaluating Auctions: The Credit Assignment Problem**\n",
    "If we knew what the best action was at each step, we could train the neural network as usual, by minimizing the cross entropy between the estimated probability distribution and the target probability distribution. It would just be regular supervised learning. However, in reinforcement learning the only guidance the agent gets is through rewards, and rewards are typically sparse and delayed. For example, if the agent manages to balance the pole for 100 steps, how can it know which of the 100 actions it took were good, and which of them were bad? All it knows is that the pole fell after the last action, but surely this last action is not entirely responsible. This is called the credit assignment problem: when the agent gets a reward, it is hard for it to know which actions should get credited (or blamed) for it. Think of a dog that gets rewarded hours after it behaved well; will it understand what it is being rewarded for?\n",
    "\n",
    "To tackle this problem, a common strategy is to evaluate an action based on the sum of all the rewards that come after it, usually applying a discount factor, γ (gamma), at each step. This sum of discounted rewards is called the action’s return. Consider the example in Figure 18-6 from the book. If an agent decides to go right three times in a row and gets +10 reward after the first step, 0 after the second step, and finally –50 after the third step, then assuming we use a discount factor γ = 0.8, the first action will have a return of 10 + γ × 0 + γ × (–50) = –22. If the discount factor is close to 0, then future rewards won’t count for much compared to immediate rewards. Conversely, if the discount factor is close to 1, then rewards far into the future will count almost as much as immediate rewards. Typical discount factors vary from 0.9 to 0.99. With a discount factor of 0.95, rewards 13 steps into the future count roughly for half as much as immediate rewards (since 0.95 ≈ 0.5), while with a discount factor of 0.99, rewards 69 steps into the future count for half as much as immediate rewards. In the CartPole environment, actions have fairly short-term effects, so choosing a discount factor of 0.95 seems reasonable.\n",
    "\n",
    "Of course, a good action may be followed by several bad actions that cause the pole to fall quickly, resulting in the good action getting a low return. Similarly, a good actor may sometimes star in a terrible movie. However, if we play the game enough times, on average good actions will get a higher return than bad ones. We want to estimate how much better or worse an action is, compared to the other possible actions, on average. This is called the action advantage. For this, we must run many episodes and normalize all the action returns, by subtracting the mean and dividing by the standard deviation. After that, we can reasonably assume that actions with a negative advantage were bad while actions with a positive advantage were good. OK, now that we have a way to evaluate each action, we are ready to train our first agent using policy gradients. Let’s see how."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e638d656",
   "metadata": {},
   "source": [
    "## **Policy Gradients**\n",
    "As discussed earlier, PG algorithms optimize the parameters of a policy by following the gradients toward higher rewards. One popular class of PG algorithms, called REINFORCE algorithms, was [introduced back in 1992](https://homl.info/132) by Ronald Williams. Here is one common variant:\n",
    "1. First, let the neural network policy play the game several times, and at each step, compute the gradients that would make the chosen action even more likely—but don’t apply these gradients yet.\n",
    "\n",
    "2. Once you have run several episodes, compute each action’s advantage, using the method described in the previous section.\n",
    "   \n",
    "3. If an action’s advantage is positive, it means that the action was probably good, and you want to apply the gradients computed earlier to make the action even more likely to be chosen in the future. However, if the action’s advantage is negative, it means the action was probably bad, and you want to apply the opposite gradients to make this action slightly less likely in the future. The solution is to multiply each gradient vector by the corresponding action’s advantage.\n",
    "   \n",
    "4. Finally, compute the mean of all the resulting gradient vectors, and use it to perform a gradient descent step.\n",
    "\n",
    "Let’s use Keras to implement this algorithm. We will train the neural network policy we built earlier so that it learns to balance the pole on the cart. First, we need a function that will play one step. We will pretend for now that whatever action it takes is the right one so that we can compute the loss and its gradients. These gradients will just be saved for a while, and we will modify them later depending on how good or bad the action turned out to be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba9115de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, obs, model, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        left_proba = model(obs[np.newaxis])\n",
    "        action = (tf.random.uniform([1, 1]) > left_proba)\n",
    "        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32)\n",
    "        loss = tf.reduce_mean(loss_fn(y_target, left_proba))\n",
    "        \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    obs, reward, done, truncated, info = env.step(int(action))\n",
    "    return obs, reward, done, truncated, grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d4bc12",
   "metadata": {},
   "source": [
    "Let’s walk though this function:\n",
    "- Within the GradientTape block (see Chapter 12), we start by calling the model, giving it a single observation. We reshape the observation so it becomes a batch containing a single instance, as the model expects a batch. This outputs the probability of going left.\n",
    "\n",
    "- Next, we sample a random float between 0 and 1, and we check whether it is greater than left_proba. The action will be False with probability left_proba, or True with probability 1 – left_proba. Once we cast this Boolean to an integer, the action will be 0 (left) or 1 (right) with the appropriate probabilities.\n",
    "  \n",
    "- We now define the target probability of going left: it is 1 minus the action (cast to a float). If the action is 0 (left), then the target probability of going left will be 1. If the action is 1 (right), then the target probability will be 0.\n",
    "\n",
    "- Then we compute the loss using the given loss function, and we use the tape to compute the gradient of the loss with regard to the model’s trainable variables. Again, these gradients will be tweaked later, before we apply them, depending on how good or bad the action turned out to be.\n",
    "\n",
    "- Finally, we play the selected action, and we return the new observation, the reward, whether the episode is ended or not, whether it is truncated or not, and of course the gradients that we just computed.\n",
    "\n",
    "Now let’s create another function that will rely on the ***play_one_step()*** function to play multiple episodes, returning all the rewards and gradients for each episode and each step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27df325e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
    "    all_rewards = []\n",
    "    all_grads = []\n",
    "    for episode in range(n_episodes):\n",
    "        current_rewards = []\n",
    "        current_grads = []\n",
    "        obs, info = env.reset()\n",
    "        for step in range(n_max_steps):\n",
    "            obs, reward, done, truncated, grads = play_one_step(\n",
    "                env, obs, model, loss_fn)\n",
    "            current_rewards.append(reward)\n",
    "            current_grads.append(grads)\n",
    "            if done or truncated:\n",
    "                break\n",
    "        all_rewards.append(current_rewards)\n",
    "        all_grads.append(current_grads)\n",
    "        \n",
    "    return all_rewards, all_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203ef093",
   "metadata": {},
   "source": [
    "This code returns a list of reward lists: one reward list per episode, containing one reward per step. It also returns a list of gradient lists: one gradient list per episode, each containing one tuple of gradients per step and each tuple containing one gradient tensor per trainable variable.\n",
    "\n",
    "The algorithm will use the play_multiple_episodes() function to play the game several times (e.g., 10 times), then it will go back and look at all the rewards, discount them, and normalize them. To do that, we need a couple more functions; the first will compute the sum of future discounted rewards at each step, and the second will normalize all these discounted rewards (i.e., the returns) across many episodes by subtracting the mean and dividing by the standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce1b43ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, discount_factor):\n",
    "    discounted = np.array(rewards)\n",
    "    for step in range(len(rewards) - 2, -1, -1):\n",
    "        discounted[step] += discounted[step + 1] * discount_factor\n",
    "    return discounted\n",
    "\n",
    "def discount_and_normalize_rewards(all_rewards, discount_factor):\n",
    "    all_discounted_rewards = [discount_rewards(rewards, discount_factor)\n",
    "                            for rewards in all_rewards]\n",
    "    flat_rewards = np.concatenate(all_discounted_rewards)\n",
    "    reward_mean = flat_rewards.mean()\n",
    "    reward_std = flat_rewards.std()\n",
    "    return [(discounted_rewards - reward_mean) / reward_std\n",
    "            for discounted_rewards in all_discounted_rewards]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c273cbea",
   "metadata": {},
   "source": [
    "Let’s check that this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "240d2140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-22, -40, -50])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_rewards([10, 0, -50], discount_factor=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae6604aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.28435071, -0.86597718, -1.18910299]),\n",
       " array([1.26665318, 1.0727777 ])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_and_normalize_rewards([[10, 0, -50], [10, 20]], discount_factor=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e760bfb",
   "metadata": {},
   "source": [
    "The call to discount_rewards() returns exactly what we expect (see Figure 18-6) from the book. You can verify that the function discount_and_normalize_rewards() does indeed return the normalized action advantages for each action in both episodes. Notice that the first episode was much worse than the second, so its normalized advantages are all negative; all actions from the first episode would be considered bad, and conversely all actions from the second episode would be considered good. \n",
    "\n",
    "We are almost ready to run the algorithm! Now let’s define the hyperparameters. We will run 150 training iterations, playing 10 episodes per iteration, and each episode will last at most 200 steps. We will use a discount factor of 0.95:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17427693",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 150\n",
    "n_episodes_per_update = 10\n",
    "n_max_steps = 200\n",
    "discount_factor = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8385da1b",
   "metadata": {},
   "source": [
    "We also need an optimizer and the loss function. A regular Nadam optimizer with learning rate 0.01 will do just fine, and we will use the binary cross-entropy loss function because we are training a binary classifier (there are two possible actions—left or right):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c84c4953",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=0.01)\n",
    "loss_fn = tf.keras.losses.binary_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03426b4",
   "metadata": {},
   "source": [
    "We are now ready to build and run the training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2dbb8811",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iterations):\n\u001b[0;32m----> 2\u001b[0m     all_rewards, all_grads \u001b[38;5;241m=\u001b[39m \u001b[43mplay_multiple_episodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_episodes_per_update\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_max_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     all_final_rewards \u001b[38;5;241m=\u001b[39m discount_and_normalize_rewards(all_rewards,\n\u001b[1;32m      5\u001b[0m                                                         discount_factor)\n\u001b[1;32m      7\u001b[0m     all_mean_grads \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[20], line 9\u001b[0m, in \u001b[0;36mplay_multiple_episodes\u001b[0;34m(env, n_episodes, n_max_steps, model, loss_fn)\u001b[0m\n\u001b[1;32m      7\u001b[0m obs, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_max_steps):\n\u001b[0;32m----> 9\u001b[0m     obs, reward, done, truncated, grads \u001b[38;5;241m=\u001b[39m \u001b[43mplay_one_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     current_rewards\u001b[38;5;241m.\u001b[39mappend(reward)\n\u001b[1;32m     12\u001b[0m     current_grads\u001b[38;5;241m.\u001b[39mappend(grads)\n",
      "Cell \u001b[0;32mIn[19], line 6\u001b[0m, in \u001b[0;36mplay_one_step\u001b[0;34m(env, obs, model, loss_fn)\u001b[0m\n\u001b[1;32m      4\u001b[0m     action \u001b[38;5;241m=\u001b[39m (tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m>\u001b[39m left_proba)\n\u001b[1;32m      5\u001b[0m     y_target \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant([[\u001b[38;5;241m1.\u001b[39m]]) \u001b[38;5;241m-\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(action, tf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m----> 6\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_proba\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, model\u001b[38;5;241m.\u001b[39mtrainable_variables)\n\u001b[1;32m      9\u001b[0m obs, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;28mint\u001b[39m(action))\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py:88\u001b[0m, in \u001b[0;36mweak_tensor_unary_op_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     87\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     90\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1260\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1260\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1262\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1263\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/tensorflow/python/ops/math_ops.py:2594\u001b[0m, in \u001b[0;36mreduce_mean\u001b[0;34m(input_tensor, axis, keepdims, name)\u001b[0m\n\u001b[1;32m   2541\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Computes the mean of elements across dimensions of a tensor.\u001b[39;00m\n\u001b[1;32m   2542\u001b[0m \n\u001b[1;32m   2543\u001b[0m \u001b[38;5;124;03mReduces `input_tensor` along the dimensions given in `axis` by computing the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2588\u001b[0m \u001b[38;5;124;03m@end_compatibility\u001b[39;00m\n\u001b[1;32m   2589\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2590\u001b[0m keepdims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(keepdims)\n\u001b[1;32m   2591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _may_reduce_to_scalar(\n\u001b[1;32m   2592\u001b[0m     keepdims, axis,\n\u001b[1;32m   2593\u001b[0m     gen_math_ops\u001b[38;5;241m.\u001b[39mmean(\n\u001b[0;32m-> 2594\u001b[0m         input_tensor, \u001b[43m_ReductionDims\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m, keepdims,\n\u001b[1;32m   2595\u001b[0m         name\u001b[38;5;241m=\u001b[39mname))\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/tensorflow/python/ops/math_ops.py:2089\u001b[0m, in \u001b[0;36m_ReductionDims\u001b[0;34m(x, axis)\u001b[0m\n\u001b[1;32m   2087\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2088\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2089\u001b[0m     x_rank \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[38;5;241m.\u001b[39mrank\n\u001b[1;32m   2090\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m   2091\u001b[0m     x_rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:527\u001b[0m, in \u001b[0;36m_EagerTensorBase.shape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensor_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# pylint: disable=access-member-before-definition\u001b[39;00m\n\u001b[1;32m    523\u001b[0m   \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    524\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;66;03m# `_tensor_shape` is declared and defined in the definition of\u001b[39;00m\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;66;03m# `EagerTensor`, in C.\u001b[39;00m\n\u001b[0;32m--> 527\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensor_shape \u001b[38;5;241m=\u001b[39m \u001b[43mtensor_shape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensorShape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shape_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ai_env/lib/python3.10/site-packages/tensorflow/python/framework/tensor_shape.py:830\u001b[0m, in \u001b[0;36mTensorShape.__init__\u001b[0;34m(self, dims)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new TensorShape with the given dimensions.\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \n\u001b[1;32m    823\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;124;03m  TypeError: If dims cannot be converted to a list of dimensions.\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dims, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):  \u001b[38;5;66;03m# Most common case.\u001b[39;00m\n\u001b[0;32m--> 830\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(as_dimension(d)\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dims)\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    832\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for iteration in range(n_iterations):\n",
    "    all_rewards, all_grads = play_multiple_episodes(\n",
    "        env, n_episodes_per_update, n_max_steps, model, loss_fn)\n",
    "    all_final_rewards = discount_and_normalize_rewards(all_rewards,\n",
    "                                                        discount_factor)\n",
    "    \n",
    "    all_mean_grads = []\n",
    "    for var_index in range(len(model.trainable_variables)):\n",
    "        mean_grads = tf.reduce_mean(\n",
    "            [final_reward * all_grads[episode_index][step][var_index]\n",
    "            for episode_index, final_rewards in enumerate(all_final_rewards)\n",
    "                for step, final_reward in enumerate(final_rewards)], axis=0)\n",
    "        all_mean_grads.append(mean_grads)\n",
    "        \n",
    "    optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3433df59",
   "metadata": {},
   "source": [
    "Let’s walk through this code:\n",
    "- At each training iteration, this loop calls the play_multiple_episodes() function, which plays 10 episodes and returns the rewards and gradients for each step in each episode.\n",
    "  \n",
    "- Then we call the discount_and_normalize_rewards() function to compute each action’s normalized advantage, called the final_reward in this code. This provides a measure of how good or bad each action actually was, in hindsight.\n",
    "  \n",
    "- Next, we go through each trainable variable, and for each of them we compute the weighted mean of the gradients for that variable over all episodes and all steps, weighted by the final_reward.\n",
    "  \n",
    "- Finally, we apply these mean gradients using the optimizer: the model’s trainable variables will be tweaked, and hopefully the policy will be a bit better.\n",
    "  \n",
    "And we’re done! This code will train the neural network policy, and it will successfully learn to balance the pole on the cart. The mean reward per episode will get very close to 200. By default, that’s the maximum for this environment. Success!\n",
    "\n",
    "The simple policy gradients algorithm we just trained solved the CartPole task, but it would not scale well to larger and more complex tasks. Indeed, it is highly sample inefficient, meaning it needs to explore the game for a very long time before it can make significant progress. This is due to the fact that it must run multiple episodes to estimate the advantage of each action, as we have seen. However, it is the foundation of more powerful algorithms, such as actor-critic algorithms (which we will discuss briefly at the end of this chapter).\n",
    "\n",
    "> #### **TIP**\n",
    "> Researchers try to find algorithms that work well even when the agent initially knows nothing about the environment. However, unless you are writing a paper, you should not hesitate to inject prior knowledge into the agent, as it will speed up training dramatically. For example, since you know that the pole should be as vertical as possible, you could add negative rewards proportional to the pole’s angle. This will make the rewards much less sparse and speed up training. Also, if you already have a reasonably good policy (e.g., hardcoded), you may want to train the neural network to imitate it before using policy gradients to improve it.\n",
    "\n",
    "We will now look at another popular family of algorithms. Whereas PG algorithms directly try to optimize the policy to increase rewards, the algorithms we will explore now are less direct: the agent learns to estimate the expected return for each state, or for each action in each state, then it uses this knowledge to decide how to act. To understand these algorithms, we must first consider Markov decision processes (MDPs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc812436",
   "metadata": {},
   "source": [
    "## **Markov Decision Processes**\n",
    "In the early 20th century, the mathematician Andrey Markov studied stochastic processes with no memory, called Markov chains. Such a process has a fixed number of states, and it randomly evolves from one state to another at each step. The probability for it to evolve from a state s to a state s′ is fixed, and it depends only on the pair (s, s′), not on past states. This is why we say that the system has no memory.\n",
    "\n",
    "Figure 18-7 from the book shows an example of a Markov chain with four states.\n",
    "\n",
    "![Example of a Markov chain](markov1.png)\n",
    "\n",
    "Suppose that the process starts in state s , and there is a 70% chance that it will remain in that state at the next step. Eventually it is bound to leave that state and never come back, because no other state points back to s . If it goes to state s , it will then most likely go to state s (90% probability), then immediately back to state s (with 100% probability). It may alternate a number of times between these two states, but eventually it will fall into state s and remain there forever, since there’s no way out: this is called a terminal state. Markov chains can have very different dynamics, and they are heavily used in thermodynamics, chemistry, statistics, and much more.\n",
    "\n",
    "Markov decision processes were first described in the 1950s by [Richard Bellman](https://homl.info/133). They resemble Markov chains, but with a twist: at each step, an agent can choose one of several possible actions, and the transition probabilities depend on the chosen action. Moreover, some state transitions return some reward (positive or negative), and the agent’s goal is to find a policy that will maximize reward over time.\n",
    "\n",
    "For example, the MDP represented in Figure 18-8 from the book has three states (represented by circles) and up to three possible discrete actions at each step (represented by diamonds).\n",
    "\n",
    "![Example of a Markov decision process](markov2.png)\n",
    "\n",
    "f it starts in state s , the agent can choose between actions a , a , or a . If it chooses action a , it just remains in state s with certainty, and without any reward. It can thus decide to stay there forever if it wants to. But if it chooses action a , it has a 70% probability of gaining a reward of +10 and remaining in state s . It can then try again and again to gain as much reward as possible, but at one point it is going to end up instead in state s . In state s it has only two possible actions: a or a . It can choose to stay put by repeatedly choosing action a , or it can choose to move on to state s and get a negative reward of –50 (ouch). In state s it has no choice but to take action a , which will most likely lead it back to state s , gaining a reward of +40 on the way. You get the picture. By looking at this MDP, can you guess which strategy will gain the most reward over time? In state s it is clear that action a is the best option, and in state s the agent has no choice but to take action a , but in state s it is not obvious whether the agent should stay put (a<sub>0</sub>) or go through the fire (a<sub>2</sub>).\n",
    "\n",
    "Bellman found a way to estimate the optimal state value of any state s, noted V*(s), which is the sum of all discounted future rewards the agent can expect on average after it reaches the state, assuming it acts optimally. He showed that if the agent acts optimally, then the Bellman optimality equation applies (see Equation 18-1) from the book. This recursive equation says that if the agent acts optimally, then the optimal value of the current state is equal to the reward it will get on average after taking one optimal action, plus the expected optimal value of all possible next states that this action can lead to.\n",
    "\n",
    "V<sup><sup>*</sup></sup>(s) =max<sub><sub>a</sub></sub> ∑<sub><sub>s′</sub></sub>T (s, a, s′) [R (s, a, s′) + γ⋅V<sup><sup>*</sup></sup>(s′)] for all s\n",
    "\n",
    "In this equation:\n",
    "- T(s, a, s′) is the transition probability from state s to state s′, given that the agent chose action a. For example, in Figure 18-8, T(s , a , s ) = 0.8.\n",
    "  \n",
    "- R(s, a, s′) is the reward that the agent gets when it goes from state s to state s′, given that the agent chose action a. For example, in Figure 18-8, R(s , a , s ) = +40.\n",
    "  \n",
    "- γ is the discount factor.\n",
    "\n",
    "This equation leads directly to an algorithm that can precisely estimate the optimal state value of every possible state: first initialize all the state value estimates to zero, and then iteratively update them using the value iteration algorithm (see Equation 18-2). A remarkable result is that, given enough time, these estimates are guaranteed to converge to the optimal state values, corresponding to the optimal policy.\n",
    "\n",
    "V<sub><sub>k+1</sub></sub>(s) =max<sub><sub>a</sub></sub> ∑<sub><sub>s′</sub></sub>T (s, a, s′) [R (s, a, s′) + γ⋅V<sub><sub>k</sub></sub>(s′)] for all s\n",
    "\n",
    "In this equation, V<sub>k</sub>(s) is the estimated value of state s at the k<sup>th</sup> iteration of the algorithm.\n",
    "\n",
    "> #### **NOTE**\n",
    "> This algorithm is an example of dynamic programming, which breaks down a complex problem into tractable subproblems that can be tackled iteratively.\n",
    "\n",
    "Knowing the optimal state values can be useful, in particular to evaluate a policy, but it does not give us the optimal policy for the agent. Luckily, Bellman found a very similar algorithm to estimate the optimal state-action values, generally called Q-values (quality values). The optimal Q-value of the state-action pair (s, a), noted Q*(s, a), is the sum of discounted future rewards the agent can expect on average after it reaches the state s and chooses action a, but before it sees the outcome of this action, assuming it acts optimally after that action.\n",
    "\n",
    "Let’s look at how it works. Once again, you start by initializing all the Q-value estimates to zero, then you update them using the Q-value iteration algorithm (see Equation 18-3) from the book.\n",
    "\n",
    "Q<sub>k+1</sub> (s, a) ← ∑<sup><sup>s′</sup></sup> T(s, a, s′) [R(s, a, s′) + γ⋅max<sub><sub>a′</sub></sub> Q<sub>k</sub>(s′, a′)] for all (s, a)\n",
    "\n",
    "Once you have the optimal Q-values, defining the optimal policy, noted π*(s), is trivial; when the agent is in state s, it should choose the action with the highest Q-value for that state: π*(s) =argmax<sub><sub>a</sub></sub> Q*(s, a).\n",
    "\n",
    "Let's apply this algorithm to the MDP represented in Figure 18-8. First, we need to define the MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "78bb0fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probabilities = [\n",
    "    [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]], \n",
    "    [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]], \n",
    "    [None, [0.8, 0.1, 0.1], None]\n",
    "]\n",
    "\n",
    "rewards = [ # shape=[s, a, s']\n",
    "    [[+10, 0, 0], [0, 0, 0], [0, 0, 0]],\n",
    "    [[0, 0, 0], [0, 0, 0], [0, 0, -50]],\n",
    "    [[0, 0, 0], [+40, 0, 0], [0, 0, 0]]\n",
    "]\n",
    "\n",
    "possible_actions = [[0, 1, 2], [0, 2], [1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98803842",
   "metadata": {},
   "source": [
    "For example, to know the transition probability of going from s to s after playing action a , we will look up transition_probabilities[2][1][0] (which is 0.8). Similarly, to get the corresponding reward, we will look up rewards[2][1][0] (which is +40). And to get the list of possible actions in s , we will look up possible_actions[2] (in this case, only action a is possible). Next, we must initialize all the Q-values to zero (except for the impossible actions, for which we set the Q-values to –∞):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8cef82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "Q_values = np.full((3, 3), -np.inf) # -np.inf for impossible actions\n",
    "for state, actions in enumerate(possible_actions):\n",
    "    Q_values[state, actions] = 0.0 # for all possible actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20d3107",
   "metadata": {},
   "source": [
    "Now let’s run the Q-value iteration algorithm. It applies Equation 18-3 from the book repeatedly, to all Q-values, for every state and every possible action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "31f60fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9 # The discount factor\n",
    "\n",
    "for iteration in range(50):\n",
    "    Q_prev = Q_values.copy()\n",
    "    for s in range(3):\n",
    "        for a in possible_actions[s]:\n",
    "            Q_values[s, a] = np.sum([\n",
    "                transition_probabilities[s][a][sp]\n",
    "                * (rewards[s][a][sp] + gamma * Q_prev[sp].max())\n",
    "            for sp in range(3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853ff608",
   "metadata": {},
   "source": [
    "That's it! The resulting Q-values look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51a5c8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18.91891892, 17.02702702, 13.62162162],\n",
       "       [ 0.        ,        -inf, -4.87971488],\n",
       "       [       -inf, 50.13365013,        -inf]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1bc6f9",
   "metadata": {},
   "source": [
    "For example, when the agent is in state s and it chooses action a , the expected sum of discounted future rewards is approximately 17.0.\n",
    "\n",
    "For each state, we can find the action that has the highest Q_value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e28a1aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_values.argmax(axis=1) # Optimal action for each state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c9787c",
   "metadata": {},
   "source": [
    "This gives us the optimal policy for this MDP when using a discount factor of 0.90: in state s choose action a , in state s choose action a (i.e., stay put), and in state s choose action a (the only possible action). Interestingly, if we increase the discount factor to 0.95, the optimal policy changes: in state s the best action becomes a (go through the fire!). This makes sense because the more you value future rewards, the more you are willing to put up with some pain now for the promise of future bliss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e8aa25",
   "metadata": {},
   "source": [
    "## **Temporal Difference Learning**\n",
    "Reinforcement learning problems with discrete actions can often be modeled as Markov decision processes, but the agent initially has no idea what the transition probabilities are (it does not know T(s, a, s′)), and it does not know what the rewards are going to be either (it does not know R(s, a, s′)). It must experience each state and each transition at least once to know the rewards, and it must experience them multiple times if it is to have a reasonable estimate of the transition probabilities.\n",
    "\n",
    "The ***temporal difference***(**TD**) learning algorithm is very similar to the Q-value iteration algorithm, but tweaked to take into account the fact that the agent has only partial knowledge of the MDP. In general we assume that the agent initially knows only the possible states and actions, and nothing more. The agent uses an exploration policy —for example, a purely random policy—to explore the MDP, and as it progresses, the TD learning algorithm updates the estimates of the state values based on the transitions and rewards that are actually observed (see Equation 18-4 from the book).\n",
    "\n",
    "![TD learning algorithm](tdqn.png)\n",
    "\n",
    "> #### **TIP**\n",
    "> TD learning has many similarities with stochastic gradient descent, including the fact that it handles one sample at a time. Moreover, just like SGD, it can only truly converge if you gradually reduce the learning rate; otherwise, it will keep bouncing around the optimum Qvalues.\n",
    "\n",
    "For each state s, this algorithm keeps track of a running average of the immediate rewards the agent gets upon leaving that state, plus the rewards it expects to get later, assuming it acts optimally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e7880e",
   "metadata": {},
   "source": [
    "## **Q-Learning**\n",
    "Similarly, the Q-learning algorithm is an adaptation of the Q-value iteration algorithm to the situation where the transition probabilities and the rewards are initially unknown (see Equation 18-5). Q-learning works by watching an agent play (e.g., randomly) and gradually improving its estimates of the Q-values. Once it has accurate Q-value estimates (or close enough), then the optimal policy is just choosing the action that has the highest Q-value (i.e., the greedy policy).\n",
    "\n",
    "Q(s, a) ←<sub><sub>α</sub></sub> r + γ ⋅ max<sub><sub>a′</sub></sub>Q(s′, a′)\n",
    "\n",
    "For each state-action pair (s, a), this algorithm keeps track of a running average of the rewards r the agent gets upon leaving the state s with action a, plus the sum of discounted future rewards it expects to get. To estimate this sum, we take the maximum of the Q-value estimates for the next state s′, since we assume that the target policy will act optimally from then on.\n",
    "\n",
    "Let’s implement the Q-learning algorithm. First, we will need to make an agent explore the environment. For this, we need a step function so that the agent can execute one action and get the resulting state and reward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b345ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(state, action):\n",
    "    probas = transition_probabilities[state][action]\n",
    "    next_state = np.random.choice([0, 1, 2], p=probas)\n",
    "    reward = rewards[state][action][next_state]\n",
    "    return next_state, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f230c05",
   "metadata": {},
   "source": [
    "Now let’s implement the agent’s exploration policy. Since the state space is pretty small, a simple random policy will be sufficient. If we run the algorithm for long enough, the agent will visit every state many times, and it will also try every possible action many times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "051f1329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploration_policy(state):\n",
    "    return np.random.choice(possible_actions[state])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3bf025",
   "metadata": {},
   "source": [
    "Next, after we initialize the Q-values just like earlier, we are ready to run the Q-learning algorithm with learning rate decay (using power scheduling, introduced in Chapter 11):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf785399",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha0 = 0.05 # Initialize learning rate\n",
    "decay = 0.005 # Learning rate decay\n",
    "gamma = 0.90 # Discount factor\n",
    "state = 0 # Initial state\n",
    "\n",
    "for iteration in range(10_000):\n",
    "    action = exploration_policy(state)\n",
    "    next_state, reward = step(state, action)\n",
    "    next_value = Q_values[next_state].max() # greedy policy at the next step\n",
    "    alpha = alpha0 / (1 + iteration * decay)\n",
    "    Q_values[state, action] *= 1 - alpha\n",
    "    Q_values[state, action] += alpha * (reward + gamma * next_value)\n",
    "    state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191395a8",
   "metadata": {},
   "source": [
    "This algorithm will converge to the optimal Q-values, but it will take many iterations, and possibly quite a lot of hyperparameter tuning. As you can see in Figure 18-9 from the book, the Q-value iteration algorithm (left) converges very quickly, in fewer than 20 iterations, while the Q-learning algorithm (right) takes about 8,000 iterations to converge. Obviously, not knowing the transition probabilities or the rewards makes finding the optimal policy significantly harder!\n",
    "\n",
    "The Q-learning algorithm is called an off-policy algorithm because the policy being trained is not necessarily the one used during training. For example, in the code we just ran, the policy being executed (the exploration policy) was completely random, while the policy being trained was never used. After training, the optimal policy corresponds to systematically choosing the action with the highest Q-value. Conversely, the policy gradients algorithm is an on-policy algorithm: it explores the world using the policy being trained. It is somewhat surprising that Q-learning is capable of learning the optimal policy by just watching an agent act randomly. Imagine learning to play golf when your teacher is a blindfolded monkey. Can we do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b08a51",
   "metadata": {},
   "source": [
    "### **Exploration Policies**\n",
    "Of course, Q-learning can work only if the exploration policy explores the MDP thoroughly enough. Although a purely random policy is guaranteed to eventually visit every state and every transition many times, it may take an extremely long time to do so. Therefore, a better option is to use the ε-greedy policy (ε is epsilon): at each step it acts randomly with probability ε, or greedily with probability 1–ε (i.e., choosing the action with the highest Qvalue). The advantage of the ε-greedy policy (compared to a completely random policy) is that it will spend more and more time exploring the interesting parts of the environment, as the Q-value estimates get better and better, while still spending some time visiting unknown regions of the MDP. It is quite common to start with a high value for ε (e.g., 1.0) and then gradually reduce it (e.g., down to 0.05).\n",
    "\n",
    "Alternatively, rather than relying only on chance for exploration, another approach is to encourage the exploration policy to try actions that it has not tried much before. This can be implemented as a bonus added to the Q-value estimates, as shown in Equation below.\n",
    "\n",
    "Q(s, a) ←<sub><sub>α</sub></sub> r + γ⋅max<sub><sub>a′</sub></sub> f(Q(s′, a′),N(s′, a′))\n",
    "\n",
    "In this equation:\n",
    "- N(s′, a′) counts the number of times the action a′ was chosen in state s′.\n",
    "- f(Q, N) is an exploration function, such as f(Q, N) = Q + κ/(1 + N), where κ is a curiosity hyperparameter that measures how much the agent is attracted to the unknown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2022c2",
   "metadata": {},
   "source": [
    "### **Approximate Q-Learning and Deep Q-Learning**\n",
    "The main problem with Q-learning is that it does not scale well to large (or even medium) MDPs with many states and actions. For example, suppose you wanted to use Q-learning to train an agent to play Ms. Pac-Man (see Figure 18-1 from the book). There are about 150 pellets that Ms. Pac-Man can eat, each of which can be present or absent (i.e., already eaten). So, the number of possible states is greater than 2 ≈ 10 . And if you add all the possible combinations of positions for all the ghosts and Ms. Pac-Man, the number of possible states becomes larger than the number of atoms in our planet, so there’s absolutely no way you can keep track of an estimate for every single Q-value.\n",
    "\n",
    "The solution is to find a function Q (s, a) that approximates the Q-value of any state-action pair (s, a) using a manageable number of parameters (given by the parameter vector θ). This is called approximate Q-learning. For years it was recommended to use linear combinations of handcrafted features extracted from the state (e.g., the distances of the closest ghosts, their directions, and so on) to estimate Q-values, but in 2013, [DeepMind](https://homl.info/dqn) showed that using deep neural networks can work much better, especially for complex problems, and it does not require any feature engineering. A DNN used to estimate Q-values is called a deep Q-network (DQN), and using a DQN for approximate Q-learning is called deep Q-learning.\n",
    "\n",
    "Now, how can we train a DQN? Well, consider the approximate Q-value computed by the DQN for a given stateaction pair (s, a). Thanks to Bellman, we know we want this approximate Q-value to be as close as possible to the reward r that we actually observe after playing action a in state s, plus the discounted value of playing optimally from then on. To estimate this sum of future discounted rewards, we can just execute the DQN on the next state s′, for all possible actions a′. We get an approximate future Q-value for each possible action. We then pick the highest (since we assume we will be playing optimally) and discount it, and this gives us an estimate of the sum of future discounted rewards. By summing the reward r and the future discounted value estimate, we get a target Q-value y(s, a) for the state-action pair (s, a), as shown in Equation below.\n",
    "\n",
    "y(s, a) = r + γ ⋅ max<sub><sub>a′</sub></sub> Q<sub><sub>𝛉</sub></sub>(s′, a′)\n",
    "\n",
    "With this target Q-value, we can run a training step using any gradient descent algorithm. Specifically, we generally try to minimize the squared error between the estimated Q-value Q (s, a) and the target Q-value y(s, a), or the Huber loss to reduce the algorithm’s sensitivity to large errors. And that’s the deep Q-learning algorithm! Let’s see how to implement it to solve the CartPole environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05146281",
   "metadata": {},
   "source": [
    "## **Implementing Deep Q-Learning**\n",
    "The first thing we need is a deep Q-network. In theory, we need a neural net that takes a state-action pair as input, and outputs an approximate Q-value. However, in practice it’s much more efficient to use a neural net that takes only a state as input, and outputs one approximate Q-value for each possible action. To solve the CartPole environment, we do not need a very complicated neural net; a couple of hidden layers will do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "26d9a2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaxon/anaconda3/envs/ai_env/lib/python3.10/site-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "input_shape = [4] # == env.observation_space.shape\n",
    "n_outputs = 2 # ==env.action_space.n\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation=\"elu\", input_shape=input_shape), \n",
    "    tf.keras.layers.Dense(32, activation=\"elu\"), \n",
    "    tf.keras.layers.Dense(n_outputs)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57d6b5c",
   "metadata": {},
   "source": [
    "To select an action using this DQN, we pick the action with the largest predicted Q-value. To ensure that the agent explores the environment, we will use an ε-greedy policy (i.e., we will choose a random action with probability ε):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "14b00469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(state, epsilon=0):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(low=0, high=n_outputs)\n",
    "    else:\n",
    "        Q_values = model.predict(state[np.newaxis], verbose=0)[0]\n",
    "        return Q_values.argmax() # optimal action according to the"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df8530b",
   "metadata": {},
   "source": [
    "Instead of training the DQN based only on the latest experiences, we will store all experiences in a replay buffer (or replay memory), and we will sample a random training batch from it at each training iteration. This helps reduce the correlations between the experiences in a training batch, which tremendously helps training. For this, we will just use a double-ended queue (***deque***):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f40c174e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "replay_buffer = deque(maxlen=2000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2ea2df",
   "metadata": {},
   "source": [
    "> #### **TIP**\n",
    "> A deque is a queue elements can be efficiently added to or removed from on both ends. Inserting and deleting items from the ends of the queue is very fast, but random access can be slow when the queue gets long. If you need a very large replay buffer, you should use a circular buffer instead (see the notebook for an implementation), or check out DeepMind’s Reverb library.\n",
    "\n",
    "Each experience will be composed of six elements: a state s, the action a that the agent took, the resulting reward r, the next state s′ it reached, a Boolean indicating whether the episode ended at that point (done), and finally another Boolean indicating whether the episode was truncated at that point. We will need a small function to sample a random batch of experiences from the replay buffer. It will return six NumPy arrays corresponding to the six experience elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bed0c240",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_experiences(batch_size):\n",
    "    indices = np.random.randint(low=0, high=len(replay_buffer), size=batch_size)\n",
    "    batch = [replay_buffer[index] for index in indices]\n",
    "    return [\n",
    "        np.array([experience[field_index] for experience in batch])\n",
    "        for field_index in range(6)\n",
    "    ] # [states, actions, rewards, next_states, dones, truncateds]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c59bd3",
   "metadata": {},
   "source": [
    "Let’s also create a function that will play a single step using the ε-greedy policy, then store the resulting experience in the replay buffer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "96354b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_step(env, state, epsilon):\n",
    "    action = epsilon_greedy_policy(state, epsilon)\n",
    "    next_state, reward, done, truncated, info = env.step(action)\n",
    "    replay_buffer.append((state, action, reward, next_state, done, truncated))\n",
    "    return next_state, reward, done, truncated, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415ea852",
   "metadata": {},
   "source": [
    "Finally, let’s create one last function that will sample a batch of experiences from the replay buffer and train the DQN by performing a single gradient descent step on this batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c8ae5bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "discount_factor = 0.95\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=1e-2)\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "def training_step(batch_size):\n",
    "    experiences = sample_experiences(batch_size)\n",
    "    states, actions, rewards, next_states, dones, truncateds = experiences\n",
    "    next_Q_values = model.predict(next_states, verbose=0)\n",
    "    max_next_Q_values = next_Q_values.max(axis=1)\n",
    "    runs = 1.0 - (dones | truncateds) # episode is not done or truncated\n",
    "    target_Q_values = rewards + runs * discount_factor * max_next_Q_values\n",
    "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
    "    mask = tf.one_hot(actions, n_outputs)\n",
    "    with tf.GradientTape() as tape:\n",
    "        all_Q_values = model(states)\n",
    "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
    "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
    "        \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094845ff",
   "metadata": {},
   "source": [
    "Here's what's happening in this code:\n",
    "- First we define some hyperparameters, and we create the optimizer and the loss function.\n",
    "\n",
    "- Then we create the ***training_step()*** function. It starts by sampling a batch of experiences, then it uses the DQN to predict the Q-value for each possible action in each experience’s next state. Since we assume that the agent will be playing optimally, we only keep the maximum Q-value for each next state. Next, we use Equation 18-7 from the book to compute the target Q-value for each experience’s state-action pair.\n",
    "  \n",
    "- We want to use the DQN to compute the Q-value for each experienced state-action pair, but the DQN will also output the Q-values for the other possible actions, not just for the action that was actually chosen by the agent. So, we need to mask out all the Q-values we do not need. The tf.one_hot() function makes it possible to convert an array of action indices into such a mask. For example, if the first three experiences contain actions 1, 1, 0, respectively, then the mask will start with [[0, 1], [0, 1], [1, 0],...]. We can then multiply the DQN’s output with this mask, and this will zero out all the Q-values we do not want. We then sum over axis 1 to get rid of all the zeros, keeping only the Q-values of the experienced stateaction pairs. This gives us the Q_values tensor, containing one predicted Q-value for each experience in the batch.\n",
    "  \n",
    "- Next, we compute the loss: it is the mean squared error between the target and predicted Q-values for the experienced state-action pairs.\n",
    "  \n",
    "- Finally, we perform a gradient descent step to minimize the loss with regard to the model’s trainable variables.\n",
    "\n",
    "This was the hardest part. Now training the model is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f39e9e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(600):\n",
    "    obs, info = env.reset()\n",
    "    for step in range(200):\n",
    "        epsilon = max(1 - episode / 500, 0.01)\n",
    "        obs, reward, done, truncated, info = play_one_step(env, obs, epsilon)\n",
    "        if done or truncated:\n",
    "            break\n",
    "        \n",
    "        if episode > 50:\n",
    "            training_step(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926d2c9c",
   "metadata": {},
   "source": [
    "We run 600 episodes, each for a maximum of 200 steps. At each step, we first compute the ***epsilon*** value for the ε-greedy policy: it will go from 1 down to 0.01, linearly, in a bit under 500 episodes. Then we call the ***play_one_step()*** function, which will use the ε-greedy policy to pick an action, then execute it and record the experience in the replay buffer. If the episode is done or truncated, we exit the loop. Finally, if we are past episode 50, we call the ***training_step()*** function to train the model on one batch sampled from the replay buffer. The reason we play many episodes without training is to give the replay buffer some time to fill up (if we don’t wait enough, then there will not be enough diversity in the replay buffer). And that’s it: we just implemented the Deep Q-learning algorithm!\n",
    "\n",
    "Figure 18-10 from the book shows the total rewards the agent got during each episode\n",
    "\n",
    "As you can see, the algorithm took a while to start learning anything, in part because ε was very high at the beginning. Then its progress was erratic: it first reached the max reward around episode 220, but it immediately dropped, then bounced up and down a few times, and soon after it looked like it had finally stabilized near the max reward, at around episode 320, its score again dropped down dramatically. This is called catastrophic forgetting, and it is one of the big problems facing virtually all RL algorithms: as the agent explores the environment, it updates its policy, but what it learns in one part of the environment may break what it learned earlier in other parts of the environment. The experiences are quite correlated, and the learning environment keeps changing—this is not ideal for gradient descent! If you increase the size of the replay buffer, the algorithm will be less subject to this problem. Tuning the learning rate may also help. But the truth is, reinforcement learning is hard: training is often unstable, and you may need to try many hyperparameter values and random seeds before you find a combination that works well. For example, if you try changing the activation function from \"elu\" to \"relu\", the performance will be much lower.\n",
    "\n",
    "> #### **NOTE**\n",
    "> Reinforcement learning is notoriously difficult, largely because of the training instabilities and the huge sensitivity to the choice of hyperparameter values and random seeds. As the researcher Andrej Karpathy put it, “[Supervised learning] wants to work. […] RL must be forced to work”. You will need time, patience, perseverance, and perhaps a bit of luck too. This is a major reason RL is not as widely adopted as regular deep learning (e.g., convolutional nets). But there are a few real-world applications, beyond AlphaGo and Atari games: for example, Google uses RL to optimize its datacenter costs, and it is used in some robotics applications, for hyperparameter tuning, and in recommender systems.\n",
    "\n",
    "You might wonder why we didn’t plot the loss. It turns out that loss is a poor indicator of the model’s performance. The loss might go down, yet the agent might perform worse (e.g., this can happen when the agent gets stuck in one small region of the environment, and the DQN starts overfitting this region). Conversely, the loss could go up, yet the agent might perform better (e.g., if the DQN was underestimating the Q-values and it starts correctly increasing its predictions, the agent will likely perform better, getting more rewards, but the loss might increase because the DQN also sets the targets, which will be larger too). So, it’s preferable to plot the rewards.\n",
    "\n",
    "The basic deep Q-learning algorithm we’ve been using so far would be too unstable to learn to play Atari games. So how did DeepMind do it? Well, they tweaked the algorithm!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb16272a",
   "metadata": {},
   "source": [
    "## **Deep Q-Learning Variants**\n",
    "In the basic deep Q-learning algorithm, the model is used both to make predictions and to set its own targets. This can lead to a situation analogous to a dog chasing its own tail. This feedback loop can make the network unstable: it can diverge, oscillate, freeze, and so on. To solve this problem, in their 2013 paper the DeepMind researchers used two DQNs instead of one: the first is the online model, which learns at each step and is used to move the agent around, and the other is the target model used only to define the targets. The target model is just a clone of the online model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3ea925",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = tf.keras.models.clone_model(model) # clone the model's architectures\n",
    "target.set_weights(model.get_weight()) # copy the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "63ce725f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Sequential name=sequential_2, built=True>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e559eaf8",
   "metadata": {},
   "source": [
    "Then, in the ***training_step()*** function, we just need to change one line to use the target model instead of the online model when computing the Q-values of the next states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43160db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_Q_values = target.predict(next_states, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fc1e5c",
   "metadata": {},
   "source": [
    "Finally, in the training loop, we must copy the weights of the online model to the target model, at regular intervals (e.g., every 50 episodes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22b0ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if episode % 50 == 0:\n",
    "    target.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecd89a7",
   "metadata": {},
   "source": [
    "Since the target model is updated much less often than the online model, the Q-value targets are more stable, the feedback loop we discussed earlier is dampened, and its effects are less severe. This approach was one of the DeepMind researchers’ main contributions in their 2013 paper, allowing agents to learn to play Atari games from raw pixels. To stabilize training, they used a tiny learning rate of 0.00025, they updated the target model only every 10,000 steps (instead of 50), and they used a very large replay buffer of 1 million experiences. They decreased epsilon very slowly, from 1 to 0.1 in 1 million steps, and they let the algorithm run for 50 million steps. Moreover, their DQN was a deep convolutional net.\n",
    "\n",
    "Now let’s take a look at another DQN variant that managed to beat the state of the art once more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf869931",
   "metadata": {},
   "source": [
    "### **Double DQN**\n",
    "In a [2015 paper](https://homl.info/doubledqn), DeepMind researchers tweaked their DQN algorithm, increasing its performance and somewhat stabilizing training. They called this variant double DQN. The update was based on the observation that the target network is prone to overestimating Q-values. Indeed, suppose all actions are equally good: the Q-values estimated by the target model should be identical, but since they are approximations, some may be slightly greater than others, by pure chance. The target model will always select the largest Q-value, which will be slightly greater than the mean Q-value, most likely overestimating the true Q-value (a bit like counting the height of the tallest random wave when measuring the depth of a pool). To fix this, the researchers proposed using the online model instead of the target model when selecting the best actions for the next states, and using the target model only to estimate the Q-values for these best actions. Here is the updated ***training_step()*** function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ae40f6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(batch_size):\n",
    "    experiences = sample_experiences(batch_size)\n",
    "    states, actions, rewards, next_states, dones, truncateds = experiences\n",
    "    next_Q_values = model.predict(next_states, verbose=0) # ≠ target.predict()\n",
    "    best_next_actions = next_Q_values.argmax(axis=1)\n",
    "    next_mask = tf.one_hot(best_next_actions, n_outputs).numpy()\n",
    "    max_next_Q_values = (target.predict(next_states, verbose=0) * next_mask\n",
    "                                       ).sum(axis=1)\n",
    "    \n",
    "    # The rest is the same as earlier\n",
    "    runs = 1.0 - (dones | truncateds) # episode is not done or truncated\n",
    "    target_Q_values = rewards + runs * discount_factor * max_next_Q_values\n",
    "    target_Q_values = target_Q_values.reshape(-1, 1)\n",
    "    mask = tf.one_hot(actions, n_outputs)\n",
    "    with tf.GradientTape() as tape:\n",
    "        all_Q_values = model(states)\n",
    "        Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True)\n",
    "        loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values))\n",
    "        \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b4fe12",
   "metadata": {},
   "source": [
    "Just a few months later, another improvement to the DQN algorithm was propose; we’ll look at that next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01461635",
   "metadata": {},
   "source": [
    "### **Prioritized Experience Replay**\n",
    "Instead of sampling experiences uniformly from the replay buffer, why not sample important experiences more frequently? This idea is called importance sampling (IS) or prioritized experience replay (PER), and it was introduced in a [2015 paper](https://homl.info/prioreplay) by DeepMind researchers (once again!).\n",
    "\n",
    "More specifically, experiences are considered “important” if they are likely to lead to fast learning progress. But how can we estimate this? One reasonable approach is to measure the magnitude of the TD error δ = r + γ·V(s′) – V(s). A large TD error indicates that a transition (s, a, s′) is very surprising, and thus probably worth learning from. When an experience is recorded in the replay buffer, its priority is set to a very large value, to ensure that it gets sampled at least once. However, once it is sampled (and every time it is sampled), the TD error δ is computed, and this experience’s priority is set to p = |δ| (plus a small constant to ensure that every experience has a nonzero probability of being sampled). The probability P of sampling an experience with priority p is proportional to p , where ζ is a hyperparameter that controls how greedy we want importance sampling to be: when ζ = 0, we just get uniform sampling, and when ζ = 1, we get full-blown importance sampling. In the paper, the authors used ζ = 0.6, but the optimal value will depend on the task.\n",
    "\n",
    "There’s one catch, though: since the samples will be biased toward important experiences, we must compensate for this bias during training by downweighting the experiences according to their importance, or else the model will just overfit the important experiences. To be clear, we want important experiences to be sampled more often, but this also means we must give them a lower weight during training. To do this, we define each experience’s training weight as w = (n P) , where n is the number of experiences in the replay buffer, and β is a hyperparameter that controls how much we want to compensate for the importance sampling bias (0 means not at all, while 1 means entirely). In the paper, the authors used β = 0.4 at the beginning of training and linearly increased it to β = 1 by the end of training. Again, the optimal value will depend on the task, but if you increase one, you will usually want to increase the other as well.\n",
    "\n",
    "Now let’s look at one last important variant of the DQN algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6a3e36",
   "metadata": {},
   "source": [
    "### **Dueling DQN**\n",
    "The dueling DQN algorithm (DDQN, not to be confused with double DQN, although both techniques can easily be combined) was introduced in yet another [2015 paper](https://homl.info/ddqn) by DeepMind researchers. To understand how it works, we must first note that the Q-value of a state-action pair (s, a) can be expressed as Q(s, a) = V(s) + A(s, a), where V(s) is the value of state s and A(s, a) is the advantage of  aking the action a in state s, compared to all other possible actions in that state. Moreover, the value of a state is equal to the Q-value of the best action a for that state (sincewe assume the optimal policy will pick the best action), so V(s) = Q(s, a ), which implies that A(s, a ) = 0. In a dueling DQN, the model estimates both the value of the state and the advantage of each possible action. Since the best action should have an advantage of 0, the model subtracts the maximum predicted advantage from all predicted advantages. Here is a simple DDQN model, implemented using the functional API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69332cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_states = tf.keras.layers.Input(shape=[4])\n",
    "hidden1 = tf.keras.layers.Dense(32, activation=\"elu\")(input_states)\n",
    "hidden2 = tf.keras.layers.Dense(32, activation=\"elu\")(hidden1)\n",
    "state_values = tf.keras.layers.Dense(1)(hidden2)\n",
    "raw_advantages = tf.keras.layers.Dense(n_outputs)(hidden2)\n",
    "advantages = raw_advantages - tf.reduce_max(raw_advantages, axis=1,\n",
    "                                            keepdims=True)\n",
    "Q_values = state_values + advantages\n",
    "model = tf.keras.Model(inputs=[input_states], outputs=[Q_values])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd569094",
   "metadata": {},
   "source": [
    "The rest of the algorithm is just the same as earlier. In fact, you can build a double dueling DQN and combine it with prioritized experience replay! More generally, many RL techniques can be combined, as DeepMind demonstrated in a [2017 paper](https://homl.info/rainbow): the paper’s authors combined six different techniques into an agent called Rainbow, which largely outperformed the state of the art.\n",
    "\n",
    "As you can see, deep reinforcement learning is a fast-growing field and there’s much more to discover!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75289f3c",
   "metadata": {},
   "source": [
    "## **Overview of some Popular RL Algorithms**\n",
    "Before we close this chapter, let’s take a brief look at a few other popular algorithms:\n",
    "1. ***[AlphaGo](https://homl.info/alphago)***\n",
    "    - AlphaGo uses a variant of Monte Carlo tree search (MCTS) based on deep neural networks to beat human champions at the game of Go. MCTS was invented in 1949 by Nicholas Metropolis and Stanislaw Ulam. It selects the best move after running many simulations, repeatedly exploring the search tree starting from the current position, and spending more time on the most promising branches. When it reaches a node that it hasn’t visited before, it plays randomly until the game ends, and updates its estimates for each visited node (excluding the random moves), increasing or decreasing each estimate depending on the final outcome. AlphaGo is based on the same principle, but it uses a policy network to select moves, rather than playing randomly. This policy net is trained using policy gradients. The original algorithm involved three more neural networks, and was more complicated, but it was simplified in the [AlphaGo Zero paper](https://homl.info/alphagozero), which uses a single neural network to both select moves and evaluate game states. The [AlphaZero paper](https://homl.info/alphazero) generalized this algorithm, making it capable of tackling not only the game of Go, but also chess and shogi (Japanese chess). Lastly, the [MuZero paper](https://homl.info/muzero) continued to improve upon this algorithm, outperforming the previous iterations even though the agent starts out without even knowing the rules of the game!\n",
    "\n",
    "2. ***Actor-critic algorithms***\n",
    "    - Actor-critics are a family of RL algorithms that combine policy gradients with deep Q-networks. An actorcritic agent contains two neural networks: a policy net and a DQN. The DQN is trained normally, by learning from the agent’s experiences. The policy net learns differently (and much faster) than in regular PG: instead of estimating the value of each action by going through multiple episodes, then summing the future discounted rewards for each action, and finally normalizing them, the agent (actor) relies on the action values estimated by the DQN (critic). It’s a bit like an athlete (the agent) learning with the help of a coach (the DQN).\n",
    "  \n",
    "3. [***Asynchronous advantage actor-critic (A3C)***](https://homl.info/a3C)\n",
    "    - This is an important actor-critic variant introduced by DeepMind researchers in 2016 where multiple agents learn in parallel, exploring different copies of the environment. At regular intervals, but asynchronously (hence the name), each agent pushes some weight updates to a master network, then it pulls the latest weights from that network. Each agent thus contributes to improving the master network and benefits from what the other agents have learned. Moreover, instead of estimating the Q-values, the DQN estimates the advantage of each action (hence the second A in the name), which stabilizes training.\n",
    "  \n",
    "5. [***Advantage actor-critic(A2C)***](https://homl.info/a2c)\n",
    "    - A2C is a variant of the A3C algorithm that removes the asynchronicity. All model updates are synchronous, so gradient updates are performed over larger batches, which allows the model to better utilize the power of the GPU.\n",
    "\n",
    "6. [***Soft actor-critic (SAC)***](https://homl.info/sac)\n",
    "    - SAC is an actor-critic variant proposed in 2018 by Tuomas Haarnoja and other UC Berkeley researchers. It learns not only rewards, but also to maximize the entropy of its actions. In other words, it tries to be as unpredictable as possible while still getting as many rewards as possible. This encourages the agent to explore the environment, which speeds up training, and makes it less likely to repeatedly execute the same action when the DQN produces imperfect estimates. This algorithm has demonstrated an amazing sample efficiency (contrary to all the previous algorithms, which learn very slowly).\n",
    "\n",
    "7. [***Proximal policy optimization (PPO)***](https://homl.info/ppo)\n",
    "    - This algorithm by John Schulman and other OpenAI researchers is based on A2C, but it clips the loss function to avoid excessively large weight updates (which often lead to training instabilities). PPO is a simplification of the previous [*trust region policy optimization*](https://homl.info/trpo) (TRPO) algorithm, also by OpenAI. OpenAI made the news in April 2019 with its AI called OpenAI Five, based on the PPO algorithm, which defeated the world champions at the multiplayer game Dota 2.\n",
    "  \n",
    "8. [***Curiosity-based exploration***](https://homl.info/curiosity)\n",
    "    - A recurring problem in RL is the sparsity of the rewards, which makes learning very slow and inefficient. Deepak Pathak and other UC Berkeley researchers have proposed an exciting way to tackle this issue: why not ignore the rewards, and just make the agent extremely curious to explore the environment? The rewards thus become intrinsic to the agent, rather than coming from the environment. Similarly, stimulating curiosity in a child is more likely to give good results than purely rewarding the child for getting good grades. How does this work? The agent continuously tries to predict the outcome of its actions, and it seeks situations where the outcome does not match its predictions. In other words, it wants to be surprised. If the outcome is predictable (boring), it goes elsewhere. However, if the outcome is unpredictable but the agent notices that it has no control over it, it also gets bored after a while. With only curiosity, the authors succeeded in training an agent at many video games: even though the agent gets no penalty for losing, the game starts over, which is boring so it learns to avoid it.\n",
    "\n",
    "9. ***Open-ended learning (OEL)***\n",
    "    - The objective of OEL is to train agents capable of endlessly learning new and interesting tasks, typically generated procedurally. We’re not there yet, but there has been some amazing progress over the last few years. For example, a [*2019 paper*](https://homl.info/poet) by a team of researchers from Uber AI introduced the POET algorithm, which generates multiple simulated 2D environments with bumps and holes and trains one agent per environment: the agent’s goal is to walk as fast as possible while avoiding the obstacles. The algorithm starts out with simple environments, but they gradually get harder over time: this is called curriculum learning. Moreover, although each agent is only trained within one environment, it must regularly compete against other agents, across all environments. In each environment, the winner is copied over and it replaces the agent that was there before. This way, knowledge is regularly transferred across environments, and the most adaptable agents are selected. In the end, the agents are much better walkers than agents trained on a single task, and they can tackle much harder environments. Of course, this principle can be applied to other environments and tasks as well. If you’re interested in OEL, make sure to check out the Enhanced POET paper, as well as DeepMind’s 2021 paper on this topic.\n",
    "\n",
    "> #### **TIP**\n",
    "> If you’d like to learn more about reinforcement learning, check out the book Reinforcement Learning by Phil Winder (O’Reilly).\n",
    "\n",
    "We covered many topics in this chapter: policy gradients, Markov chains, Markov decision processes, Q-learning, approximate Q-learning, and deep Q-learning and its main variants (fixed Q-value targets, double DQN, dueling DQN, and prioritized experience replay), and finally we took a quick look at a few other popular algorithms. Reinforcement learning is a huge and exciting field, with new ideas and algorithms popping out every day, so I hope this chapter sparked your curiosity: there is a whole world to explore!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5510e66b",
   "metadata": {},
   "source": [
    "# **-------------------END!------------------**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
